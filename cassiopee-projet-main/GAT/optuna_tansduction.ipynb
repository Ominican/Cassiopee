{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69545/47490617.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_features_for_training = torch.tensor(x_train, dtype=torch.float)\n",
      "/tmp/ipykernel_69545/47490617.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_features_for_testing = torch.tensor(x_test, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "Dataframe_Labels = pd.read_csv(\"../../BLCA_DATA/Workspace/labels_str.csv\")\n",
    "Dataframe_link = pd.read_csv(\"../../BLCA_DATA/Workspace/patient_norm.csv\")\n",
    "Dataframe_node= pd.read_csv(\"../../BLCA_DATA/Workspace/node_embedding.csv\")\n",
    "\n",
    "Dataframe_Labels['class_int'], uniques = pd.factorize(Dataframe_Labels['class'])\n",
    "\n",
    "classes_dict = {0: 'LumP', \n",
    "                1: 'Ba/Sq', \n",
    "                2: 'LumU', \n",
    "                3: 'Stroma-rich', \n",
    "                4: 'LumNS', \n",
    "                5: 'NE-like'\n",
    "}\n",
    "\n",
    "def count_classes_weights(tensor):\n",
    "    array = tensor.numpy()\n",
    "    classes_tab = { 0: 0, \n",
    "                    1: 0,\n",
    "                    2: 0, \n",
    "                    3: 0, \n",
    "                    4: 0,\n",
    "                    5: 0\n",
    "    }\n",
    "    for i in array:\n",
    "        classes_tab[i]+=1\n",
    "\n",
    "    mean_nb_classes = 0\n",
    "    for i in classes_tab:\n",
    "        mean_nb_classes += i\n",
    "    mean_nb_classes *= 1/len(classes_tab)\n",
    "    \n",
    "    # normalize the weights\n",
    "    weight_sum = 0\n",
    "    for i in range(len(classes_tab)):\n",
    "        if classes_tab[i] != 0:\n",
    "            weight_sum += mean_nb_classes / classes_tab[i]\n",
    "    alpha = 1 / weight_sum\n",
    "\n",
    "    weight_dict = {}\n",
    "    used_classes = []\n",
    "    for i in range(len(classes_tab)):\n",
    "        if classes_tab[i] != 0:\n",
    "            weight_dict[i] = alpha * (mean_nb_classes / classes_tab[i]) *50\n",
    "            used_classes.append(classes_dict[i])\n",
    "\n",
    "    return used_classes, weight_dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "node_features = Dataframe_node.drop(columns=['Patient']).values\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "train_val_indices = [i for i in range(node_features.shape[0])]\n",
    "_, test_indices = train_test_split(\n",
    "    range(len(node_features)), \n",
    "    test_size=0.2, \n",
    "    random_state=1234\n",
    ")\n",
    "\n",
    "x_train = node_features[train_val_indices]\n",
    "x_test = node_features[test_indices]\n",
    "\n",
    "patient_similarity = cosine_similarity(Dataframe_link.iloc[:, 1:])\n",
    "similarity_threshold = 0.5  # Exemple de seuil de similaritÃ©\n",
    "\n",
    "# Calculate the edges and attention ridges for training\n",
    "edge_index_for_training = []\n",
    "edge_attr_for_training = []\n",
    "re_indexed_i = 0\n",
    "re_indexed_j = 0\n",
    "for i in range(len(train_val_indices)):\n",
    "    for j in range(i+1, len(train_val_indices)):\n",
    "        if patient_similarity[i, j] > similarity_threshold:\n",
    "            edge_index_for_training.append([re_indexed_i, re_indexed_j])\n",
    "            edge_attr_for_training.append((patient_similarity[i, j] - similarity_threshold)/(1 - similarity_threshold))\n",
    "        re_indexed_j +=1\n",
    "    re_indexed_i +=1\n",
    "    re_indexed_j = 0\n",
    "\n",
    "# Calculate the edges and attention ridges for testing\n",
    "edge_index_for_testing = []\n",
    "edge_attr_for_testing = []\n",
    "re_indexed_i = 0\n",
    "re_indexed_j = 0\n",
    "for i in test_indices:\n",
    "    for j in test_indices:\n",
    "        if i >= j :\n",
    "            break\n",
    "        if patient_similarity[i, j] > similarity_threshold:\n",
    "            edge_index_for_testing.append([re_indexed_i, re_indexed_j])\n",
    "            edge_attr_for_testing.append((patient_similarity[i, j] - similarity_threshold)/(1 - similarity_threshold))\n",
    "        re_indexed_j +=1\n",
    "    re_indexed_i +=1\n",
    "    re_indexed_j = 0\n",
    "\n",
    "node_labels = Dataframe_Labels[\"class_int\"].values\n",
    "train_labels = torch.tensor(node_labels[train_val_indices], dtype=torch.long)\n",
    "test_labels = torch.tensor(node_labels[test_indices], dtype=torch.long)\n",
    "\n",
    "node_labels = torch.tensor(node_labels, dtype=torch.long)\n",
    "\n",
    "used_classes, weight_dict = count_classes_weights(node_labels)\n",
    "Dataframe_Labels['weight'] = [weight_dict[x] for x in Dataframe_Labels['class_int']]\n",
    "node_weights = torch.tensor(Dataframe_Labels['weight'], dtype=torch.float)\n",
    "\n",
    "edge_features_for_training = torch.tensor(x_train, dtype=torch.float)\n",
    "edge_index_for_training = torch.tensor(edge_index_for_training, dtype=torch.int64).t().contiguous()\n",
    "edge_attr_for_training = torch.tensor(edge_attr_for_training, dtype=torch.float)\n",
    "node_weights_for_training = node_weights[train_val_indices]\n",
    "\n",
    "edge_features_for_testing = torch.tensor(x_test, dtype=torch.float)\n",
    "edge_index_for_testing = torch.tensor(edge_index_for_testing, dtype=torch.int64).t().contiguous()\n",
    "edge_attr_for_testing = torch.tensor(edge_attr_for_testing, dtype=torch.float)\n",
    "node_weights_for_testing = node_weights[test_indices]\n",
    "\n",
    "num_classes = 6\n",
    "test_mask = torch.zeros(len(edge_features_for_training), dtype=torch.bool)\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "train_val_data = Data(\n",
    "    x=edge_features_for_training, \n",
    "    edge_index=edge_index_for_training, \n",
    "    edge_attr=edge_attr_for_training, \n",
    "    y=train_labels, \n",
    "    weights=node_weights_for_training, \n",
    "    num_classes=num_classes,\n",
    "    num_nodes = len(edge_features_for_training),\n",
    "    num_features = edge_features_for_training.shape[1],\n",
    "    test_mask = test_mask\n",
    ")\n",
    "\n",
    "test_data = Data(\n",
    "    x=edge_features_for_testing, \n",
    "    edge_index=edge_index_for_testing, \n",
    "    edge_attr=edge_attr_for_testing, \n",
    "    y=test_labels, \n",
    "    weights=node_weights_for_testing, \n",
    "    num_classes=num_classes, \n",
    "    num_nodes = len(edge_features_for_testing),\n",
    "    num_features = edge_features_for_testing.shape[1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-14 22:03:14,617] A new study created in memory with name: no-name-ba1c44b3-326d-4708-9157-b18c5304faad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Epoch: 001, Loss: 2.6318, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 002, Loss: 5.4608, Train 0.1337, Val 0.1385\n",
      "Fold: 1, Epoch: 003, Loss: 3.3197, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 004, Loss: 2.9164, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 005, Loss: 3.7918, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 006, Loss: 3.1001, Train 0.1089, Val 0.0615\n",
      "Fold: 1, Epoch: 007, Loss: 2.4963, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 008, Loss: 2.7575, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 009, Loss: 2.9815, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 010, Loss: 2.5297, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 011, Loss: 2.3259, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 012, Loss: 2.3408, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 013, Loss: 2.5021, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 014, Loss: 2.5161, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 015, Loss: 2.2968, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 016, Loss: 2.1752, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 017, Loss: 2.3291, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 018, Loss: 2.4578, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 019, Loss: 2.5711, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 020, Loss: 2.2673, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 021, Loss: 2.1503, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 022, Loss: 2.2636, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 023, Loss: 2.3391, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 024, Loss: 2.3789, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 025, Loss: 2.1973, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 026, Loss: 2.1572, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 027, Loss: 2.1695, Train 0.3094, Val 0.3692\n",
      "Fold: 1, Epoch: 028, Loss: 2.2490, Train 0.3119, Val 0.3692\n",
      "Fold: 1, Epoch: 029, Loss: 2.2210, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 030, Loss: 2.1450, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 031, Loss: 2.1368, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 032, Loss: 2.1534, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 033, Loss: 2.1862, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 034, Loss: 2.1714, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 035, Loss: 2.1464, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 036, Loss: 2.1654, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 037, Loss: 2.1537, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 038, Loss: 2.1500, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 039, Loss: 2.1518, Train 0.3762, Val 0.3538\n",
      "Fold: 1, Epoch: 040, Loss: 2.2179, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 041, Loss: 2.2135, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 042, Loss: 2.1526, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 043, Loss: 2.1710, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 044, Loss: 2.1775, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 045, Loss: 2.1908, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 046, Loss: 2.1347, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 047, Loss: 2.1585, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 048, Loss: 2.1494, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 049, Loss: 2.1791, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 050, Loss: 2.1631, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 051, Loss: 2.1470, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 052, Loss: 2.1383, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 053, Loss: 2.1695, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 054, Loss: 2.1656, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 055, Loss: 2.1460, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 056, Loss: 2.1267, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 057, Loss: 2.1492, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 058, Loss: 2.1751, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 059, Loss: 2.1951, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 060, Loss: 2.1252, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 061, Loss: 2.1378, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 062, Loss: 2.1312, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 063, Loss: 2.2055, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 064, Loss: 2.1587, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 065, Loss: 2.1270, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 066, Loss: 2.1466, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 067, Loss: 2.1530, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 068, Loss: 2.2037, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 069, Loss: 2.1273, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 070, Loss: 2.1449, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 071, Loss: 2.1267, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 072, Loss: 2.1247, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 073, Loss: 2.1469, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 074, Loss: 2.1378, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 075, Loss: 2.1295, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 076, Loss: 2.1374, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 077, Loss: 2.1283, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 078, Loss: 2.1597, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 079, Loss: 2.1317, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 080, Loss: 2.1469, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 081, Loss: 2.1513, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 082, Loss: 2.1424, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 083, Loss: 2.1392, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 084, Loss: 2.1235, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 085, Loss: 2.1375, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 086, Loss: 2.1751, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 087, Loss: 2.1238, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 088, Loss: 2.1490, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 089, Loss: 2.1505, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 090, Loss: 2.1269, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 091, Loss: 2.1303, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 092, Loss: 2.1493, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 093, Loss: 2.1171, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 094, Loss: 2.1266, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 095, Loss: 2.1325, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 096, Loss: 2.1378, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 097, Loss: 2.1374, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 098, Loss: 2.1592, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 099, Loss: 2.1174, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 100, Loss: 2.1493, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 101, Loss: 2.1423, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 102, Loss: 2.1723, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 103, Loss: 2.1346, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 104, Loss: 2.1454, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 105, Loss: 2.1320, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 106, Loss: 2.1293, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 107, Loss: 2.1486, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 108, Loss: 2.1240, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 109, Loss: 2.1154, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 110, Loss: 2.1326, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 111, Loss: 2.1249, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 112, Loss: 2.1363, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 113, Loss: 2.1147, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 114, Loss: 2.1259, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 115, Loss: 2.1163, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 116, Loss: 2.1076, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 117, Loss: 2.1289, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 118, Loss: 2.1220, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 119, Loss: 2.1286, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 120, Loss: 2.1260, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 121, Loss: 2.1032, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 122, Loss: 2.1148, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 123, Loss: 2.1276, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 124, Loss: 2.1214, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 125, Loss: 2.1164, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 126, Loss: 2.1167, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 127, Loss: 2.1144, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 128, Loss: 2.1424, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 129, Loss: 2.1236, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 130, Loss: 2.1407, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 131, Loss: 2.1005, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 132, Loss: 2.1050, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 133, Loss: 2.1089, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 134, Loss: 2.1204, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 135, Loss: 2.1277, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 136, Loss: 2.1121, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 137, Loss: 2.1139, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 138, Loss: 2.1335, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 139, Loss: 2.1008, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 140, Loss: 2.1213, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 141, Loss: 2.1077, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 142, Loss: 2.1068, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 143, Loss: 2.1019, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 144, Loss: 2.1104, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 145, Loss: 2.1097, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 146, Loss: 2.1203, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 147, Loss: 2.0968, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 148, Loss: 2.1269, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 149, Loss: 2.1266, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 150, Loss: 2.1385, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 151, Loss: 2.1103, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 152, Loss: 2.1875, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 153, Loss: 2.1298, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 154, Loss: 2.0890, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 155, Loss: 2.1239, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 156, Loss: 2.1418, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 157, Loss: 2.1438, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 158, Loss: 2.1184, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 159, Loss: 2.1476, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 160, Loss: 2.1292, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 161, Loss: 2.1219, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 162, Loss: 2.1002, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 163, Loss: 2.1256, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 164, Loss: 2.1014, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 165, Loss: 2.1078, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 166, Loss: 2.1057, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 167, Loss: 2.0954, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 168, Loss: 2.1025, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 169, Loss: 2.1082, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 170, Loss: 2.0983, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 171, Loss: 2.1092, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 172, Loss: 2.1027, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 173, Loss: 2.1188, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 174, Loss: 2.1067, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 175, Loss: 2.1072, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 176, Loss: 2.1024, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 177, Loss: 2.0984, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 178, Loss: 2.0951, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 179, Loss: 2.1023, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 180, Loss: 2.1184, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 181, Loss: 2.0928, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 182, Loss: 2.0938, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 183, Loss: 2.0992, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 184, Loss: 2.1385, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 185, Loss: 2.1040, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 186, Loss: 2.1048, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 187, Loss: 2.1162, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 188, Loss: 2.1257, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 189, Loss: 2.0915, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 190, Loss: 2.1498, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 191, Loss: 2.0987, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 192, Loss: 2.0882, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 193, Loss: 2.1236, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 194, Loss: 2.1292, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 195, Loss: 2.1204, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 196, Loss: 2.0997, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 197, Loss: 2.1109, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 198, Loss: 2.0858, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 199, Loss: 2.0980, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 200, Loss: 2.0969, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 201, Loss: 2.0931, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 202, Loss: 2.1004, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 203, Loss: 2.1182, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 204, Loss: 2.0999, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 205, Loss: 2.1125, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 206, Loss: 2.0766, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 207, Loss: 2.1086, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 208, Loss: 2.0969, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 209, Loss: 2.1076, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 210, Loss: 2.0772, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 211, Loss: 2.1069, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 212, Loss: 2.1581, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 213, Loss: 2.0731, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 214, Loss: 2.1602, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 215, Loss: 2.1152, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 216, Loss: 2.0705, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 217, Loss: 2.0830, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 218, Loss: 2.1336, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 219, Loss: 2.0909, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 220, Loss: 2.1010, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 221, Loss: 2.1152, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 222, Loss: 2.0919, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 223, Loss: 2.0763, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 224, Loss: 2.0909, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 225, Loss: 2.1109, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 226, Loss: 2.0881, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 227, Loss: 2.0397, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 228, Loss: 2.0691, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 229, Loss: 2.0916, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 230, Loss: 2.0352, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 231, Loss: 2.0451, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 232, Loss: 2.0550, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 233, Loss: 2.0623, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 234, Loss: 2.0525, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 235, Loss: 2.0198, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 236, Loss: 2.0577, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 237, Loss: 2.0308, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 238, Loss: 2.0151, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 239, Loss: 2.0281, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 240, Loss: 2.0328, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 241, Loss: 2.0135, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 242, Loss: 2.0377, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 243, Loss: 2.0460, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 244, Loss: 2.0096, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 245, Loss: 2.0514, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 246, Loss: 1.9762, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 247, Loss: 1.9892, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 248, Loss: 1.9956, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 249, Loss: 1.9819, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 250, Loss: 1.9377, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 251, Loss: 1.9190, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 252, Loss: 1.9194, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 253, Loss: 1.9396, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 254, Loss: 1.8907, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 255, Loss: 1.8915, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 256, Loss: 1.9426, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 257, Loss: 1.8824, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 258, Loss: 1.8446, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 259, Loss: 1.8180, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 260, Loss: 1.8662, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 261, Loss: 1.8107, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 262, Loss: 1.7582, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 263, Loss: 1.7285, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 264, Loss: 1.6661, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 265, Loss: 1.6356, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 266, Loss: 1.6488, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 267, Loss: 1.6939, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 268, Loss: 1.5905, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 269, Loss: 1.5249, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 270, Loss: 1.6061, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 271, Loss: 1.5027, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 272, Loss: 1.5284, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 273, Loss: 1.3402, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 274, Loss: 1.5492, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 275, Loss: 1.5303, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 276, Loss: 1.5445, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 277, Loss: 1.4104, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 278, Loss: 1.4918, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 279, Loss: 1.4273, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 280, Loss: 1.4187, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 281, Loss: 1.3625, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 282, Loss: 1.4270, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 283, Loss: 1.4230, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 284, Loss: 1.3779, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 285, Loss: 1.3382, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 286, Loss: 1.3253, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 287, Loss: 1.4523, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 288, Loss: 1.3545, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 289, Loss: 1.3514, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 290, Loss: 1.3631, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 291, Loss: 1.2812, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 292, Loss: 1.3228, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 293, Loss: 1.3449, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 294, Loss: 1.4055, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 295, Loss: 1.2899, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 296, Loss: 1.3960, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 297, Loss: 1.4548, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 298, Loss: 1.2579, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 299, Loss: 1.3927, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 300, Loss: 1.3351, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 301, Loss: 1.3161, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 302, Loss: 1.3560, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 303, Loss: 1.4285, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 304, Loss: 1.3129, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 305, Loss: 1.3207, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 306, Loss: 1.3133, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 307, Loss: 1.3595, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 308, Loss: 1.2398, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 309, Loss: 1.2574, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 310, Loss: 1.2968, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 311, Loss: 1.4603, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 312, Loss: 1.2792, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 313, Loss: 1.3213, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 314, Loss: 1.3193, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 315, Loss: 1.2626, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 316, Loss: 1.3511, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 317, Loss: 1.3304, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 318, Loss: 1.2561, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 319, Loss: 1.2765, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 320, Loss: 1.3133, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 321, Loss: 1.3391, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 322, Loss: 1.3310, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 323, Loss: 1.2925, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 324, Loss: 1.2241, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 325, Loss: 1.2950, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 326, Loss: 1.3130, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 327, Loss: 1.2814, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 328, Loss: 1.2658, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 329, Loss: 1.3199, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 330, Loss: 1.2960, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 331, Loss: 1.3084, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 332, Loss: 1.3005, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 333, Loss: 1.2910, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 334, Loss: 1.3332, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 335, Loss: 1.2843, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 336, Loss: 1.3715, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 337, Loss: 1.3092, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 338, Loss: 1.2669, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 339, Loss: 1.2885, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 340, Loss: 1.2902, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 341, Loss: 1.2522, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 342, Loss: 1.3034, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 343, Loss: 1.2690, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 344, Loss: 1.1906, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 345, Loss: 1.2013, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 346, Loss: 1.2946, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 347, Loss: 1.2188, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 348, Loss: 1.2055, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 349, Loss: 1.1816, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 350, Loss: 1.2465, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 351, Loss: 1.2571, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 352, Loss: 1.1736, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 353, Loss: 1.2212, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 354, Loss: 1.3244, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 355, Loss: 1.1395, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 356, Loss: 1.1888, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 357, Loss: 1.1748, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 358, Loss: 1.1873, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 359, Loss: 1.2257, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 360, Loss: 1.2016, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 361, Loss: 1.2576, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 362, Loss: 1.2085, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 363, Loss: 1.1885, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 364, Loss: 1.2457, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 365, Loss: 1.1445, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 366, Loss: 1.2719, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 367, Loss: 1.3238, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 368, Loss: 1.0849, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 369, Loss: 1.1607, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 370, Loss: 1.2296, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 371, Loss: 1.3981, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 372, Loss: 1.2530, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 373, Loss: 1.1713, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 374, Loss: 1.5772, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 375, Loss: 1.2530, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 376, Loss: 1.1798, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 377, Loss: 1.2900, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 378, Loss: 1.2487, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 379, Loss: 1.2280, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 380, Loss: 1.1066, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 381, Loss: 1.1887, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 382, Loss: 1.3142, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 383, Loss: 1.1562, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 384, Loss: 1.1815, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 385, Loss: 1.1462, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 386, Loss: 1.1750, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 387, Loss: 1.2775, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 388, Loss: 1.1033, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 389, Loss: 1.2328, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 390, Loss: 1.1899, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 391, Loss: 1.1767, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 392, Loss: 1.0789, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 393, Loss: 1.0864, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 394, Loss: 1.1338, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 395, Loss: 1.3048, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 396, Loss: 1.1392, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 397, Loss: 1.0221, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 398, Loss: 1.1944, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 399, Loss: 1.1796, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 400, Loss: 1.0257, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 401, Loss: 1.0570, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 402, Loss: 1.0887, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 403, Loss: 1.1383, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 404, Loss: 1.0390, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 405, Loss: 0.9967, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 406, Loss: 1.0590, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 407, Loss: 1.1057, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 408, Loss: 0.9355, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 409, Loss: 1.0003, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 410, Loss: 0.9526, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 411, Loss: 1.0403, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 412, Loss: 1.0273, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 413, Loss: 1.0671, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 414, Loss: 1.0009, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 415, Loss: 1.0882, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 416, Loss: 0.9577, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 417, Loss: 1.0378, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 418, Loss: 0.8989, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 419, Loss: 0.9502, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 420, Loss: 1.0308, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 421, Loss: 0.8998, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 422, Loss: 0.9276, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 423, Loss: 0.8810, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 424, Loss: 0.9272, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 425, Loss: 0.9614, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 426, Loss: 0.8399, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 427, Loss: 1.0201, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 428, Loss: 0.8566, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 429, Loss: 0.9246, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 430, Loss: 0.8430, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 431, Loss: 0.9972, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 432, Loss: 0.9563, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 433, Loss: 0.9115, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 434, Loss: 0.8979, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 435, Loss: 0.9001, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 436, Loss: 0.8500, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 437, Loss: 0.9318, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 438, Loss: 1.0037, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 439, Loss: 0.9016, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 440, Loss: 0.8403, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 441, Loss: 0.9823, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 442, Loss: 0.8824, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 443, Loss: 0.8898, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 444, Loss: 0.8781, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 445, Loss: 1.0778, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 446, Loss: 0.8047, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 447, Loss: 0.9080, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 448, Loss: 0.9409, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 449, Loss: 0.8490, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 450, Loss: 0.8635, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 451, Loss: 0.9051, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 452, Loss: 0.9715, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 453, Loss: 0.8892, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 454, Loss: 0.8172, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 455, Loss: 0.8825, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 456, Loss: 0.8826, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 457, Loss: 0.8240, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 458, Loss: 0.8230, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 459, Loss: 0.8064, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 460, Loss: 0.8142, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 461, Loss: 0.7568, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 462, Loss: 0.7759, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 463, Loss: 0.8533, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 464, Loss: 0.7551, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 465, Loss: 0.7710, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 466, Loss: 0.8433, Train 0.3837, Val 0.3538\n",
      "Fold: 1, Epoch: 467, Loss: 0.8741, Train 0.3837, Val 0.3538\n",
      "Fold: 1, Epoch: 468, Loss: 0.7333, Train 0.3837, Val 0.3538\n",
      "Fold: 1, Epoch: 469, Loss: 0.7939, Train 0.3886, Val 0.3538\n",
      "Fold: 1, Epoch: 470, Loss: 0.8099, Train 0.3911, Val 0.3538\n",
      "Fold: 1, Epoch: 471, Loss: 0.9436, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 472, Loss: 0.8227, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 473, Loss: 0.7552, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 474, Loss: 0.9477, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 475, Loss: 0.8625, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 476, Loss: 0.7605, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 477, Loss: 0.8409, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 478, Loss: 0.8309, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 479, Loss: 0.8038, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 480, Loss: 0.7649, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 481, Loss: 0.7921, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 482, Loss: 0.7006, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 483, Loss: 0.7623, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 484, Loss: 0.7592, Train 0.3837, Val 0.3538\n",
      "Fold: 1, Epoch: 485, Loss: 0.7252, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 486, Loss: 0.6679, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 487, Loss: 0.7101, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 488, Loss: 0.6964, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 489, Loss: 0.8103, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 490, Loss: 0.7119, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 491, Loss: 0.6253, Train 0.4183, Val 0.3692\n",
      "Fold: 1, Epoch: 492, Loss: 0.9005, Train 0.6460, Val 0.6615\n",
      "Fold: 1, Epoch: 493, Loss: 0.9991, Train 0.5322, Val 0.5538\n",
      "Fold: 1, Epoch: 494, Loss: 0.7429, Train 0.4183, Val 0.4000\n",
      "Fold: 1, Epoch: 495, Loss: 0.6184, Train 0.4431, Val 0.4000\n",
      "Fold: 1, Epoch: 496, Loss: 0.7328, Train 0.3515, Val 0.3385\n",
      "Fold: 1, Epoch: 497, Loss: 0.7793, Train 0.2624, Val 0.2462\n",
      "Fold: 1, Epoch: 498, Loss: 0.6595, Train 0.2723, Val 0.2308\n",
      "Fold: 1, Epoch: 499, Loss: 0.7587, Train 0.6040, Val 0.6462\n",
      "Fold: 1, Epoch: 500, Loss: 0.7684, Train 0.5842, Val 0.5846\n",
      "Fold: 1, Epoch: 501, Loss: 0.9762, Train 0.3837, Val 0.3077\n",
      "Fold: 1, Epoch: 502, Loss: 0.6544, Train 0.3292, Val 0.2615\n",
      "Fold: 1, Epoch: 503, Loss: 0.6472, Train 0.3936, Val 0.3538\n",
      "Fold: 1, Epoch: 504, Loss: 0.6488, Train 0.4728, Val 0.4000\n",
      "Fold: 1, Epoch: 505, Loss: 0.8846, Train 0.4777, Val 0.4000\n",
      "Fold: 1, Epoch: 506, Loss: 0.6277, Train 0.5000, Val 0.4462\n",
      "Fold: 1, Epoch: 507, Loss: 0.6178, Train 0.7450, Val 0.7846\n",
      "Fold: 1, Epoch: 508, Loss: 0.7400, Train 0.7723, Val 0.7846\n",
      "Fold: 1, Epoch: 509, Loss: 0.7438, Train 0.7351, Val 0.7846\n",
      "Fold: 1, Epoch: 510, Loss: 0.6719, Train 0.4802, Val 0.4154\n",
      "Fold: 1, Epoch: 511, Loss: 0.5502, Train 0.4703, Val 0.4154\n",
      "Fold: 1, Epoch: 512, Loss: 0.7090, Train 0.4505, Val 0.4154\n",
      "Fold: 1, Epoch: 513, Loss: 0.6489, Train 0.4134, Val 0.3692\n",
      "Fold: 1, Epoch: 514, Loss: 0.5961, Train 0.3490, Val 0.3385\n",
      "Fold: 1, Epoch: 515, Loss: 0.5686, Train 0.3589, Val 0.3385\n",
      "Fold: 1, Epoch: 516, Loss: 0.6387, Train 0.4505, Val 0.3692\n",
      "Fold: 1, Epoch: 517, Loss: 0.5807, Train 0.4629, Val 0.3846\n",
      "Fold: 1, Epoch: 518, Loss: 0.6060, Train 0.4208, Val 0.3538\n",
      "Fold: 1, Epoch: 519, Loss: 0.6042, Train 0.3960, Val 0.3538\n",
      "Fold: 1, Epoch: 520, Loss: 0.5709, Train 0.4035, Val 0.3538\n",
      "Fold: 1, Epoch: 521, Loss: 0.7084, Train 0.4530, Val 0.3385\n",
      "Fold: 1, Epoch: 522, Loss: 0.6360, Train 0.4876, Val 0.4000\n",
      "Fold: 1, Epoch: 523, Loss: 0.6330, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 524, Loss: 0.5471, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 525, Loss: 0.5264, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 526, Loss: 0.4523, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 527, Loss: 0.5901, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 528, Loss: 0.5289, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 529, Loss: 0.5579, Train 0.4926, Val 0.4154\n",
      "Fold: 1, Epoch: 530, Loss: 0.5751, Train 0.4926, Val 0.4154\n",
      "Fold: 1, Epoch: 531, Loss: 0.5468, Train 0.4926, Val 0.4154\n",
      "Fold: 1, Epoch: 532, Loss: 0.4626, Train 0.4827, Val 0.4154\n",
      "Fold: 1, Epoch: 533, Loss: 0.5811, Train 0.4703, Val 0.3846\n",
      "Fold: 1, Epoch: 534, Loss: 0.5313, Train 0.4134, Val 0.3692\n",
      "Fold: 1, Epoch: 535, Loss: 0.5854, Train 0.2946, Val 0.2308\n",
      "Fold: 1, Epoch: 536, Loss: 0.6055, Train 0.3391, Val 0.2615\n",
      "Fold: 1, Epoch: 537, Loss: 0.4820, Train 0.3812, Val 0.2923\n",
      "Fold: 1, Epoch: 538, Loss: 0.5167, Train 0.4084, Val 0.3231\n",
      "Fold: 1, Epoch: 539, Loss: 0.5272, Train 0.3465, Val 0.2615\n",
      "Fold: 1, Epoch: 540, Loss: 0.4369, Train 0.2772, Val 0.2000\n",
      "Fold: 1, Epoch: 541, Loss: 0.5536, Train 0.1955, Val 0.0769\n",
      "Fold: 1, Epoch: 542, Loss: 0.5522, Train 0.1460, Val 0.0923\n",
      "Fold: 1, Epoch: 543, Loss: 0.5656, Train 0.1361, Val 0.0769\n",
      "Fold: 1, Epoch: 544, Loss: 0.6059, Train 0.1584, Val 0.0769\n",
      "Fold: 1, Epoch: 545, Loss: 0.5901, Train 0.1559, Val 0.0923\n",
      "Fold: 1, Epoch: 546, Loss: 0.6215, Train 0.1807, Val 0.0769\n",
      "Fold: 1, Epoch: 547, Loss: 0.6407, Train 0.2203, Val 0.0923\n",
      "Fold: 1, Epoch: 548, Loss: 0.4486, Train 0.4678, Val 0.3846\n",
      "Fold: 1, Epoch: 549, Loss: 0.6487, Train 0.4901, Val 0.4154\n",
      "Fold: 1, Epoch: 550, Loss: 0.6017, Train 0.5941, Val 0.5385\n",
      "Fold: 1, Epoch: 551, Loss: 0.5760, Train 0.7525, Val 0.7538\n",
      "Fold: 1, Epoch: 552, Loss: 0.5611, Train 0.7871, Val 0.7846\n",
      "Fold: 1, Epoch: 553, Loss: 0.5262, Train 0.7871, Val 0.7846\n",
      "Fold: 1, Epoch: 554, Loss: 0.7195, Train 0.6386, Val 0.6000\n",
      "Fold: 1, Epoch: 555, Loss: 0.4142, Train 0.4827, Val 0.4000\n",
      "Fold: 1, Epoch: 556, Loss: 0.5612, Train 0.4455, Val 0.3692\n",
      "Fold: 1, Epoch: 557, Loss: 0.5140, Train 0.3713, Val 0.3077\n",
      "Fold: 1, Epoch: 558, Loss: 0.5279, Train 0.3045, Val 0.2615\n",
      "Fold: 1, Epoch: 559, Loss: 0.5250, Train 0.3292, Val 0.2923\n",
      "Fold: 1, Epoch: 560, Loss: 0.5448, Train 0.4579, Val 0.3846\n",
      "Fold: 1, Epoch: 561, Loss: 0.4796, Train 0.4827, Val 0.4154\n",
      "Fold: 1, Epoch: 562, Loss: 0.4198, Train 0.4851, Val 0.4154\n",
      "Fold: 1, Epoch: 563, Loss: 0.4834, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 564, Loss: 0.5019, Train 0.5446, Val 0.4769\n",
      "Fold: 1, Epoch: 565, Loss: 0.4474, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 566, Loss: 0.4776, Train 0.4851, Val 0.4154\n",
      "Fold: 1, Epoch: 567, Loss: 0.5040, Train 0.4851, Val 0.4154\n",
      "Fold: 1, Epoch: 568, Loss: 0.5521, Train 0.4851, Val 0.4154\n",
      "Fold: 1, Epoch: 569, Loss: 0.4828, Train 0.4777, Val 0.4000\n",
      "Fold: 1, Epoch: 570, Loss: 0.4171, Train 0.4653, Val 0.3846\n",
      "Fold: 1, Epoch: 571, Loss: 0.5820, Train 0.4208, Val 0.3692\n",
      "Fold: 1, Epoch: 572, Loss: 0.3742, Train 0.3094, Val 0.2462\n",
      "Fold: 1, Epoch: 573, Loss: 0.5287, Train 0.3540, Val 0.2769\n",
      "Fold: 1, Epoch: 574, Loss: 0.5824, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 575, Loss: 0.5624, Train 0.4950, Val 0.3846\n",
      "Fold: 1, Epoch: 576, Loss: 0.5333, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 577, Loss: 0.5393, Train 0.4802, Val 0.4154\n",
      "Fold: 1, Epoch: 578, Loss: 0.5836, Train 0.4802, Val 0.4154\n",
      "Fold: 1, Epoch: 579, Loss: 0.4386, Train 0.4678, Val 0.4000\n",
      "Fold: 1, Epoch: 580, Loss: 0.3911, Train 0.3738, Val 0.2923\n",
      "Fold: 1, Epoch: 581, Loss: 0.5087, Train 0.2153, Val 0.0923\n",
      "Fold: 1, Epoch: 582, Loss: 0.4557, Train 0.2030, Val 0.0923\n",
      "Fold: 1, Epoch: 583, Loss: 0.5199, Train 0.4678, Val 0.4462\n",
      "Fold: 1, Epoch: 584, Loss: 0.4323, Train 0.4851, Val 0.4462\n",
      "Fold: 1, Epoch: 585, Loss: 0.6459, Train 0.4208, Val 0.3385\n",
      "Fold: 1, Epoch: 586, Loss: 0.4795, Train 0.3911, Val 0.2923\n",
      "Fold: 1, Epoch: 587, Loss: 0.4186, Train 0.4381, Val 0.3538\n",
      "Fold: 1, Epoch: 588, Loss: 0.6628, Train 0.4703, Val 0.3846\n",
      "Fold: 1, Epoch: 589, Loss: 0.4769, Train 0.4802, Val 0.4000\n",
      "Fold: 1, Epoch: 590, Loss: 0.4759, Train 0.4802, Val 0.4000\n",
      "Fold: 1, Epoch: 591, Loss: 0.5045, Train 0.4901, Val 0.3846\n",
      "Fold: 1, Epoch: 592, Loss: 0.5020, Train 0.5050, Val 0.4000\n",
      "Fold: 1, Epoch: 593, Loss: 0.4507, Train 0.7178, Val 0.6923\n",
      "Fold: 1, Epoch: 594, Loss: 0.5853, Train 0.7277, Val 0.7231\n",
      "Fold: 1, Epoch: 595, Loss: 0.4882, Train 0.7351, Val 0.7231\n",
      "Fold: 1, Epoch: 596, Loss: 0.5392, Train 0.7203, Val 0.6769\n",
      "Fold: 1, Epoch: 597, Loss: 0.5466, Train 0.6262, Val 0.5846\n",
      "Fold: 1, Epoch: 598, Loss: 0.5610, Train 0.5817, Val 0.5077\n",
      "Fold: 1, Epoch: 599, Loss: 0.4487, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 600, Loss: 0.4432, Train 0.4950, Val 0.3846\n",
      "Fold: 1, Epoch: 601, Loss: 0.4826, Train 0.3663, Val 0.2923\n",
      "Fold: 1, Epoch: 602, Loss: 0.5711, Train 0.2129, Val 0.0923\n",
      "Fold: 1, Epoch: 603, Loss: 0.7125, Train 0.2277, Val 0.1231\n",
      "Fold: 1, Epoch: 604, Loss: 0.4792, Train 0.4332, Val 0.3692\n",
      "Fold: 1, Epoch: 605, Loss: 0.4077, Train 0.4950, Val 0.3846\n",
      "Fold: 1, Epoch: 606, Loss: 0.4854, Train 0.5421, Val 0.4615\n",
      "Fold: 1, Epoch: 607, Loss: 0.5206, Train 0.5743, Val 0.4923\n",
      "Fold: 1, Epoch: 608, Loss: 0.4392, Train 0.5743, Val 0.4769\n",
      "Fold: 1, Epoch: 609, Loss: 0.4046, Train 0.5767, Val 0.5077\n",
      "Fold: 1, Epoch: 610, Loss: 0.4000, Train 0.4728, Val 0.3692\n",
      "Fold: 1, Epoch: 611, Loss: 0.4947, Train 0.2500, Val 0.1385\n",
      "Fold: 1, Epoch: 612, Loss: 0.4605, Train 0.2475, Val 0.1077\n",
      "Fold: 1, Epoch: 613, Loss: 0.3932, Train 0.2302, Val 0.1077\n",
      "Fold: 1, Epoch: 614, Loss: 0.5136, Train 0.2178, Val 0.0923\n",
      "Fold: 1, Epoch: 615, Loss: 0.3739, Train 0.1931, Val 0.0769\n",
      "Fold: 1, Epoch: 616, Loss: 0.4795, Train 0.1832, Val 0.0769\n",
      "Fold: 1, Epoch: 617, Loss: 0.4311, Train 0.1832, Val 0.0769\n",
      "Fold: 1, Epoch: 618, Loss: 0.4246, Train 0.1708, Val 0.0769\n",
      "Fold: 1, Epoch: 619, Loss: 0.4750, Train 0.1708, Val 0.0923\n",
      "Fold: 1, Epoch: 620, Loss: 0.6266, Train 0.1832, Val 0.0923\n",
      "Fold: 1, Epoch: 621, Loss: 0.4952, Train 0.2079, Val 0.1077\n",
      "Fold: 1, Epoch: 622, Loss: 0.4219, Train 0.3837, Val 0.3538\n",
      "Fold: 1, Epoch: 623, Loss: 0.4634, Train 0.5792, Val 0.5538\n",
      "Fold: 1, Epoch: 624, Loss: 0.4679, Train 0.7970, Val 0.7692\n",
      "Fold: 1, Epoch: 625, Loss: 0.4344, Train 0.7970, Val 0.7846\n",
      "Fold: 1, Epoch: 626, Loss: 0.4926, Train 0.7847, Val 0.7846\n",
      "Fold: 1, Epoch: 627, Loss: 0.5322, Train 0.7426, Val 0.7692\n",
      "Fold: 1, Epoch: 628, Loss: 0.5407, Train 0.5866, Val 0.5385\n",
      "Fold: 1, Epoch: 629, Loss: 0.3540, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 630, Loss: 0.4520, Train 0.4728, Val 0.3846\n",
      "Fold: 1, Epoch: 631, Loss: 0.4143, Train 0.3144, Val 0.2615\n",
      "Fold: 1, Epoch: 632, Loss: 0.5116, Train 0.2550, Val 0.1692\n",
      "Fold: 1, Epoch: 633, Loss: 0.4794, Train 0.4109, Val 0.3538\n",
      "Fold: 1, Epoch: 634, Loss: 0.4807, Train 0.4752, Val 0.3846\n",
      "Fold: 1, Epoch: 635, Loss: 0.4962, Train 0.4876, Val 0.4154\n",
      "Fold: 1, Epoch: 636, Loss: 0.4411, Train 0.4802, Val 0.4154\n",
      "Fold: 1, Epoch: 637, Loss: 0.4764, Train 0.4827, Val 0.4154\n",
      "Fold: 1, Epoch: 638, Loss: 0.4394, Train 0.4901, Val 0.4154\n",
      "Fold: 1, Epoch: 639, Loss: 0.4805, Train 0.5149, Val 0.4154\n",
      "Fold: 1, Epoch: 640, Loss: 0.4270, Train 0.6634, Val 0.6308\n",
      "Fold: 1, Epoch: 641, Loss: 0.4126, Train 0.6807, Val 0.6308\n",
      "Fold: 1, Epoch: 642, Loss: 0.4025, Train 0.6485, Val 0.6308\n",
      "Fold: 1, Epoch: 643, Loss: 0.4885, Train 0.4851, Val 0.3846\n",
      "Fold: 1, Epoch: 644, Loss: 0.4818, Train 0.4604, Val 0.3538\n",
      "Fold: 1, Epoch: 645, Loss: 0.4410, Train 0.4455, Val 0.3538\n",
      "Fold: 1, Epoch: 646, Loss: 0.3763, Train 0.4505, Val 0.3538\n",
      "Fold: 1, Epoch: 647, Loss: 0.3931, Train 0.4431, Val 0.3538\n",
      "Fold: 1, Epoch: 648, Loss: 0.4768, Train 0.4703, Val 0.3846\n",
      "Fold: 1, Epoch: 649, Loss: 0.4066, Train 0.4975, Val 0.4000\n",
      "Fold: 1, Epoch: 650, Loss: 0.5059, Train 0.5891, Val 0.5538\n",
      "Fold: 1, Epoch: 651, Loss: 0.4664, Train 0.6881, Val 0.6923\n",
      "Fold: 1, Epoch: 652, Loss: 0.4064, Train 0.6040, Val 0.6462\n",
      "Fold: 1, Epoch: 653, Loss: 0.3712, Train 0.5322, Val 0.5538\n",
      "Fold: 1, Epoch: 654, Loss: 0.4312, Train 0.5272, Val 0.5385\n",
      "Fold: 1, Epoch: 655, Loss: 0.4157, Train 0.4777, Val 0.4615\n",
      "Fold: 1, Epoch: 656, Loss: 0.3084, Train 0.2946, Val 0.1846\n",
      "Fold: 1, Epoch: 657, Loss: 0.4870, Train 0.2500, Val 0.1692\n",
      "Fold: 1, Epoch: 658, Loss: 0.3088, Train 0.2921, Val 0.2462\n",
      "Fold: 1, Epoch: 659, Loss: 0.4134, Train 0.4406, Val 0.3692\n",
      "Fold: 1, Epoch: 660, Loss: 0.4082, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 661, Loss: 0.4680, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 662, Loss: 0.4748, Train 0.5025, Val 0.3846\n",
      "Fold: 1, Epoch: 663, Loss: 0.4669, Train 0.5000, Val 0.3846\n",
      "Fold: 1, Epoch: 664, Loss: 0.3787, Train 0.4926, Val 0.3846\n",
      "Fold: 1, Epoch: 665, Loss: 0.4736, Train 0.4876, Val 0.3846\n",
      "Fold: 1, Epoch: 666, Loss: 0.3794, Train 0.4728, Val 0.3846\n",
      "Fold: 1, Epoch: 667, Loss: 0.4481, Train 0.4752, Val 0.3846\n",
      "Fold: 1, Epoch: 668, Loss: 0.5116, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 669, Loss: 0.4347, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 670, Loss: 0.3352, Train 0.4876, Val 0.4154\n",
      "Fold: 1, Epoch: 671, Loss: 0.5001, Train 0.4876, Val 0.4154\n",
      "Fold: 1, Epoch: 672, Loss: 0.3742, Train 0.4876, Val 0.4154\n",
      "Fold: 1, Epoch: 673, Loss: 0.3965, Train 0.5446, Val 0.4615\n",
      "Fold: 1, Epoch: 674, Loss: 0.4574, Train 0.7327, Val 0.7231\n",
      "Fold: 1, Epoch: 675, Loss: 0.4377, Train 0.7450, Val 0.7692\n",
      "Fold: 1, Epoch: 676, Loss: 0.3526, Train 0.6782, Val 0.6308\n",
      "Fold: 1, Epoch: 677, Loss: 0.4446, Train 0.5124, Val 0.4154\n",
      "Fold: 1, Epoch: 678, Loss: 0.3720, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 679, Loss: 0.3402, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 680, Loss: 0.3915, Train 0.4876, Val 0.3846\n",
      "Fold: 1, Epoch: 681, Loss: 0.3638, Train 0.4827, Val 0.3846\n",
      "Fold: 1, Epoch: 682, Loss: 0.3150, Train 0.4752, Val 0.3846\n",
      "Fold: 1, Epoch: 683, Loss: 0.4109, Train 0.4728, Val 0.4000\n",
      "Fold: 1, Epoch: 684, Loss: 0.3203, Train 0.4703, Val 0.3846\n",
      "Fold: 1, Epoch: 685, Loss: 0.3717, Train 0.4728, Val 0.3846\n",
      "Fold: 1, Epoch: 686, Loss: 0.3648, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 687, Loss: 0.3266, Train 0.4926, Val 0.4154\n",
      "Fold: 1, Epoch: 688, Loss: 0.3258, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 689, Loss: 0.3824, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 690, Loss: 0.5966, Train 0.4926, Val 0.4154\n",
      "Fold: 1, Epoch: 691, Loss: 0.4590, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 692, Loss: 0.3107, Train 0.5050, Val 0.4000\n",
      "Fold: 1, Epoch: 693, Loss: 0.3046, Train 0.5817, Val 0.4769\n",
      "Fold: 1, Epoch: 694, Loss: 0.4482, Train 0.5371, Val 0.4462\n",
      "Fold: 1, Epoch: 695, Loss: 0.3140, Train 0.5000, Val 0.4000\n",
      "Fold: 1, Epoch: 696, Loss: 0.4637, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 697, Loss: 0.3609, Train 0.4802, Val 0.3846\n",
      "Fold: 1, Epoch: 698, Loss: 0.3759, Train 0.4802, Val 0.3846\n",
      "Fold: 1, Epoch: 699, Loss: 0.4549, Train 0.4802, Val 0.4000\n",
      "Fold: 1, Epoch: 700, Loss: 0.3907, Train 0.4777, Val 0.3846\n",
      "Fold: 1, Epoch: 701, Loss: 0.4072, Train 0.4851, Val 0.3846\n",
      "Fold: 1, Epoch: 702, Loss: 0.3623, Train 0.4926, Val 0.3846\n",
      "Fold: 1, Epoch: 703, Loss: 0.4217, Train 0.4975, Val 0.3846\n",
      "Fold: 1, Epoch: 704, Loss: 0.3431, Train 0.5000, Val 0.4000\n",
      "Fold: 1, Epoch: 705, Loss: 0.4076, Train 0.4926, Val 0.3846\n",
      "Fold: 1, Epoch: 706, Loss: 0.3582, Train 0.4876, Val 0.4000\n",
      "Fold: 1, Epoch: 707, Loss: 0.3770, Train 0.4827, Val 0.4000\n",
      "Fold: 1, Epoch: 708, Loss: 0.3888, Train 0.4579, Val 0.3846\n",
      "Fold: 1, Epoch: 709, Loss: 0.3092, Train 0.4851, Val 0.4000\n",
      "Fold: 1, Epoch: 710, Loss: 0.3685, Train 0.4876, Val 0.4000\n",
      "Fold: 1, Epoch: 711, Loss: 0.2838, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 712, Loss: 0.2828, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 713, Loss: 0.3658, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 714, Loss: 0.4296, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 715, Loss: 0.3512, Train 0.4901, Val 0.3846\n",
      "Fold: 1, Epoch: 716, Loss: 0.3402, Train 0.4901, Val 0.3846\n",
      "Fold: 1, Epoch: 717, Loss: 0.3379, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 718, Loss: 0.5024, Train 0.5000, Val 0.3846\n",
      "Fold: 1, Epoch: 719, Loss: 0.4476, Train 0.5124, Val 0.3846\n",
      "Fold: 1, Epoch: 720, Loss: 0.4127, Train 0.5619, Val 0.4308\n",
      "Fold: 1, Epoch: 721, Loss: 0.5210, Train 0.5124, Val 0.3846\n",
      "Fold: 1, Epoch: 722, Loss: 0.3153, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 723, Loss: 0.3567, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 724, Loss: 0.3711, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 725, Loss: 0.3587, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 726, Loss: 0.3933, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 727, Loss: 0.4741, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 728, Loss: 0.3923, Train 0.5000, Val 0.4000\n",
      "Fold: 1, Epoch: 729, Loss: 0.3173, Train 0.5074, Val 0.4000\n",
      "Fold: 1, Epoch: 730, Loss: 0.3393, Train 0.4950, Val 0.3846\n",
      "Fold: 1, Epoch: 731, Loss: 0.4539, Train 0.4876, Val 0.3846\n",
      "Fold: 1, Epoch: 732, Loss: 0.4492, Train 0.4876, Val 0.4000\n",
      "Fold: 1, Epoch: 733, Loss: 0.3857, Train 0.4876, Val 0.4000\n",
      "Fold: 1, Epoch: 734, Loss: 0.4841, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 735, Loss: 0.4472, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 736, Loss: 0.3330, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 737, Loss: 0.3494, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 738, Loss: 0.3280, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 739, Loss: 0.3315, Train 0.5025, Val 0.4000\n",
      "Fold: 1, Epoch: 740, Loss: 0.3822, Train 0.5025, Val 0.4000\n",
      "Fold: 1, Epoch: 741, Loss: 0.2678, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 742, Loss: 0.3899, Train 0.4975, Val 0.4000\n",
      "Fold: 1, Epoch: 743, Loss: 0.3140, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 744, Loss: 0.3484, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 745, Loss: 0.3931, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 746, Loss: 0.4530, Train 0.4802, Val 0.3846\n",
      "Fold: 1, Epoch: 747, Loss: 0.4067, Train 0.4752, Val 0.3846\n",
      "Fold: 1, Epoch: 748, Loss: 0.4373, Train 0.4728, Val 0.3846\n",
      "Fold: 1, Epoch: 749, Loss: 0.3783, Train 0.4752, Val 0.3846\n",
      "Fold: 1, Epoch: 750, Loss: 0.3640, Train 0.4802, Val 0.3846\n",
      "Fold: 1, Epoch: 751, Loss: 0.3164, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 752, Loss: 0.3894, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 753, Loss: 0.4283, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 754, Loss: 0.3518, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 755, Loss: 0.4838, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 756, Loss: 0.5348, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 757, Loss: 0.3407, Train 0.5000, Val 0.3846\n",
      "Fold: 1, Epoch: 758, Loss: 0.3189, Train 0.5198, Val 0.3846\n",
      "Fold: 1, Epoch: 759, Loss: 0.3542, Train 0.5347, Val 0.4154\n",
      "Fold: 1, Epoch: 760, Loss: 0.3614, Train 0.5223, Val 0.3846\n",
      "Fold: 1, Epoch: 761, Loss: 0.4105, Train 0.5025, Val 0.3846\n",
      "Fold: 1, Epoch: 762, Loss: 0.3769, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 763, Loss: 0.3795, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 764, Loss: 0.4285, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 765, Loss: 0.3931, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 766, Loss: 0.3649, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 767, Loss: 0.2843, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 768, Loss: 0.3255, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 769, Loss: 0.3637, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 770, Loss: 0.3758, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 771, Loss: 0.3303, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 772, Loss: 0.3730, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 773, Loss: 0.4436, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 774, Loss: 0.4034, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 775, Loss: 0.3457, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 776, Loss: 0.3333, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 777, Loss: 0.3251, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 778, Loss: 0.4065, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 779, Loss: 0.3118, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 780, Loss: 0.3156, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 781, Loss: 0.3952, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 782, Loss: 0.3773, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 783, Loss: 0.3660, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 784, Loss: 0.3228, Train 0.5099, Val 0.4154\n",
      "Fold: 1, Epoch: 785, Loss: 0.2457, Train 0.5025, Val 0.4462\n",
      "Fold: 1, Epoch: 786, Loss: 0.3268, Train 0.3688, Val 0.2769\n",
      "Fold: 1, Epoch: 787, Loss: 0.3752, Train 0.2129, Val 0.1538\n",
      "Fold: 1, Epoch: 788, Loss: 0.2879, Train 0.2129, Val 0.1692\n",
      "Fold: 1, Epoch: 789, Loss: 0.3448, Train 0.2822, Val 0.2154\n",
      "Fold: 1, Epoch: 790, Loss: 0.3136, Train 0.3515, Val 0.2615\n",
      "Fold: 1, Epoch: 791, Loss: 0.3668, Train 0.3812, Val 0.3077\n",
      "Fold: 1, Epoch: 792, Loss: 0.5007, Train 0.4851, Val 0.4615\n",
      "Fold: 1, Epoch: 793, Loss: 0.2610, Train 0.5124, Val 0.4308\n",
      "Fold: 1, Epoch: 794, Loss: 0.3349, Train 0.5025, Val 0.4000\n",
      "Fold: 1, Epoch: 795, Loss: 0.2789, Train 0.5000, Val 0.3846\n",
      "Fold: 1, Epoch: 796, Loss: 0.3333, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 797, Loss: 0.3944, Train 0.5050, Val 0.4000\n",
      "Fold: 1, Epoch: 798, Loss: 0.3654, Train 0.5074, Val 0.4000\n",
      "Fold: 1, Epoch: 799, Loss: 0.3104, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 800, Loss: 0.2970, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 801, Loss: 0.4422, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 802, Loss: 0.3598, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 803, Loss: 0.3882, Train 0.5149, Val 0.4154\n",
      "Fold: 1, Epoch: 804, Loss: 0.4153, Train 0.5173, Val 0.4154\n",
      "Fold: 1, Epoch: 805, Loss: 0.4430, Train 0.5050, Val 0.4154\n",
      "Fold: 1, Epoch: 806, Loss: 0.3878, Train 0.4950, Val 0.4308\n",
      "Fold: 1, Epoch: 807, Loss: 0.4576, Train 0.4926, Val 0.4308\n",
      "Fold: 1, Epoch: 808, Loss: 0.3849, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 809, Loss: 0.4710, Train 0.4975, Val 0.4000\n",
      "Fold: 1, Epoch: 810, Loss: 0.3365, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 811, Loss: 0.2666, Train 0.4901, Val 0.3846\n",
      "Fold: 1, Epoch: 812, Loss: 0.3448, Train 0.4950, Val 0.4000\n",
      "Fold: 1, Epoch: 813, Loss: 0.3144, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 814, Loss: 0.4332, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 815, Loss: 0.2826, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 816, Loss: 0.3704, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 817, Loss: 0.3896, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 818, Loss: 0.3910, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 819, Loss: 0.3652, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 820, Loss: 0.3093, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 821, Loss: 0.4041, Train 0.4926, Val 0.4154\n",
      "Fold: 1, Epoch: 822, Loss: 0.3201, Train 0.4926, Val 0.4154\n",
      "Fold: 1, Epoch: 823, Loss: 0.2916, Train 0.4827, Val 0.4154\n",
      "Fold: 1, Epoch: 824, Loss: 0.3591, Train 0.4777, Val 0.4000\n",
      "Fold: 1, Epoch: 825, Loss: 0.3636, Train 0.4777, Val 0.3846\n",
      "Fold: 1, Epoch: 826, Loss: 0.3998, Train 0.4901, Val 0.3846\n",
      "Fold: 1, Epoch: 827, Loss: 0.2962, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 828, Loss: 0.4612, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 829, Loss: 0.2714, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 830, Loss: 0.4405, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 831, Loss: 0.2961, Train 0.5025, Val 0.4308\n",
      "Fold: 1, Epoch: 832, Loss: 0.3598, Train 0.5025, Val 0.4308\n",
      "Fold: 1, Epoch: 833, Loss: 0.2735, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 834, Loss: 0.3106, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 835, Loss: 0.3371, Train 0.5074, Val 0.4154\n",
      "Fold: 1, Epoch: 836, Loss: 0.2939, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 837, Loss: 0.3359, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 838, Loss: 0.3441, Train 0.5198, Val 0.4308\n",
      "Fold: 1, Epoch: 839, Loss: 0.2977, Train 0.5198, Val 0.4462\n",
      "Fold: 1, Epoch: 840, Loss: 0.3090, Train 0.5099, Val 0.4308\n",
      "Fold: 1, Epoch: 841, Loss: 0.4140, Train 0.5000, Val 0.4154\n",
      "Fold: 1, Epoch: 842, Loss: 0.4301, Train 0.5074, Val 0.4308\n",
      "Fold: 1, Epoch: 843, Loss: 0.3109, Train 0.5074, Val 0.4154\n",
      "Fold: 1, Epoch: 844, Loss: 0.3693, Train 0.5099, Val 0.4154\n",
      "Fold: 1, Epoch: 845, Loss: 0.3408, Train 0.5149, Val 0.4308\n",
      "Fold: 1, Epoch: 846, Loss: 0.3659, Train 0.5099, Val 0.4308\n",
      "Fold: 1, Epoch: 847, Loss: 0.4192, Train 0.5050, Val 0.4154\n",
      "Fold: 1, Epoch: 848, Loss: 0.3437, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 849, Loss: 0.3229, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 850, Loss: 0.4193, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 851, Loss: 0.2319, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 852, Loss: 0.2952, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 853, Loss: 0.2807, Train 0.4926, Val 0.4154\n",
      "Fold: 1, Epoch: 854, Loss: 0.3175, Train 0.4653, Val 0.4154\n",
      "Fold: 1, Epoch: 855, Loss: 0.3738, Train 0.4728, Val 0.4154\n",
      "Fold: 1, Epoch: 856, Loss: 0.3315, Train 0.4777, Val 0.4154\n",
      "Fold: 1, Epoch: 857, Loss: 0.3313, Train 0.5050, Val 0.4154\n",
      "Fold: 1, Epoch: 858, Loss: 0.2873, Train 0.5050, Val 0.4154\n",
      "Fold: 1, Epoch: 859, Loss: 0.3677, Train 0.5173, Val 0.4462\n",
      "Fold: 1, Epoch: 860, Loss: 0.2382, Train 0.5272, Val 0.4615\n",
      "Fold: 1, Epoch: 861, Loss: 0.3875, Train 0.5248, Val 0.4462\n",
      "Fold: 1, Epoch: 862, Loss: 0.2719, Train 0.5198, Val 0.4462\n",
      "Fold: 1, Epoch: 863, Loss: 0.3168, Train 0.5074, Val 0.4308\n",
      "Fold: 1, Epoch: 864, Loss: 0.3174, Train 0.4876, Val 0.4154\n",
      "Fold: 1, Epoch: 865, Loss: 0.3241, Train 0.4876, Val 0.4154\n",
      "Fold: 1, Epoch: 866, Loss: 0.3901, Train 0.5149, Val 0.4462\n",
      "Fold: 1, Epoch: 867, Loss: 0.2994, Train 0.5149, Val 0.4308\n",
      "Fold: 1, Epoch: 868, Loss: 0.3194, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 869, Loss: 0.2825, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 870, Loss: 0.3604, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 871, Loss: 0.3891, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 872, Loss: 0.2971, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 873, Loss: 0.3171, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 874, Loss: 0.3652, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 875, Loss: 0.3451, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 876, Loss: 0.3833, Train 0.5050, Val 0.4154\n",
      "Fold: 1, Epoch: 877, Loss: 0.2003, Train 0.5050, Val 0.4154\n",
      "Fold: 1, Epoch: 878, Loss: 0.4225, Train 0.5050, Val 0.4154\n",
      "Fold: 1, Epoch: 879, Loss: 0.2536, Train 0.5074, Val 0.4308\n",
      "Fold: 1, Epoch: 880, Loss: 0.3519, Train 0.5074, Val 0.4308\n",
      "Fold: 1, Epoch: 881, Loss: 0.2927, Train 0.5074, Val 0.4308\n",
      "Fold: 1, Epoch: 882, Loss: 0.3302, Train 0.4975, Val 0.4308\n",
      "Fold: 1, Epoch: 883, Loss: 0.3004, Train 0.4678, Val 0.4154\n",
      "Fold: 1, Epoch: 884, Loss: 0.2838, Train 0.4554, Val 0.4154\n",
      "Fold: 1, Epoch: 885, Loss: 0.3298, Train 0.4480, Val 0.3846\n",
      "Fold: 1, Epoch: 886, Loss: 0.3770, Train 0.4752, Val 0.4000\n",
      "Fold: 1, Epoch: 887, Loss: 0.2695, Train 0.4802, Val 0.4154\n",
      "Fold: 1, Epoch: 888, Loss: 0.3102, Train 0.4950, Val 0.4308\n",
      "Fold: 1, Epoch: 889, Loss: 0.2831, Train 0.5124, Val 0.4308\n",
      "Fold: 1, Epoch: 890, Loss: 0.2870, Train 0.4926, Val 0.4615\n",
      "Fold: 1, Epoch: 891, Loss: 0.3500, Train 0.4728, Val 0.4462\n",
      "Fold: 1, Epoch: 892, Loss: 0.3149, Train 0.4728, Val 0.4615\n",
      "Fold: 1, Epoch: 893, Loss: 0.2831, Train 0.4703, Val 0.4000\n",
      "Fold: 1, Epoch: 894, Loss: 0.2993, Train 0.4381, Val 0.3692\n",
      "Fold: 1, Epoch: 895, Loss: 0.2984, Train 0.4752, Val 0.3846\n",
      "Fold: 1, Epoch: 896, Loss: 0.3364, Train 0.4282, Val 0.3692\n",
      "Fold: 1, Epoch: 897, Loss: 0.2726, Train 0.3861, Val 0.3385\n",
      "Fold: 1, Epoch: 898, Loss: 0.2785, Train 0.3564, Val 0.3077\n",
      "Fold: 1, Epoch: 899, Loss: 0.2858, Train 0.3317, Val 0.2769\n",
      "Fold: 1, Epoch: 900, Loss: 0.3532, Train 0.3490, Val 0.3077\n",
      "Fold: 1, Epoch: 901, Loss: 0.3689, Train 0.3441, Val 0.2923\n",
      "Fold: 1, Epoch: 902, Loss: 0.2456, Train 0.3589, Val 0.3385\n",
      "Fold: 1, Epoch: 903, Loss: 0.3705, Train 0.2797, Val 0.2308\n",
      "Fold: 1, Epoch: 904, Loss: 0.2987, Train 0.2649, Val 0.2308\n",
      "Fold: 1, Epoch: 905, Loss: 0.3985, Train 0.2550, Val 0.2000\n",
      "Fold: 1, Epoch: 906, Loss: 0.2595, Train 0.2748, Val 0.2154\n",
      "Fold: 1, Epoch: 907, Loss: 0.3130, Train 0.3020, Val 0.2462\n",
      "Fold: 1, Epoch: 908, Loss: 0.3530, Train 0.4480, Val 0.3846\n",
      "Fold: 1, Epoch: 909, Loss: 0.3845, Train 0.4901, Val 0.4462\n",
      "Fold: 1, Epoch: 910, Loss: 0.4561, Train 0.5074, Val 0.4308\n",
      "Fold: 1, Epoch: 911, Loss: 0.3500, Train 0.5099, Val 0.4308\n",
      "Fold: 1, Epoch: 912, Loss: 0.4425, Train 0.5050, Val 0.4308\n",
      "Fold: 1, Epoch: 913, Loss: 0.3331, Train 0.5025, Val 0.4308\n",
      "Fold: 1, Epoch: 914, Loss: 0.3108, Train 0.4728, Val 0.4308\n",
      "Fold: 1, Epoch: 915, Loss: 0.3687, Train 0.4406, Val 0.3846\n",
      "Fold: 1, Epoch: 916, Loss: 0.3405, Train 0.3614, Val 0.3077\n",
      "Fold: 1, Epoch: 917, Loss: 0.3884, Train 0.3465, Val 0.2923\n",
      "Fold: 1, Epoch: 918, Loss: 0.2868, Train 0.3589, Val 0.2923\n",
      "Fold: 1, Epoch: 919, Loss: 0.2710, Train 0.3762, Val 0.3077\n",
      "Fold: 1, Epoch: 920, Loss: 0.3646, Train 0.4208, Val 0.3231\n",
      "Fold: 1, Epoch: 921, Loss: 0.3463, Train 0.4653, Val 0.4000\n",
      "Fold: 1, Epoch: 922, Loss: 0.3220, Train 0.4827, Val 0.4462\n",
      "Fold: 1, Epoch: 923, Loss: 0.2428, Train 0.4827, Val 0.4462\n",
      "Fold: 1, Epoch: 924, Loss: 0.2842, Train 0.4678, Val 0.3846\n",
      "Fold: 1, Epoch: 925, Loss: 0.2914, Train 0.4282, Val 0.3385\n",
      "Fold: 1, Epoch: 926, Loss: 0.2775, Train 0.3515, Val 0.3231\n",
      "Fold: 1, Epoch: 927, Loss: 0.3846, Train 0.2426, Val 0.2000\n",
      "Fold: 1, Epoch: 928, Loss: 0.2952, Train 0.2153, Val 0.1692\n",
      "Fold: 1, Epoch: 929, Loss: 0.2562, Train 0.2129, Val 0.1692\n",
      "Fold: 1, Epoch: 930, Loss: 0.3792, Train 0.2129, Val 0.1692\n",
      "Fold: 1, Epoch: 931, Loss: 0.2694, Train 0.2921, Val 0.2308\n",
      "Fold: 1, Epoch: 932, Loss: 0.2654, Train 0.3688, Val 0.3077\n",
      "Fold: 1, Epoch: 933, Loss: 0.2769, Train 0.4332, Val 0.3385\n",
      "Fold: 1, Epoch: 934, Loss: 0.2866, Train 0.4554, Val 0.4154\n",
      "Fold: 1, Epoch: 935, Loss: 0.3183, Train 0.4752, Val 0.4615\n",
      "Fold: 1, Epoch: 936, Loss: 0.2920, Train 0.5025, Val 0.4462\n",
      "Fold: 1, Epoch: 937, Loss: 0.2867, Train 0.5149, Val 0.4308\n",
      "Fold: 1, Epoch: 938, Loss: 0.3334, Train 0.5099, Val 0.4154\n",
      "Fold: 1, Epoch: 939, Loss: 0.3108, Train 0.5074, Val 0.4308\n",
      "Fold: 1, Epoch: 940, Loss: 0.2844, Train 0.4975, Val 0.4308\n",
      "Fold: 1, Epoch: 941, Loss: 0.3135, Train 0.4802, Val 0.4308\n",
      "Fold: 1, Epoch: 942, Loss: 0.2623, Train 0.4579, Val 0.4308\n",
      "Fold: 1, Epoch: 943, Loss: 0.3611, Train 0.4332, Val 0.3692\n",
      "Fold: 1, Epoch: 944, Loss: 0.2218, Train 0.4257, Val 0.3538\n",
      "Fold: 1, Epoch: 945, Loss: 0.2854, Train 0.4035, Val 0.3231\n",
      "Fold: 1, Epoch: 946, Loss: 0.2071, Train 0.3688, Val 0.3077\n",
      "Fold: 1, Epoch: 947, Loss: 0.2937, Train 0.4109, Val 0.3385\n",
      "Fold: 1, Epoch: 948, Loss: 0.2367, Train 0.4703, Val 0.4462\n",
      "Fold: 1, Epoch: 949, Loss: 0.3316, Train 0.4975, Val 0.4462\n",
      "Fold: 1, Epoch: 950, Loss: 0.2794, Train 0.5173, Val 0.4308\n",
      "Fold: 1, Epoch: 951, Loss: 0.2485, Train 0.5149, Val 0.4154\n",
      "Fold: 1, Epoch: 952, Loss: 0.3880, Train 0.5149, Val 0.4000\n",
      "Fold: 1, Epoch: 953, Loss: 0.2646, Train 0.5149, Val 0.4154\n",
      "Fold: 1, Epoch: 954, Loss: 0.3498, Train 0.5223, Val 0.4462\n",
      "Fold: 1, Epoch: 955, Loss: 0.3498, Train 0.5173, Val 0.4462\n",
      "Fold: 1, Epoch: 956, Loss: 0.2454, Train 0.5000, Val 0.4308\n",
      "Fold: 1, Epoch: 957, Loss: 0.3402, Train 0.4703, Val 0.4154\n",
      "Fold: 1, Epoch: 958, Loss: 0.3518, Train 0.4926, Val 0.4308\n",
      "Fold: 1, Epoch: 959, Loss: 0.2562, Train 0.5050, Val 0.4462\n",
      "Fold: 1, Epoch: 960, Loss: 0.2638, Train 0.5198, Val 0.4308\n",
      "Fold: 1, Epoch: 961, Loss: 0.3247, Train 0.5173, Val 0.4308\n",
      "Fold: 1, Epoch: 962, Loss: 0.3289, Train 0.5149, Val 0.4308\n",
      "Fold: 1, Epoch: 963, Loss: 0.4332, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 964, Loss: 0.3465, Train 0.4827, Val 0.4154\n",
      "Fold: 1, Epoch: 965, Loss: 0.3888, Train 0.4950, Val 0.4154\n",
      "Fold: 1, Epoch: 966, Loss: 0.3041, Train 0.5025, Val 0.4154\n",
      "Fold: 1, Epoch: 967, Loss: 0.2422, Train 0.5074, Val 0.4154\n",
      "Fold: 1, Epoch: 968, Loss: 0.3236, Train 0.5173, Val 0.4154\n",
      "Fold: 1, Epoch: 969, Loss: 0.2240, Train 0.5322, Val 0.4308\n",
      "Fold: 1, Epoch: 970, Loss: 0.3011, Train 0.5347, Val 0.4308\n",
      "Fold: 1, Epoch: 971, Loss: 0.2452, Train 0.5396, Val 0.4462\n",
      "Fold: 1, Epoch: 972, Loss: 0.4202, Train 0.5347, Val 0.4462\n",
      "Fold: 1, Epoch: 973, Loss: 0.4001, Train 0.5050, Val 0.4462\n",
      "Fold: 1, Epoch: 974, Loss: 0.2522, Train 0.4876, Val 0.4462\n",
      "Fold: 1, Epoch: 975, Loss: 0.3659, Train 0.4356, Val 0.3692\n",
      "Fold: 1, Epoch: 976, Loss: 0.3301, Train 0.3960, Val 0.3231\n",
      "Fold: 1, Epoch: 977, Loss: 0.3140, Train 0.4035, Val 0.3231\n",
      "Fold: 1, Epoch: 978, Loss: 0.3024, Train 0.4604, Val 0.4308\n",
      "Fold: 1, Epoch: 979, Loss: 0.2844, Train 0.4926, Val 0.4462\n",
      "Fold: 1, Epoch: 980, Loss: 0.3634, Train 0.5173, Val 0.4308\n",
      "Fold: 1, Epoch: 981, Loss: 0.2982, Train 0.5099, Val 0.4154\n",
      "Fold: 1, Epoch: 982, Loss: 0.3970, Train 0.4975, Val 0.4154\n",
      "Fold: 1, Epoch: 983, Loss: 0.2809, Train 0.5000, Val 0.4308\n",
      "Fold: 1, Epoch: 984, Loss: 0.2798, Train 0.4975, Val 0.4308\n",
      "Fold: 1, Epoch: 985, Loss: 0.3608, Train 0.5000, Val 0.4308\n",
      "Fold: 1, Epoch: 986, Loss: 0.2930, Train 0.5074, Val 0.4308\n",
      "Fold: 1, Epoch: 987, Loss: 0.2904, Train 0.5124, Val 0.4308\n",
      "Fold: 1, Epoch: 988, Loss: 0.3229, Train 0.5248, Val 0.4462\n",
      "Fold: 1, Epoch: 989, Loss: 0.2484, Train 0.5248, Val 0.4308\n",
      "Fold: 1, Epoch: 990, Loss: 0.3428, Train 0.5198, Val 0.4462\n",
      "Fold: 1, Epoch: 991, Loss: 0.2771, Train 0.5074, Val 0.4462\n",
      "Fold: 1, Epoch: 992, Loss: 0.2389, Train 0.4950, Val 0.4308\n",
      "Fold: 1, Epoch: 993, Loss: 0.4338, Train 0.4579, Val 0.3846\n",
      "Fold: 1, Epoch: 994, Loss: 0.2462, Train 0.3837, Val 0.3231\n",
      "Fold: 1, Epoch: 995, Loss: 0.3358, Train 0.2153, Val 0.1538\n",
      "Fold: 1, Epoch: 996, Loss: 0.2759, Train 0.1683, Val 0.0769\n",
      "Fold: 1, Epoch: 997, Loss: 0.2998, Train 0.1683, Val 0.0923\n",
      "Fold: 1, Epoch: 998, Loss: 0.2857, Train 0.1708, Val 0.1077\n",
      "Fold: 1, Epoch: 999, Loss: 0.2827, Train 0.1955, Val 0.1538\n",
      "Fold: 2, Epoch: 001, Loss: 2.6318, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 002, Loss: 5.4608, Train 0.1337, Val 0.1385\n",
      "Fold: 2, Epoch: 003, Loss: 3.3197, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 004, Loss: 2.9164, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 005, Loss: 3.7918, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 006, Loss: 3.1001, Train 0.1089, Val 0.0923\n",
      "Fold: 2, Epoch: 007, Loss: 2.4963, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 008, Loss: 2.7575, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 009, Loss: 2.9815, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 010, Loss: 2.5297, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 011, Loss: 2.3259, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 012, Loss: 2.3408, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 013, Loss: 2.5021, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 014, Loss: 2.5161, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 015, Loss: 2.2968, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 016, Loss: 2.1752, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 017, Loss: 2.3291, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 018, Loss: 2.4578, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 019, Loss: 2.5711, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 020, Loss: 2.2673, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 021, Loss: 2.1503, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 022, Loss: 2.2636, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 023, Loss: 2.3391, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 024, Loss: 2.3789, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 025, Loss: 2.1973, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 026, Loss: 2.1572, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 027, Loss: 2.1695, Train 0.3094, Val 0.3692\n",
      "Fold: 2, Epoch: 028, Loss: 2.2490, Train 0.3119, Val 0.3692\n",
      "Fold: 2, Epoch: 029, Loss: 2.2210, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 030, Loss: 2.1450, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 031, Loss: 2.1368, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 032, Loss: 2.1534, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 033, Loss: 2.1862, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 034, Loss: 2.1714, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 035, Loss: 2.1464, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 036, Loss: 2.1654, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 037, Loss: 2.1537, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 038, Loss: 2.1500, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 039, Loss: 2.1518, Train 0.3762, Val 0.4000\n",
      "Fold: 2, Epoch: 040, Loss: 2.2179, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 041, Loss: 2.2135, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 042, Loss: 2.1526, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 043, Loss: 2.1710, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 044, Loss: 2.1775, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 045, Loss: 2.1908, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 046, Loss: 2.1347, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 047, Loss: 2.1585, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 048, Loss: 2.1494, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 049, Loss: 2.1791, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 050, Loss: 2.1631, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 051, Loss: 2.1470, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 052, Loss: 2.1383, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 053, Loss: 2.1695, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 054, Loss: 2.1656, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 055, Loss: 2.1460, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 056, Loss: 2.1267, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 057, Loss: 2.1492, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 058, Loss: 2.1751, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 059, Loss: 2.1951, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 060, Loss: 2.1252, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 061, Loss: 2.1378, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 062, Loss: 2.1312, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 063, Loss: 2.2055, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 064, Loss: 2.1587, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 065, Loss: 2.1270, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 066, Loss: 2.1466, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 067, Loss: 2.1530, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 068, Loss: 2.2037, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 069, Loss: 2.1273, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 070, Loss: 2.1449, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 071, Loss: 2.1267, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 072, Loss: 2.1247, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 073, Loss: 2.1469, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 074, Loss: 2.1378, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 075, Loss: 2.1295, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 076, Loss: 2.1374, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 077, Loss: 2.1283, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 078, Loss: 2.1597, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 079, Loss: 2.1317, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 080, Loss: 2.1469, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 081, Loss: 2.1513, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 082, Loss: 2.1424, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 083, Loss: 2.1392, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 084, Loss: 2.1235, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 085, Loss: 2.1375, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 086, Loss: 2.1751, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 087, Loss: 2.1238, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 088, Loss: 2.1490, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 089, Loss: 2.1505, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 090, Loss: 2.1269, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 091, Loss: 2.1303, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 092, Loss: 2.1493, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 093, Loss: 2.1171, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 094, Loss: 2.1266, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 095, Loss: 2.1325, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 096, Loss: 2.1378, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 097, Loss: 2.1374, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 098, Loss: 2.1592, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 099, Loss: 2.1174, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 100, Loss: 2.1493, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 101, Loss: 2.1423, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 102, Loss: 2.1723, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 103, Loss: 2.1346, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 104, Loss: 2.1454, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 105, Loss: 2.1320, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 106, Loss: 2.1293, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 107, Loss: 2.1486, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 108, Loss: 2.1240, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 109, Loss: 2.1154, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 110, Loss: 2.1326, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 111, Loss: 2.1249, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 112, Loss: 2.1363, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 113, Loss: 2.1147, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 114, Loss: 2.1259, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 115, Loss: 2.1163, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 116, Loss: 2.1076, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 117, Loss: 2.1289, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 118, Loss: 2.1220, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 119, Loss: 2.1286, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 120, Loss: 2.1260, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 121, Loss: 2.1032, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 122, Loss: 2.1148, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 123, Loss: 2.1276, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 124, Loss: 2.1214, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 125, Loss: 2.1164, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 126, Loss: 2.1167, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 127, Loss: 2.1144, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 128, Loss: 2.1424, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 129, Loss: 2.1236, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 130, Loss: 2.1407, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 131, Loss: 2.1005, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 132, Loss: 2.1050, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 133, Loss: 2.1089, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 134, Loss: 2.1204, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 135, Loss: 2.1277, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 136, Loss: 2.1121, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 137, Loss: 2.1139, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 138, Loss: 2.1335, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 139, Loss: 2.1008, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 140, Loss: 2.1213, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 141, Loss: 2.1077, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 142, Loss: 2.1068, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 143, Loss: 2.1019, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 144, Loss: 2.1104, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 145, Loss: 2.1097, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 146, Loss: 2.1203, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 147, Loss: 2.0968, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 148, Loss: 2.1269, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 149, Loss: 2.1266, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 150, Loss: 2.1385, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 151, Loss: 2.1103, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 152, Loss: 2.1875, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 153, Loss: 2.1298, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 154, Loss: 2.0890, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 155, Loss: 2.1239, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 156, Loss: 2.1418, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 157, Loss: 2.1438, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 158, Loss: 2.1184, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 159, Loss: 2.1476, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 160, Loss: 2.1292, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 161, Loss: 2.1219, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 162, Loss: 2.1002, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 163, Loss: 2.1256, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 164, Loss: 2.1014, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 165, Loss: 2.1078, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 166, Loss: 2.1057, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 167, Loss: 2.0954, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 168, Loss: 2.1025, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 169, Loss: 2.1082, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 170, Loss: 2.0983, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 171, Loss: 2.1092, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 172, Loss: 2.1027, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 173, Loss: 2.1188, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 174, Loss: 2.1067, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 175, Loss: 2.1072, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 176, Loss: 2.1024, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 177, Loss: 2.0984, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 178, Loss: 2.0951, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 179, Loss: 2.1023, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 180, Loss: 2.1184, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 181, Loss: 2.0928, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 182, Loss: 2.0938, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 183, Loss: 2.0992, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 184, Loss: 2.1385, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 185, Loss: 2.1040, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 186, Loss: 2.1048, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 187, Loss: 2.1162, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 188, Loss: 2.1257, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 189, Loss: 2.0915, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 190, Loss: 2.1498, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 191, Loss: 2.0987, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 192, Loss: 2.0882, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 193, Loss: 2.1236, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 194, Loss: 2.1292, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 195, Loss: 2.1204, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 196, Loss: 2.0997, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 197, Loss: 2.1109, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 198, Loss: 2.0858, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 199, Loss: 2.0980, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 200, Loss: 2.0969, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 201, Loss: 2.0931, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 202, Loss: 2.1004, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 203, Loss: 2.1182, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 204, Loss: 2.0999, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 205, Loss: 2.1125, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 206, Loss: 2.0766, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 207, Loss: 2.1086, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 208, Loss: 2.0969, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 209, Loss: 2.1076, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 210, Loss: 2.0772, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 211, Loss: 2.1069, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 212, Loss: 2.1581, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 213, Loss: 2.0731, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 214, Loss: 2.1602, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 215, Loss: 2.1152, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 216, Loss: 2.0705, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 217, Loss: 2.0830, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 218, Loss: 2.1336, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 219, Loss: 2.0909, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 220, Loss: 2.1010, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 221, Loss: 2.1152, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 222, Loss: 2.0919, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 223, Loss: 2.0763, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 224, Loss: 2.0909, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 225, Loss: 2.1109, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 226, Loss: 2.0881, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 227, Loss: 2.0397, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 228, Loss: 2.0691, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 229, Loss: 2.0916, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 230, Loss: 2.0352, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 231, Loss: 2.0451, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 232, Loss: 2.0550, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 233, Loss: 2.0623, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 234, Loss: 2.0525, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 235, Loss: 2.0198, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 236, Loss: 2.0577, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 237, Loss: 2.0308, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 238, Loss: 2.0151, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 239, Loss: 2.0281, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 240, Loss: 2.0328, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 241, Loss: 2.0135, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 242, Loss: 2.0377, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 243, Loss: 2.0460, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 244, Loss: 2.0096, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 245, Loss: 2.0514, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 246, Loss: 1.9762, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 247, Loss: 1.9892, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 248, Loss: 1.9956, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 249, Loss: 1.9819, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 250, Loss: 1.9377, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 251, Loss: 1.9190, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 252, Loss: 1.9194, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 253, Loss: 1.9396, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 254, Loss: 1.8907, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 255, Loss: 1.8915, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 256, Loss: 1.9426, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 257, Loss: 1.8824, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 258, Loss: 1.8446, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 259, Loss: 1.8180, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 260, Loss: 1.8662, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 261, Loss: 1.8107, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 262, Loss: 1.7582, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 263, Loss: 1.7285, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 264, Loss: 1.6661, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 265, Loss: 1.6356, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 266, Loss: 1.6488, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 267, Loss: 1.6939, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 268, Loss: 1.5905, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 269, Loss: 1.5249, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 270, Loss: 1.6061, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 271, Loss: 1.5027, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 272, Loss: 1.5284, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 273, Loss: 1.3402, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 274, Loss: 1.5492, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 275, Loss: 1.5303, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 276, Loss: 1.5445, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 277, Loss: 1.4104, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 278, Loss: 1.4918, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 279, Loss: 1.4273, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 280, Loss: 1.4187, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 281, Loss: 1.3625, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 282, Loss: 1.4270, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 283, Loss: 1.4230, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 284, Loss: 1.3779, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 285, Loss: 1.3382, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 286, Loss: 1.3253, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 287, Loss: 1.4523, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 288, Loss: 1.3545, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 289, Loss: 1.3514, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 290, Loss: 1.3631, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 291, Loss: 1.2812, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 292, Loss: 1.3228, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 293, Loss: 1.3449, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 294, Loss: 1.4055, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 295, Loss: 1.2899, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 296, Loss: 1.3960, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 297, Loss: 1.4548, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 298, Loss: 1.2579, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 299, Loss: 1.3927, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 300, Loss: 1.3351, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 301, Loss: 1.3161, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 302, Loss: 1.3560, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 303, Loss: 1.4285, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 304, Loss: 1.3129, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 305, Loss: 1.3207, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 306, Loss: 1.3133, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 307, Loss: 1.3595, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 308, Loss: 1.2398, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 309, Loss: 1.2574, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 310, Loss: 1.2968, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 311, Loss: 1.4603, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 312, Loss: 1.2792, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 313, Loss: 1.3213, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 314, Loss: 1.3193, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 315, Loss: 1.2626, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 316, Loss: 1.3511, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 317, Loss: 1.3304, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 318, Loss: 1.2561, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 319, Loss: 1.2765, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 320, Loss: 1.3133, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 321, Loss: 1.3391, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 322, Loss: 1.3310, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 323, Loss: 1.2925, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 324, Loss: 1.2241, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 325, Loss: 1.2950, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 326, Loss: 1.3130, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 327, Loss: 1.2814, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 328, Loss: 1.2658, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 329, Loss: 1.3199, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 330, Loss: 1.2960, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 331, Loss: 1.3084, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 332, Loss: 1.3005, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 333, Loss: 1.2910, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 334, Loss: 1.3332, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 335, Loss: 1.2843, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 336, Loss: 1.3715, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 337, Loss: 1.3092, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 338, Loss: 1.2669, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 339, Loss: 1.2885, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 340, Loss: 1.2902, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 341, Loss: 1.2522, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 342, Loss: 1.3034, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 343, Loss: 1.2690, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 344, Loss: 1.1906, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 345, Loss: 1.2013, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 346, Loss: 1.2946, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 347, Loss: 1.2188, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 348, Loss: 1.2055, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 349, Loss: 1.1816, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 350, Loss: 1.2465, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 351, Loss: 1.2571, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 352, Loss: 1.1736, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 353, Loss: 1.2212, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 354, Loss: 1.3244, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 355, Loss: 1.1395, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 356, Loss: 1.1888, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 357, Loss: 1.1748, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 358, Loss: 1.1873, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 359, Loss: 1.2257, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 360, Loss: 1.2016, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 361, Loss: 1.2576, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 362, Loss: 1.2085, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 363, Loss: 1.1885, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 364, Loss: 1.2457, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 365, Loss: 1.1445, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 366, Loss: 1.2719, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 367, Loss: 1.3238, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 368, Loss: 1.0849, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 369, Loss: 1.1607, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 370, Loss: 1.2296, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 371, Loss: 1.3981, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 372, Loss: 1.2530, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 373, Loss: 1.1713, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 374, Loss: 1.5772, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 375, Loss: 1.2530, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 376, Loss: 1.1798, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 377, Loss: 1.2900, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 378, Loss: 1.2487, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 379, Loss: 1.2280, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 380, Loss: 1.1066, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 381, Loss: 1.1887, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 382, Loss: 1.3142, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 383, Loss: 1.1562, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 384, Loss: 1.1815, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 385, Loss: 1.1462, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 386, Loss: 1.1750, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 387, Loss: 1.2775, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 388, Loss: 1.1033, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 389, Loss: 1.2328, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 390, Loss: 1.1899, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 391, Loss: 1.1767, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 392, Loss: 1.0789, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 393, Loss: 1.0864, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 394, Loss: 1.1338, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 395, Loss: 1.3048, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 396, Loss: 1.1392, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 397, Loss: 1.0221, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 398, Loss: 1.1944, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 399, Loss: 1.1796, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 400, Loss: 1.0257, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 401, Loss: 1.0570, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 402, Loss: 1.0887, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 403, Loss: 1.1383, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 404, Loss: 1.0390, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 405, Loss: 0.9967, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 406, Loss: 1.0590, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 407, Loss: 1.1057, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 408, Loss: 0.9355, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 409, Loss: 1.0003, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 410, Loss: 0.9526, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 411, Loss: 1.0403, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 412, Loss: 1.0273, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 413, Loss: 1.0671, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 414, Loss: 1.0009, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 415, Loss: 1.0882, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 416, Loss: 0.9577, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 417, Loss: 1.0378, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 418, Loss: 0.8989, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 419, Loss: 0.9502, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 420, Loss: 1.0308, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 421, Loss: 0.8998, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 422, Loss: 0.9276, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 423, Loss: 0.8810, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 424, Loss: 0.9272, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 425, Loss: 0.9614, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 426, Loss: 0.8399, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 427, Loss: 1.0201, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 428, Loss: 0.8566, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 429, Loss: 0.9246, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 430, Loss: 0.8430, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 431, Loss: 0.9972, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 432, Loss: 0.9563, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 433, Loss: 0.9115, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 434, Loss: 0.8979, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 435, Loss: 0.9001, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 436, Loss: 0.8500, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 437, Loss: 0.9318, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 438, Loss: 1.0037, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 439, Loss: 0.9016, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 440, Loss: 0.8403, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 441, Loss: 0.9823, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 442, Loss: 0.8824, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 443, Loss: 0.8898, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 444, Loss: 0.8781, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 445, Loss: 1.0778, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 446, Loss: 0.8047, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 447, Loss: 0.9080, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 448, Loss: 0.9409, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 449, Loss: 0.8490, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 450, Loss: 0.8635, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 451, Loss: 0.9051, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 452, Loss: 0.9715, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 453, Loss: 0.8892, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 454, Loss: 0.8172, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 455, Loss: 0.8825, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 456, Loss: 0.8826, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 457, Loss: 0.8240, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 458, Loss: 0.8230, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 459, Loss: 0.8064, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 460, Loss: 0.8142, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 461, Loss: 0.7568, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 462, Loss: 0.7759, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 463, Loss: 0.8533, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 464, Loss: 0.7551, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 465, Loss: 0.7710, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 466, Loss: 0.8433, Train 0.3837, Val 0.4000\n",
      "Fold: 2, Epoch: 467, Loss: 0.8741, Train 0.3837, Val 0.4000\n",
      "Fold: 2, Epoch: 468, Loss: 0.7333, Train 0.3837, Val 0.4000\n",
      "Fold: 2, Epoch: 469, Loss: 0.7939, Train 0.3886, Val 0.4000\n",
      "Fold: 2, Epoch: 470, Loss: 0.8099, Train 0.3911, Val 0.4000\n",
      "Fold: 2, Epoch: 471, Loss: 0.9436, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 472, Loss: 0.8227, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 473, Loss: 0.7552, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 474, Loss: 0.9477, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 475, Loss: 0.8625, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 476, Loss: 0.7605, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 477, Loss: 0.8409, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 478, Loss: 0.8309, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 479, Loss: 0.8038, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 480, Loss: 0.7649, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 481, Loss: 0.7921, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 482, Loss: 0.7006, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 483, Loss: 0.7623, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 484, Loss: 0.7592, Train 0.3837, Val 0.4000\n",
      "Fold: 2, Epoch: 485, Loss: 0.7252, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 486, Loss: 0.6679, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 487, Loss: 0.7101, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 488, Loss: 0.6964, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 489, Loss: 0.8103, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 490, Loss: 0.7119, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 491, Loss: 0.6253, Train 0.4183, Val 0.4462\n",
      "Fold: 2, Epoch: 492, Loss: 0.9005, Train 0.6460, Val 0.6769\n",
      "Fold: 2, Epoch: 493, Loss: 0.9991, Train 0.5322, Val 0.5692\n",
      "Fold: 2, Epoch: 494, Loss: 0.7429, Train 0.4183, Val 0.4154\n",
      "Fold: 2, Epoch: 495, Loss: 0.6184, Train 0.4431, Val 0.4308\n",
      "Fold: 2, Epoch: 496, Loss: 0.7328, Train 0.3515, Val 0.3077\n",
      "Fold: 2, Epoch: 497, Loss: 0.7793, Train 0.2624, Val 0.2308\n",
      "Fold: 2, Epoch: 498, Loss: 0.6595, Train 0.2723, Val 0.2308\n",
      "Fold: 2, Epoch: 499, Loss: 0.7587, Train 0.6040, Val 0.6154\n",
      "Fold: 2, Epoch: 500, Loss: 0.7684, Train 0.5842, Val 0.6000\n",
      "Fold: 2, Epoch: 501, Loss: 0.9762, Train 0.3837, Val 0.3231\n",
      "Fold: 2, Epoch: 502, Loss: 0.6544, Train 0.3292, Val 0.2923\n",
      "Fold: 2, Epoch: 503, Loss: 0.6472, Train 0.3936, Val 0.3538\n",
      "Fold: 2, Epoch: 504, Loss: 0.6488, Train 0.4728, Val 0.4615\n",
      "Fold: 2, Epoch: 505, Loss: 0.8846, Train 0.4777, Val 0.4769\n",
      "Fold: 2, Epoch: 506, Loss: 0.6277, Train 0.5000, Val 0.5077\n",
      "Fold: 2, Epoch: 507, Loss: 0.6178, Train 0.7450, Val 0.8000\n",
      "Fold: 2, Epoch: 508, Loss: 0.7400, Train 0.7723, Val 0.8154\n",
      "Fold: 2, Epoch: 509, Loss: 0.7438, Train 0.7351, Val 0.7692\n",
      "Fold: 2, Epoch: 510, Loss: 0.6719, Train 0.4802, Val 0.4769\n",
      "Fold: 2, Epoch: 511, Loss: 0.5502, Train 0.4703, Val 0.4769\n",
      "Fold: 2, Epoch: 512, Loss: 0.7090, Train 0.4505, Val 0.4615\n",
      "Fold: 2, Epoch: 513, Loss: 0.6489, Train 0.4134, Val 0.4154\n",
      "Fold: 2, Epoch: 514, Loss: 0.5961, Train 0.3490, Val 0.3231\n",
      "Fold: 2, Epoch: 515, Loss: 0.5686, Train 0.3589, Val 0.3231\n",
      "Fold: 2, Epoch: 516, Loss: 0.6387, Train 0.4505, Val 0.4000\n",
      "Fold: 2, Epoch: 517, Loss: 0.5807, Train 0.4629, Val 0.4615\n",
      "Fold: 2, Epoch: 518, Loss: 0.6060, Train 0.4208, Val 0.4000\n",
      "Fold: 2, Epoch: 519, Loss: 0.6042, Train 0.3960, Val 0.3692\n",
      "Fold: 2, Epoch: 520, Loss: 0.5709, Train 0.4035, Val 0.3846\n",
      "Fold: 2, Epoch: 521, Loss: 0.7084, Train 0.4530, Val 0.4615\n",
      "Fold: 2, Epoch: 522, Loss: 0.6360, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 523, Loss: 0.6330, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 524, Loss: 0.5471, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 525, Loss: 0.5264, Train 0.4950, Val 0.4769\n",
      "Fold: 2, Epoch: 526, Loss: 0.4523, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 527, Loss: 0.5901, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 528, Loss: 0.5289, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 529, Loss: 0.5579, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 530, Loss: 0.5751, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 531, Loss: 0.5468, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 532, Loss: 0.4626, Train 0.4827, Val 0.4769\n",
      "Fold: 2, Epoch: 533, Loss: 0.5811, Train 0.4703, Val 0.4615\n",
      "Fold: 2, Epoch: 534, Loss: 0.5313, Train 0.4134, Val 0.3846\n",
      "Fold: 2, Epoch: 535, Loss: 0.5854, Train 0.2946, Val 0.2154\n",
      "Fold: 2, Epoch: 536, Loss: 0.6055, Train 0.3391, Val 0.2615\n",
      "Fold: 2, Epoch: 537, Loss: 0.4820, Train 0.3812, Val 0.3077\n",
      "Fold: 2, Epoch: 538, Loss: 0.5167, Train 0.4084, Val 0.3385\n",
      "Fold: 2, Epoch: 539, Loss: 0.5272, Train 0.3465, Val 0.2769\n",
      "Fold: 2, Epoch: 540, Loss: 0.4369, Train 0.2772, Val 0.2154\n",
      "Fold: 2, Epoch: 541, Loss: 0.5536, Train 0.1955, Val 0.1385\n",
      "Fold: 2, Epoch: 542, Loss: 0.5522, Train 0.1460, Val 0.0923\n",
      "Fold: 2, Epoch: 543, Loss: 0.5656, Train 0.1361, Val 0.0923\n",
      "Fold: 2, Epoch: 544, Loss: 0.6059, Train 0.1584, Val 0.0923\n",
      "Fold: 2, Epoch: 545, Loss: 0.5901, Train 0.1559, Val 0.0923\n",
      "Fold: 2, Epoch: 546, Loss: 0.6215, Train 0.1807, Val 0.0923\n",
      "Fold: 2, Epoch: 547, Loss: 0.6407, Train 0.2203, Val 0.1385\n",
      "Fold: 2, Epoch: 548, Loss: 0.4486, Train 0.4678, Val 0.4615\n",
      "Fold: 2, Epoch: 549, Loss: 0.6487, Train 0.4901, Val 0.4769\n",
      "Fold: 2, Epoch: 550, Loss: 0.6017, Train 0.5941, Val 0.6462\n",
      "Fold: 2, Epoch: 551, Loss: 0.5760, Train 0.7525, Val 0.7692\n",
      "Fold: 2, Epoch: 552, Loss: 0.5611, Train 0.7871, Val 0.8154\n",
      "Fold: 2, Epoch: 553, Loss: 0.5262, Train 0.7871, Val 0.8154\n",
      "Fold: 2, Epoch: 554, Loss: 0.7195, Train 0.6386, Val 0.6462\n",
      "Fold: 2, Epoch: 555, Loss: 0.4142, Train 0.4827, Val 0.4769\n",
      "Fold: 2, Epoch: 556, Loss: 0.5612, Train 0.4455, Val 0.4154\n",
      "Fold: 2, Epoch: 557, Loss: 0.5140, Train 0.3713, Val 0.3385\n",
      "Fold: 2, Epoch: 558, Loss: 0.5279, Train 0.3045, Val 0.2462\n",
      "Fold: 2, Epoch: 559, Loss: 0.5250, Train 0.3292, Val 0.2615\n",
      "Fold: 2, Epoch: 560, Loss: 0.5448, Train 0.4579, Val 0.4462\n",
      "Fold: 2, Epoch: 561, Loss: 0.4796, Train 0.4827, Val 0.4615\n",
      "Fold: 2, Epoch: 562, Loss: 0.4198, Train 0.4851, Val 0.4769\n",
      "Fold: 2, Epoch: 563, Loss: 0.4834, Train 0.5000, Val 0.4769\n",
      "Fold: 2, Epoch: 564, Loss: 0.5019, Train 0.5446, Val 0.5538\n",
      "Fold: 2, Epoch: 565, Loss: 0.4474, Train 0.5025, Val 0.4769\n",
      "Fold: 2, Epoch: 566, Loss: 0.4776, Train 0.4851, Val 0.4769\n",
      "Fold: 2, Epoch: 567, Loss: 0.5040, Train 0.4851, Val 0.4769\n",
      "Fold: 2, Epoch: 568, Loss: 0.5521, Train 0.4851, Val 0.4769\n",
      "Fold: 2, Epoch: 569, Loss: 0.4828, Train 0.4777, Val 0.4615\n",
      "Fold: 2, Epoch: 570, Loss: 0.4171, Train 0.4653, Val 0.4615\n",
      "Fold: 2, Epoch: 571, Loss: 0.5820, Train 0.4208, Val 0.3846\n",
      "Fold: 2, Epoch: 572, Loss: 0.3742, Train 0.3094, Val 0.2615\n",
      "Fold: 2, Epoch: 573, Loss: 0.5287, Train 0.3540, Val 0.2923\n",
      "Fold: 2, Epoch: 574, Loss: 0.5824, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 575, Loss: 0.5624, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 576, Loss: 0.5333, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 577, Loss: 0.5393, Train 0.4802, Val 0.4615\n",
      "Fold: 2, Epoch: 578, Loss: 0.5836, Train 0.4802, Val 0.4769\n",
      "Fold: 2, Epoch: 579, Loss: 0.4386, Train 0.4678, Val 0.4462\n",
      "Fold: 2, Epoch: 580, Loss: 0.3911, Train 0.3738, Val 0.3692\n",
      "Fold: 2, Epoch: 581, Loss: 0.5087, Train 0.2153, Val 0.1385\n",
      "Fold: 2, Epoch: 582, Loss: 0.4557, Train 0.2030, Val 0.0923\n",
      "Fold: 2, Epoch: 583, Loss: 0.5199, Train 0.4678, Val 0.4308\n",
      "Fold: 2, Epoch: 584, Loss: 0.4323, Train 0.4851, Val 0.4462\n",
      "Fold: 2, Epoch: 585, Loss: 0.6459, Train 0.4208, Val 0.3692\n",
      "Fold: 2, Epoch: 586, Loss: 0.4795, Train 0.3911, Val 0.3538\n",
      "Fold: 2, Epoch: 587, Loss: 0.4186, Train 0.4381, Val 0.4154\n",
      "Fold: 2, Epoch: 588, Loss: 0.6628, Train 0.4703, Val 0.4308\n",
      "Fold: 2, Epoch: 589, Loss: 0.4769, Train 0.4802, Val 0.4615\n",
      "Fold: 2, Epoch: 590, Loss: 0.4759, Train 0.4802, Val 0.4615\n",
      "Fold: 2, Epoch: 591, Loss: 0.5045, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 592, Loss: 0.5020, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 593, Loss: 0.4507, Train 0.7178, Val 0.7077\n",
      "Fold: 2, Epoch: 594, Loss: 0.5853, Train 0.7277, Val 0.7231\n",
      "Fold: 2, Epoch: 595, Loss: 0.4882, Train 0.7351, Val 0.7385\n",
      "Fold: 2, Epoch: 596, Loss: 0.5392, Train 0.7203, Val 0.7385\n",
      "Fold: 2, Epoch: 597, Loss: 0.5466, Train 0.6262, Val 0.6615\n",
      "Fold: 2, Epoch: 598, Loss: 0.5610, Train 0.5817, Val 0.6308\n",
      "Fold: 2, Epoch: 599, Loss: 0.4487, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 600, Loss: 0.4432, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 601, Loss: 0.4826, Train 0.3663, Val 0.3385\n",
      "Fold: 2, Epoch: 602, Loss: 0.5711, Train 0.2129, Val 0.1385\n",
      "Fold: 2, Epoch: 603, Loss: 0.7125, Train 0.2277, Val 0.1538\n",
      "Fold: 2, Epoch: 604, Loss: 0.4792, Train 0.4332, Val 0.3846\n",
      "Fold: 2, Epoch: 605, Loss: 0.4077, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 606, Loss: 0.4854, Train 0.5421, Val 0.5231\n",
      "Fold: 2, Epoch: 607, Loss: 0.5206, Train 0.5743, Val 0.5538\n",
      "Fold: 2, Epoch: 608, Loss: 0.4392, Train 0.5743, Val 0.5846\n",
      "Fold: 2, Epoch: 609, Loss: 0.4046, Train 0.5767, Val 0.5846\n",
      "Fold: 2, Epoch: 610, Loss: 0.4000, Train 0.4728, Val 0.4154\n",
      "Fold: 2, Epoch: 611, Loss: 0.4947, Train 0.2500, Val 0.1385\n",
      "Fold: 2, Epoch: 612, Loss: 0.4605, Train 0.2475, Val 0.1385\n",
      "Fold: 2, Epoch: 613, Loss: 0.3932, Train 0.2302, Val 0.1385\n",
      "Fold: 2, Epoch: 614, Loss: 0.5136, Train 0.2178, Val 0.1385\n",
      "Fold: 2, Epoch: 615, Loss: 0.3739, Train 0.1931, Val 0.1231\n",
      "Fold: 2, Epoch: 616, Loss: 0.4795, Train 0.1832, Val 0.0923\n",
      "Fold: 2, Epoch: 617, Loss: 0.4311, Train 0.1832, Val 0.0923\n",
      "Fold: 2, Epoch: 618, Loss: 0.4246, Train 0.1708, Val 0.0923\n",
      "Fold: 2, Epoch: 619, Loss: 0.4750, Train 0.1708, Val 0.0923\n",
      "Fold: 2, Epoch: 620, Loss: 0.6266, Train 0.1832, Val 0.0923\n",
      "Fold: 2, Epoch: 621, Loss: 0.4952, Train 0.2079, Val 0.0923\n",
      "Fold: 2, Epoch: 622, Loss: 0.4219, Train 0.3837, Val 0.3385\n",
      "Fold: 2, Epoch: 623, Loss: 0.4634, Train 0.5792, Val 0.5231\n",
      "Fold: 2, Epoch: 624, Loss: 0.4679, Train 0.7970, Val 0.8000\n",
      "Fold: 2, Epoch: 625, Loss: 0.4344, Train 0.7970, Val 0.8154\n",
      "Fold: 2, Epoch: 626, Loss: 0.4926, Train 0.7847, Val 0.8154\n",
      "Fold: 2, Epoch: 627, Loss: 0.5322, Train 0.7426, Val 0.7385\n",
      "Fold: 2, Epoch: 628, Loss: 0.5407, Train 0.5866, Val 0.6000\n",
      "Fold: 2, Epoch: 629, Loss: 0.3540, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 630, Loss: 0.4520, Train 0.4728, Val 0.4462\n",
      "Fold: 2, Epoch: 631, Loss: 0.4143, Train 0.3144, Val 0.2462\n",
      "Fold: 2, Epoch: 632, Loss: 0.5116, Train 0.2550, Val 0.1692\n",
      "Fold: 2, Epoch: 633, Loss: 0.4794, Train 0.4109, Val 0.3692\n",
      "Fold: 2, Epoch: 634, Loss: 0.4807, Train 0.4752, Val 0.4462\n",
      "Fold: 2, Epoch: 635, Loss: 0.4962, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 636, Loss: 0.4411, Train 0.4802, Val 0.4615\n",
      "Fold: 2, Epoch: 637, Loss: 0.4764, Train 0.4827, Val 0.4615\n",
      "Fold: 2, Epoch: 638, Loss: 0.4394, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 639, Loss: 0.4805, Train 0.5149, Val 0.4769\n",
      "Fold: 2, Epoch: 640, Loss: 0.4270, Train 0.6634, Val 0.6462\n",
      "Fold: 2, Epoch: 641, Loss: 0.4126, Train 0.6807, Val 0.6769\n",
      "Fold: 2, Epoch: 642, Loss: 0.4025, Train 0.6485, Val 0.6308\n",
      "Fold: 2, Epoch: 643, Loss: 0.4885, Train 0.4851, Val 0.4154\n",
      "Fold: 2, Epoch: 644, Loss: 0.4818, Train 0.4604, Val 0.4000\n",
      "Fold: 2, Epoch: 645, Loss: 0.4410, Train 0.4455, Val 0.4000\n",
      "Fold: 2, Epoch: 646, Loss: 0.3763, Train 0.4505, Val 0.4000\n",
      "Fold: 2, Epoch: 647, Loss: 0.3931, Train 0.4431, Val 0.4000\n",
      "Fold: 2, Epoch: 648, Loss: 0.4768, Train 0.4703, Val 0.4000\n",
      "Fold: 2, Epoch: 649, Loss: 0.4066, Train 0.4975, Val 0.4308\n",
      "Fold: 2, Epoch: 650, Loss: 0.5059, Train 0.5891, Val 0.5538\n",
      "Fold: 2, Epoch: 651, Loss: 0.4664, Train 0.6881, Val 0.6923\n",
      "Fold: 2, Epoch: 652, Loss: 0.4064, Train 0.6040, Val 0.5692\n",
      "Fold: 2, Epoch: 653, Loss: 0.3712, Train 0.5322, Val 0.4769\n",
      "Fold: 2, Epoch: 654, Loss: 0.4312, Train 0.5272, Val 0.4462\n",
      "Fold: 2, Epoch: 655, Loss: 0.4157, Train 0.4777, Val 0.4154\n",
      "Fold: 2, Epoch: 656, Loss: 0.3084, Train 0.2946, Val 0.2154\n",
      "Fold: 2, Epoch: 657, Loss: 0.4870, Train 0.2500, Val 0.2000\n",
      "Fold: 2, Epoch: 658, Loss: 0.3088, Train 0.2921, Val 0.2462\n",
      "Fold: 2, Epoch: 659, Loss: 0.4134, Train 0.4406, Val 0.4000\n",
      "Fold: 2, Epoch: 660, Loss: 0.4082, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 661, Loss: 0.4680, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 662, Loss: 0.4748, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 663, Loss: 0.4669, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 664, Loss: 0.3787, Train 0.4926, Val 0.4308\n",
      "Fold: 2, Epoch: 665, Loss: 0.4736, Train 0.4876, Val 0.4308\n",
      "Fold: 2, Epoch: 666, Loss: 0.3794, Train 0.4728, Val 0.4308\n",
      "Fold: 2, Epoch: 667, Loss: 0.4481, Train 0.4752, Val 0.4308\n",
      "Fold: 2, Epoch: 668, Loss: 0.5116, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 669, Loss: 0.4347, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 670, Loss: 0.3352, Train 0.4876, Val 0.4769\n",
      "Fold: 2, Epoch: 671, Loss: 0.5001, Train 0.4876, Val 0.4769\n",
      "Fold: 2, Epoch: 672, Loss: 0.3742, Train 0.4876, Val 0.4769\n",
      "Fold: 2, Epoch: 673, Loss: 0.3965, Train 0.5446, Val 0.5077\n",
      "Fold: 2, Epoch: 674, Loss: 0.4574, Train 0.7327, Val 0.7538\n",
      "Fold: 2, Epoch: 675, Loss: 0.4377, Train 0.7450, Val 0.7538\n",
      "Fold: 2, Epoch: 676, Loss: 0.3526, Train 0.6782, Val 0.6923\n",
      "Fold: 2, Epoch: 677, Loss: 0.4446, Train 0.5124, Val 0.4769\n",
      "Fold: 2, Epoch: 678, Loss: 0.3720, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 679, Loss: 0.3402, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 680, Loss: 0.3915, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 681, Loss: 0.3638, Train 0.4827, Val 0.4462\n",
      "Fold: 2, Epoch: 682, Loss: 0.3150, Train 0.4752, Val 0.4308\n",
      "Fold: 2, Epoch: 683, Loss: 0.4109, Train 0.4728, Val 0.4308\n",
      "Fold: 2, Epoch: 684, Loss: 0.3203, Train 0.4703, Val 0.4308\n",
      "Fold: 2, Epoch: 685, Loss: 0.3717, Train 0.4728, Val 0.4308\n",
      "Fold: 2, Epoch: 686, Loss: 0.3648, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 687, Loss: 0.3266, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 688, Loss: 0.3258, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 689, Loss: 0.3824, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 690, Loss: 0.5966, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 691, Loss: 0.4590, Train 0.4950, Val 0.4769\n",
      "Fold: 2, Epoch: 692, Loss: 0.3107, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 693, Loss: 0.3046, Train 0.5817, Val 0.5385\n",
      "Fold: 2, Epoch: 694, Loss: 0.4482, Train 0.5371, Val 0.4769\n",
      "Fold: 2, Epoch: 695, Loss: 0.3140, Train 0.5000, Val 0.4308\n",
      "Fold: 2, Epoch: 696, Loss: 0.4637, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 697, Loss: 0.3609, Train 0.4802, Val 0.4615\n",
      "Fold: 2, Epoch: 698, Loss: 0.3759, Train 0.4802, Val 0.4615\n",
      "Fold: 2, Epoch: 699, Loss: 0.4549, Train 0.4802, Val 0.4462\n",
      "Fold: 2, Epoch: 700, Loss: 0.3907, Train 0.4777, Val 0.4462\n",
      "Fold: 2, Epoch: 701, Loss: 0.4072, Train 0.4851, Val 0.4615\n",
      "Fold: 2, Epoch: 702, Loss: 0.3623, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 703, Loss: 0.4217, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 704, Loss: 0.3431, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 705, Loss: 0.4076, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 706, Loss: 0.3582, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 707, Loss: 0.3770, Train 0.4827, Val 0.4462\n",
      "Fold: 2, Epoch: 708, Loss: 0.3888, Train 0.4579, Val 0.4000\n",
      "Fold: 2, Epoch: 709, Loss: 0.3092, Train 0.4851, Val 0.4462\n",
      "Fold: 2, Epoch: 710, Loss: 0.3685, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 711, Loss: 0.2838, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 712, Loss: 0.2828, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 713, Loss: 0.3658, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 714, Loss: 0.4296, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 715, Loss: 0.3512, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 716, Loss: 0.3402, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 717, Loss: 0.3379, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 718, Loss: 0.5024, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 719, Loss: 0.4476, Train 0.5124, Val 0.4615\n",
      "Fold: 2, Epoch: 720, Loss: 0.4127, Train 0.5619, Val 0.5385\n",
      "Fold: 2, Epoch: 721, Loss: 0.5210, Train 0.5124, Val 0.4615\n",
      "Fold: 2, Epoch: 722, Loss: 0.3153, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 723, Loss: 0.3567, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 724, Loss: 0.3711, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 725, Loss: 0.3587, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 726, Loss: 0.3933, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 727, Loss: 0.4741, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 728, Loss: 0.3923, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 729, Loss: 0.3173, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 730, Loss: 0.3393, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 731, Loss: 0.4539, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 732, Loss: 0.4492, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 733, Loss: 0.3857, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 734, Loss: 0.4841, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 735, Loss: 0.4472, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 736, Loss: 0.3330, Train 0.4950, Val 0.4769\n",
      "Fold: 2, Epoch: 737, Loss: 0.3494, Train 0.4950, Val 0.4769\n",
      "Fold: 2, Epoch: 738, Loss: 0.3280, Train 0.4950, Val 0.4769\n",
      "Fold: 2, Epoch: 739, Loss: 0.3315, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 740, Loss: 0.3822, Train 0.5025, Val 0.4769\n",
      "Fold: 2, Epoch: 741, Loss: 0.2678, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 742, Loss: 0.3899, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 743, Loss: 0.3140, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 744, Loss: 0.3484, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 745, Loss: 0.3931, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 746, Loss: 0.4530, Train 0.4802, Val 0.4615\n",
      "Fold: 2, Epoch: 747, Loss: 0.4067, Train 0.4752, Val 0.4308\n",
      "Fold: 2, Epoch: 748, Loss: 0.4373, Train 0.4728, Val 0.4308\n",
      "Fold: 2, Epoch: 749, Loss: 0.3783, Train 0.4752, Val 0.4308\n",
      "Fold: 2, Epoch: 750, Loss: 0.3640, Train 0.4802, Val 0.4308\n",
      "Fold: 2, Epoch: 751, Loss: 0.3164, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 752, Loss: 0.3894, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 753, Loss: 0.4283, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 754, Loss: 0.3518, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 755, Loss: 0.4838, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 756, Loss: 0.5348, Train 0.4901, Val 0.4615\n",
      "Fold: 2, Epoch: 757, Loss: 0.3407, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 758, Loss: 0.3189, Train 0.5198, Val 0.4769\n",
      "Fold: 2, Epoch: 759, Loss: 0.3542, Train 0.5347, Val 0.4923\n",
      "Fold: 2, Epoch: 760, Loss: 0.3614, Train 0.5223, Val 0.4769\n",
      "Fold: 2, Epoch: 761, Loss: 0.4105, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 762, Loss: 0.3769, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 763, Loss: 0.3795, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 764, Loss: 0.4285, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 765, Loss: 0.3931, Train 0.4950, Val 0.4769\n",
      "Fold: 2, Epoch: 766, Loss: 0.3649, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 767, Loss: 0.2843, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 768, Loss: 0.3255, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 769, Loss: 0.3637, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 770, Loss: 0.3758, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 771, Loss: 0.3303, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 772, Loss: 0.3730, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 773, Loss: 0.4436, Train 0.4926, Val 0.4769\n",
      "Fold: 2, Epoch: 774, Loss: 0.4034, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 775, Loss: 0.3457, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 776, Loss: 0.3333, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 777, Loss: 0.3251, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 778, Loss: 0.4065, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 779, Loss: 0.3118, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 780, Loss: 0.3156, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 781, Loss: 0.3952, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 782, Loss: 0.3773, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 783, Loss: 0.3660, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 784, Loss: 0.3228, Train 0.5099, Val 0.4615\n",
      "Fold: 2, Epoch: 785, Loss: 0.2457, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 786, Loss: 0.3268, Train 0.3688, Val 0.2769\n",
      "Fold: 2, Epoch: 787, Loss: 0.3752, Train 0.2129, Val 0.0923\n",
      "Fold: 2, Epoch: 788, Loss: 0.2879, Train 0.2129, Val 0.0923\n",
      "Fold: 2, Epoch: 789, Loss: 0.3448, Train 0.2822, Val 0.1385\n",
      "Fold: 2, Epoch: 790, Loss: 0.3136, Train 0.3515, Val 0.2615\n",
      "Fold: 2, Epoch: 791, Loss: 0.3668, Train 0.3812, Val 0.3538\n",
      "Fold: 2, Epoch: 792, Loss: 0.5007, Train 0.4851, Val 0.4462\n",
      "Fold: 2, Epoch: 793, Loss: 0.2610, Train 0.5124, Val 0.4615\n",
      "Fold: 2, Epoch: 794, Loss: 0.3349, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 795, Loss: 0.2789, Train 0.5000, Val 0.4462\n",
      "Fold: 2, Epoch: 796, Loss: 0.3333, Train 0.4950, Val 0.4308\n",
      "Fold: 2, Epoch: 797, Loss: 0.3944, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 798, Loss: 0.3654, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 799, Loss: 0.3104, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 800, Loss: 0.2970, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 801, Loss: 0.4422, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 802, Loss: 0.3598, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 803, Loss: 0.3882, Train 0.5149, Val 0.4615\n",
      "Fold: 2, Epoch: 804, Loss: 0.4153, Train 0.5173, Val 0.4615\n",
      "Fold: 2, Epoch: 805, Loss: 0.4430, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 806, Loss: 0.3878, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 807, Loss: 0.4576, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 808, Loss: 0.3849, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 809, Loss: 0.4710, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 810, Loss: 0.3365, Train 0.4950, Val 0.4462\n",
      "Fold: 2, Epoch: 811, Loss: 0.2666, Train 0.4901, Val 0.4308\n",
      "Fold: 2, Epoch: 812, Loss: 0.3448, Train 0.4950, Val 0.4308\n",
      "Fold: 2, Epoch: 813, Loss: 0.3144, Train 0.4975, Val 0.4462\n",
      "Fold: 2, Epoch: 814, Loss: 0.4332, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 815, Loss: 0.2826, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 816, Loss: 0.3704, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 817, Loss: 0.3896, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 818, Loss: 0.3910, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 819, Loss: 0.3652, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 820, Loss: 0.3093, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 821, Loss: 0.4041, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 822, Loss: 0.3201, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 823, Loss: 0.2916, Train 0.4827, Val 0.4615\n",
      "Fold: 2, Epoch: 824, Loss: 0.3591, Train 0.4777, Val 0.4462\n",
      "Fold: 2, Epoch: 825, Loss: 0.3636, Train 0.4777, Val 0.4154\n",
      "Fold: 2, Epoch: 826, Loss: 0.3998, Train 0.4901, Val 0.4308\n",
      "Fold: 2, Epoch: 827, Loss: 0.2962, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 828, Loss: 0.4612, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 829, Loss: 0.2714, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 830, Loss: 0.4405, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 831, Loss: 0.2961, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 832, Loss: 0.3598, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 833, Loss: 0.2735, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 834, Loss: 0.3106, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 835, Loss: 0.3371, Train 0.5074, Val 0.4462\n",
      "Fold: 2, Epoch: 836, Loss: 0.2939, Train 0.5025, Val 0.4308\n",
      "Fold: 2, Epoch: 837, Loss: 0.3359, Train 0.5000, Val 0.4308\n",
      "Fold: 2, Epoch: 838, Loss: 0.3441, Train 0.5198, Val 0.4462\n",
      "Fold: 2, Epoch: 839, Loss: 0.2977, Train 0.5198, Val 0.4615\n",
      "Fold: 2, Epoch: 840, Loss: 0.3090, Train 0.5099, Val 0.4615\n",
      "Fold: 2, Epoch: 841, Loss: 0.4140, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 842, Loss: 0.4301, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 843, Loss: 0.3109, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 844, Loss: 0.3693, Train 0.5099, Val 0.4615\n",
      "Fold: 2, Epoch: 845, Loss: 0.3408, Train 0.5149, Val 0.4462\n",
      "Fold: 2, Epoch: 846, Loss: 0.3659, Train 0.5099, Val 0.4462\n",
      "Fold: 2, Epoch: 847, Loss: 0.4192, Train 0.5050, Val 0.4462\n",
      "Fold: 2, Epoch: 848, Loss: 0.3437, Train 0.4975, Val 0.4462\n",
      "Fold: 2, Epoch: 849, Loss: 0.3229, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 850, Loss: 0.4193, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 851, Loss: 0.2319, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 852, Loss: 0.2952, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 853, Loss: 0.2807, Train 0.4926, Val 0.4615\n",
      "Fold: 2, Epoch: 854, Loss: 0.3175, Train 0.4653, Val 0.4308\n",
      "Fold: 2, Epoch: 855, Loss: 0.3738, Train 0.4728, Val 0.4308\n",
      "Fold: 2, Epoch: 856, Loss: 0.3315, Train 0.4777, Val 0.4462\n",
      "Fold: 2, Epoch: 857, Loss: 0.3313, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 858, Loss: 0.2873, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 859, Loss: 0.3677, Train 0.5173, Val 0.4615\n",
      "Fold: 2, Epoch: 860, Loss: 0.2382, Train 0.5272, Val 0.4615\n",
      "Fold: 2, Epoch: 861, Loss: 0.3875, Train 0.5248, Val 0.4615\n",
      "Fold: 2, Epoch: 862, Loss: 0.2719, Train 0.5198, Val 0.4615\n",
      "Fold: 2, Epoch: 863, Loss: 0.3168, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 864, Loss: 0.3174, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 865, Loss: 0.3241, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 866, Loss: 0.3901, Train 0.5149, Val 0.4615\n",
      "Fold: 2, Epoch: 867, Loss: 0.2994, Train 0.5149, Val 0.4615\n",
      "Fold: 2, Epoch: 868, Loss: 0.3194, Train 0.5025, Val 0.4462\n",
      "Fold: 2, Epoch: 869, Loss: 0.2825, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 870, Loss: 0.3604, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 871, Loss: 0.3891, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 872, Loss: 0.2971, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 873, Loss: 0.3171, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 874, Loss: 0.3652, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 875, Loss: 0.3451, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 876, Loss: 0.3833, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 877, Loss: 0.2003, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 878, Loss: 0.4225, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 879, Loss: 0.2536, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 880, Loss: 0.3519, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 881, Loss: 0.2927, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 882, Loss: 0.3302, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 883, Loss: 0.3004, Train 0.4678, Val 0.4462\n",
      "Fold: 2, Epoch: 884, Loss: 0.2838, Train 0.4554, Val 0.4308\n",
      "Fold: 2, Epoch: 885, Loss: 0.3298, Train 0.4480, Val 0.4308\n",
      "Fold: 2, Epoch: 886, Loss: 0.3770, Train 0.4752, Val 0.4462\n",
      "Fold: 2, Epoch: 887, Loss: 0.2695, Train 0.4802, Val 0.4462\n",
      "Fold: 2, Epoch: 888, Loss: 0.3102, Train 0.4950, Val 0.4462\n",
      "Fold: 2, Epoch: 889, Loss: 0.2831, Train 0.5124, Val 0.4615\n",
      "Fold: 2, Epoch: 890, Loss: 0.2870, Train 0.4926, Val 0.4308\n",
      "Fold: 2, Epoch: 891, Loss: 0.3500, Train 0.4728, Val 0.4154\n",
      "Fold: 2, Epoch: 892, Loss: 0.3149, Train 0.4728, Val 0.4154\n",
      "Fold: 2, Epoch: 893, Loss: 0.2831, Train 0.4703, Val 0.4462\n",
      "Fold: 2, Epoch: 894, Loss: 0.2993, Train 0.4381, Val 0.3846\n",
      "Fold: 2, Epoch: 895, Loss: 0.2984, Train 0.4752, Val 0.4000\n",
      "Fold: 2, Epoch: 896, Loss: 0.3364, Train 0.4282, Val 0.3846\n",
      "Fold: 2, Epoch: 897, Loss: 0.2726, Train 0.3861, Val 0.3538\n",
      "Fold: 2, Epoch: 898, Loss: 0.2785, Train 0.3564, Val 0.3231\n",
      "Fold: 2, Epoch: 899, Loss: 0.2858, Train 0.3317, Val 0.3077\n",
      "Fold: 2, Epoch: 900, Loss: 0.3532, Train 0.3490, Val 0.3077\n",
      "Fold: 2, Epoch: 901, Loss: 0.3689, Train 0.3441, Val 0.2923\n",
      "Fold: 2, Epoch: 902, Loss: 0.2456, Train 0.3589, Val 0.2923\n",
      "Fold: 2, Epoch: 903, Loss: 0.3705, Train 0.2797, Val 0.2308\n",
      "Fold: 2, Epoch: 904, Loss: 0.2987, Train 0.2649, Val 0.1846\n",
      "Fold: 2, Epoch: 905, Loss: 0.3985, Train 0.2550, Val 0.1846\n",
      "Fold: 2, Epoch: 906, Loss: 0.2595, Train 0.2748, Val 0.2000\n",
      "Fold: 2, Epoch: 907, Loss: 0.3130, Train 0.3020, Val 0.2923\n",
      "Fold: 2, Epoch: 908, Loss: 0.3530, Train 0.4480, Val 0.4154\n",
      "Fold: 2, Epoch: 909, Loss: 0.3845, Train 0.4901, Val 0.4308\n",
      "Fold: 2, Epoch: 910, Loss: 0.4561, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 911, Loss: 0.3500, Train 0.5099, Val 0.4615\n",
      "Fold: 2, Epoch: 912, Loss: 0.4425, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 913, Loss: 0.3331, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 914, Loss: 0.3108, Train 0.4728, Val 0.4308\n",
      "Fold: 2, Epoch: 915, Loss: 0.3687, Train 0.4406, Val 0.3846\n",
      "Fold: 2, Epoch: 916, Loss: 0.3405, Train 0.3614, Val 0.3077\n",
      "Fold: 2, Epoch: 917, Loss: 0.3884, Train 0.3465, Val 0.2923\n",
      "Fold: 2, Epoch: 918, Loss: 0.2868, Train 0.3589, Val 0.2923\n",
      "Fold: 2, Epoch: 919, Loss: 0.2710, Train 0.3762, Val 0.3231\n",
      "Fold: 2, Epoch: 920, Loss: 0.3646, Train 0.4208, Val 0.3692\n",
      "Fold: 2, Epoch: 921, Loss: 0.3463, Train 0.4653, Val 0.4308\n",
      "Fold: 2, Epoch: 922, Loss: 0.3220, Train 0.4827, Val 0.4308\n",
      "Fold: 2, Epoch: 923, Loss: 0.2428, Train 0.4827, Val 0.4308\n",
      "Fold: 2, Epoch: 924, Loss: 0.2842, Train 0.4678, Val 0.4308\n",
      "Fold: 2, Epoch: 925, Loss: 0.2914, Train 0.4282, Val 0.3846\n",
      "Fold: 2, Epoch: 926, Loss: 0.2775, Train 0.3515, Val 0.2769\n",
      "Fold: 2, Epoch: 927, Loss: 0.3846, Train 0.2426, Val 0.1385\n",
      "Fold: 2, Epoch: 928, Loss: 0.2952, Train 0.2153, Val 0.1231\n",
      "Fold: 2, Epoch: 929, Loss: 0.2562, Train 0.2129, Val 0.1231\n",
      "Fold: 2, Epoch: 930, Loss: 0.3792, Train 0.2129, Val 0.1231\n",
      "Fold: 2, Epoch: 931, Loss: 0.2694, Train 0.2921, Val 0.2462\n",
      "Fold: 2, Epoch: 932, Loss: 0.2654, Train 0.3688, Val 0.2923\n",
      "Fold: 2, Epoch: 933, Loss: 0.2769, Train 0.4332, Val 0.3846\n",
      "Fold: 2, Epoch: 934, Loss: 0.2866, Train 0.4554, Val 0.4000\n",
      "Fold: 2, Epoch: 935, Loss: 0.3183, Train 0.4752, Val 0.4308\n",
      "Fold: 2, Epoch: 936, Loss: 0.2920, Train 0.5025, Val 0.4462\n",
      "Fold: 2, Epoch: 937, Loss: 0.2867, Train 0.5149, Val 0.4615\n",
      "Fold: 2, Epoch: 938, Loss: 0.3334, Train 0.5099, Val 0.4615\n",
      "Fold: 2, Epoch: 939, Loss: 0.3108, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 940, Loss: 0.2844, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 941, Loss: 0.3135, Train 0.4802, Val 0.4462\n",
      "Fold: 2, Epoch: 942, Loss: 0.2623, Train 0.4579, Val 0.4462\n",
      "Fold: 2, Epoch: 943, Loss: 0.3611, Train 0.4332, Val 0.4154\n",
      "Fold: 2, Epoch: 944, Loss: 0.2218, Train 0.4257, Val 0.3846\n",
      "Fold: 2, Epoch: 945, Loss: 0.2854, Train 0.4035, Val 0.3846\n",
      "Fold: 2, Epoch: 946, Loss: 0.2071, Train 0.3688, Val 0.3385\n",
      "Fold: 2, Epoch: 947, Loss: 0.2937, Train 0.4109, Val 0.3846\n",
      "Fold: 2, Epoch: 948, Loss: 0.2367, Train 0.4703, Val 0.4308\n",
      "Fold: 2, Epoch: 949, Loss: 0.3316, Train 0.4975, Val 0.4462\n",
      "Fold: 2, Epoch: 950, Loss: 0.2794, Train 0.5173, Val 0.4615\n",
      "Fold: 2, Epoch: 951, Loss: 0.2485, Train 0.5149, Val 0.4615\n",
      "Fold: 2, Epoch: 952, Loss: 0.3880, Train 0.5149, Val 0.4769\n",
      "Fold: 2, Epoch: 953, Loss: 0.2646, Train 0.5149, Val 0.4615\n",
      "Fold: 2, Epoch: 954, Loss: 0.3498, Train 0.5223, Val 0.4615\n",
      "Fold: 2, Epoch: 955, Loss: 0.3498, Train 0.5173, Val 0.4615\n",
      "Fold: 2, Epoch: 956, Loss: 0.2454, Train 0.5000, Val 0.4615\n",
      "Fold: 2, Epoch: 957, Loss: 0.3402, Train 0.4703, Val 0.4462\n",
      "Fold: 2, Epoch: 958, Loss: 0.3518, Train 0.4926, Val 0.4462\n",
      "Fold: 2, Epoch: 959, Loss: 0.2562, Train 0.5050, Val 0.4615\n",
      "Fold: 2, Epoch: 960, Loss: 0.2638, Train 0.5198, Val 0.4615\n",
      "Fold: 2, Epoch: 961, Loss: 0.3247, Train 0.5173, Val 0.4615\n",
      "Fold: 2, Epoch: 962, Loss: 0.3289, Train 0.5149, Val 0.4615\n",
      "Fold: 2, Epoch: 963, Loss: 0.4332, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 964, Loss: 0.3465, Train 0.4827, Val 0.4615\n",
      "Fold: 2, Epoch: 965, Loss: 0.3888, Train 0.4950, Val 0.4615\n",
      "Fold: 2, Epoch: 966, Loss: 0.3041, Train 0.5025, Val 0.4615\n",
      "Fold: 2, Epoch: 967, Loss: 0.2422, Train 0.5074, Val 0.4615\n",
      "Fold: 2, Epoch: 968, Loss: 0.3236, Train 0.5173, Val 0.4615\n",
      "Fold: 2, Epoch: 969, Loss: 0.2240, Train 0.5322, Val 0.4615\n",
      "Fold: 2, Epoch: 970, Loss: 0.3011, Train 0.5347, Val 0.4462\n",
      "Fold: 2, Epoch: 971, Loss: 0.2452, Train 0.5396, Val 0.4615\n",
      "Fold: 2, Epoch: 972, Loss: 0.4202, Train 0.5347, Val 0.4615\n",
      "Fold: 2, Epoch: 973, Loss: 0.4001, Train 0.5050, Val 0.4462\n",
      "Fold: 2, Epoch: 974, Loss: 0.2522, Train 0.4876, Val 0.4462\n",
      "Fold: 2, Epoch: 975, Loss: 0.3659, Train 0.4356, Val 0.3692\n",
      "Fold: 2, Epoch: 976, Loss: 0.3301, Train 0.3960, Val 0.3231\n",
      "Fold: 2, Epoch: 977, Loss: 0.3140, Train 0.4035, Val 0.3538\n",
      "Fold: 2, Epoch: 978, Loss: 0.3024, Train 0.4604, Val 0.4154\n",
      "Fold: 2, Epoch: 979, Loss: 0.2844, Train 0.4926, Val 0.4308\n",
      "Fold: 2, Epoch: 980, Loss: 0.3634, Train 0.5173, Val 0.4615\n",
      "Fold: 2, Epoch: 981, Loss: 0.2982, Train 0.5099, Val 0.4615\n",
      "Fold: 2, Epoch: 982, Loss: 0.3970, Train 0.4975, Val 0.4615\n",
      "Fold: 2, Epoch: 983, Loss: 0.2809, Train 0.5000, Val 0.4769\n",
      "Fold: 2, Epoch: 984, Loss: 0.2798, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 985, Loss: 0.3608, Train 0.5000, Val 0.4769\n",
      "Fold: 2, Epoch: 986, Loss: 0.2930, Train 0.5074, Val 0.4769\n",
      "Fold: 2, Epoch: 987, Loss: 0.2904, Train 0.5124, Val 0.4615\n",
      "Fold: 2, Epoch: 988, Loss: 0.3229, Train 0.5248, Val 0.4615\n",
      "Fold: 2, Epoch: 989, Loss: 0.2484, Train 0.5248, Val 0.4615\n",
      "Fold: 2, Epoch: 990, Loss: 0.3428, Train 0.5198, Val 0.4615\n",
      "Fold: 2, Epoch: 991, Loss: 0.2771, Train 0.5074, Val 0.4462\n",
      "Fold: 2, Epoch: 992, Loss: 0.2389, Train 0.4950, Val 0.4462\n",
      "Fold: 2, Epoch: 993, Loss: 0.4338, Train 0.4579, Val 0.4000\n",
      "Fold: 2, Epoch: 994, Loss: 0.2462, Train 0.3837, Val 0.2769\n",
      "Fold: 2, Epoch: 995, Loss: 0.3358, Train 0.2153, Val 0.1385\n",
      "Fold: 2, Epoch: 996, Loss: 0.2759, Train 0.1683, Val 0.1231\n",
      "Fold: 2, Epoch: 997, Loss: 0.2998, Train 0.1683, Val 0.1231\n",
      "Fold: 2, Epoch: 998, Loss: 0.2857, Train 0.1708, Val 0.1231\n",
      "Fold: 2, Epoch: 999, Loss: 0.2827, Train 0.1955, Val 0.1231\n",
      "Fold: 3, Epoch: 001, Loss: 2.6318, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 002, Loss: 5.4608, Train 0.1337, Val 0.1846\n",
      "Fold: 3, Epoch: 003, Loss: 3.3197, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 004, Loss: 2.9164, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 005, Loss: 3.7918, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 006, Loss: 3.1001, Train 0.1089, Val 0.0923\n",
      "Fold: 3, Epoch: 007, Loss: 2.4963, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 008, Loss: 2.7575, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 009, Loss: 2.9815, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 010, Loss: 2.5297, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 011, Loss: 2.3259, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 012, Loss: 2.3408, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 013, Loss: 2.5021, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 014, Loss: 2.5161, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 015, Loss: 2.2968, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 016, Loss: 2.1752, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 017, Loss: 2.3291, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 018, Loss: 2.4578, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 019, Loss: 2.5711, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 020, Loss: 2.2673, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 021, Loss: 2.1503, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 022, Loss: 2.2636, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 023, Loss: 2.3391, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 024, Loss: 2.3789, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 025, Loss: 2.1973, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 026, Loss: 2.1572, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 027, Loss: 2.1695, Train 0.3094, Val 0.2462\n",
      "Fold: 3, Epoch: 028, Loss: 2.2490, Train 0.3119, Val 0.2462\n",
      "Fold: 3, Epoch: 029, Loss: 2.2210, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 030, Loss: 2.1450, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 031, Loss: 2.1368, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 032, Loss: 2.1534, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 033, Loss: 2.1862, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 034, Loss: 2.1714, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 035, Loss: 2.1464, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 036, Loss: 2.1654, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 037, Loss: 2.1537, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 038, Loss: 2.1500, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 039, Loss: 2.1518, Train 0.3762, Val 0.3692\n",
      "Fold: 3, Epoch: 040, Loss: 2.2179, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 041, Loss: 2.2135, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 042, Loss: 2.1526, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 043, Loss: 2.1710, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 044, Loss: 2.1775, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 045, Loss: 2.1908, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 046, Loss: 2.1347, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 047, Loss: 2.1585, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 048, Loss: 2.1494, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 049, Loss: 2.1791, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 050, Loss: 2.1631, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 051, Loss: 2.1470, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 052, Loss: 2.1383, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 053, Loss: 2.1695, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 054, Loss: 2.1656, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 055, Loss: 2.1460, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 056, Loss: 2.1267, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 057, Loss: 2.1492, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 058, Loss: 2.1751, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 059, Loss: 2.1951, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 060, Loss: 2.1252, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 061, Loss: 2.1378, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 062, Loss: 2.1312, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 063, Loss: 2.2055, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 064, Loss: 2.1587, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 065, Loss: 2.1270, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 066, Loss: 2.1466, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 067, Loss: 2.1530, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 068, Loss: 2.2037, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 069, Loss: 2.1273, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 070, Loss: 2.1449, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 071, Loss: 2.1267, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 072, Loss: 2.1247, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 073, Loss: 2.1469, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 074, Loss: 2.1378, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 075, Loss: 2.1295, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 076, Loss: 2.1374, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 077, Loss: 2.1283, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 078, Loss: 2.1597, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 079, Loss: 2.1317, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 080, Loss: 2.1469, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 081, Loss: 2.1513, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 082, Loss: 2.1424, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 083, Loss: 2.1392, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 084, Loss: 2.1235, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 085, Loss: 2.1375, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 086, Loss: 2.1751, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 087, Loss: 2.1238, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 088, Loss: 2.1490, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 089, Loss: 2.1505, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 090, Loss: 2.1269, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 091, Loss: 2.1303, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 092, Loss: 2.1493, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 093, Loss: 2.1171, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 094, Loss: 2.1266, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 095, Loss: 2.1325, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 096, Loss: 2.1378, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 097, Loss: 2.1374, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 098, Loss: 2.1592, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 099, Loss: 2.1174, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 100, Loss: 2.1493, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 101, Loss: 2.1423, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 102, Loss: 2.1723, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 103, Loss: 2.1346, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 104, Loss: 2.1454, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 105, Loss: 2.1320, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 106, Loss: 2.1293, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 107, Loss: 2.1486, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 108, Loss: 2.1240, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 109, Loss: 2.1154, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 110, Loss: 2.1326, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 111, Loss: 2.1249, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 112, Loss: 2.1363, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 113, Loss: 2.1147, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 114, Loss: 2.1259, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 115, Loss: 2.1163, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 116, Loss: 2.1076, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 117, Loss: 2.1289, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 118, Loss: 2.1220, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 119, Loss: 2.1286, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 120, Loss: 2.1260, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 121, Loss: 2.1032, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 122, Loss: 2.1148, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 123, Loss: 2.1276, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 124, Loss: 2.1214, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 125, Loss: 2.1164, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 126, Loss: 2.1167, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 127, Loss: 2.1144, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 128, Loss: 2.1424, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 129, Loss: 2.1236, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 130, Loss: 2.1407, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 131, Loss: 2.1005, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 132, Loss: 2.1050, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 133, Loss: 2.1089, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 134, Loss: 2.1204, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 135, Loss: 2.1277, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 136, Loss: 2.1121, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 137, Loss: 2.1139, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 138, Loss: 2.1335, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 139, Loss: 2.1008, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 140, Loss: 2.1213, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 141, Loss: 2.1077, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 142, Loss: 2.1068, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 143, Loss: 2.1019, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 144, Loss: 2.1104, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 145, Loss: 2.1097, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 146, Loss: 2.1203, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 147, Loss: 2.0968, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 148, Loss: 2.1269, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 149, Loss: 2.1266, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 150, Loss: 2.1385, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 151, Loss: 2.1103, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 152, Loss: 2.1875, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 153, Loss: 2.1298, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 154, Loss: 2.0890, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 155, Loss: 2.1239, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 156, Loss: 2.1418, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 157, Loss: 2.1438, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 158, Loss: 2.1184, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 159, Loss: 2.1476, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 160, Loss: 2.1292, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 161, Loss: 2.1219, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 162, Loss: 2.1002, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 163, Loss: 2.1256, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 164, Loss: 2.1014, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 165, Loss: 2.1078, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 166, Loss: 2.1057, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 167, Loss: 2.0954, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 168, Loss: 2.1025, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 169, Loss: 2.1082, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 170, Loss: 2.0983, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 171, Loss: 2.1092, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 172, Loss: 2.1027, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 173, Loss: 2.1188, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 174, Loss: 2.1067, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 175, Loss: 2.1072, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 176, Loss: 2.1024, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 177, Loss: 2.0984, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 178, Loss: 2.0951, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 179, Loss: 2.1023, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 180, Loss: 2.1184, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 181, Loss: 2.0928, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 182, Loss: 2.0938, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 183, Loss: 2.0992, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 184, Loss: 2.1385, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 185, Loss: 2.1040, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 186, Loss: 2.1048, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 187, Loss: 2.1162, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 188, Loss: 2.1257, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 189, Loss: 2.0915, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 190, Loss: 2.1498, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 191, Loss: 2.0987, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 192, Loss: 2.0882, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 193, Loss: 2.1236, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 194, Loss: 2.1292, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 195, Loss: 2.1204, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 196, Loss: 2.0997, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 197, Loss: 2.1109, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 198, Loss: 2.0858, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 199, Loss: 2.0980, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 200, Loss: 2.0969, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 201, Loss: 2.0931, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 202, Loss: 2.1004, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 203, Loss: 2.1182, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 204, Loss: 2.0999, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 205, Loss: 2.1125, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 206, Loss: 2.0766, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 207, Loss: 2.1086, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 208, Loss: 2.0969, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 209, Loss: 2.1076, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 210, Loss: 2.0772, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 211, Loss: 2.1069, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 212, Loss: 2.1581, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 213, Loss: 2.0731, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 214, Loss: 2.1602, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 215, Loss: 2.1152, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 216, Loss: 2.0705, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 217, Loss: 2.0830, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 218, Loss: 2.1336, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 219, Loss: 2.0909, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 220, Loss: 2.1010, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 221, Loss: 2.1152, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 222, Loss: 2.0919, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 223, Loss: 2.0763, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 224, Loss: 2.0909, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 225, Loss: 2.1109, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 226, Loss: 2.0881, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 227, Loss: 2.0397, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 228, Loss: 2.0691, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 229, Loss: 2.0916, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 230, Loss: 2.0352, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 231, Loss: 2.0451, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 232, Loss: 2.0550, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 233, Loss: 2.0623, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 234, Loss: 2.0525, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 235, Loss: 2.0198, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 236, Loss: 2.0577, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 237, Loss: 2.0308, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 238, Loss: 2.0151, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 239, Loss: 2.0281, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 240, Loss: 2.0328, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 241, Loss: 2.0135, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 242, Loss: 2.0377, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 243, Loss: 2.0460, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 244, Loss: 2.0096, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 245, Loss: 2.0514, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 246, Loss: 1.9762, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 247, Loss: 1.9892, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 248, Loss: 1.9956, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 249, Loss: 1.9819, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 250, Loss: 1.9377, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 251, Loss: 1.9190, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 252, Loss: 1.9194, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 253, Loss: 1.9396, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 254, Loss: 1.8907, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 255, Loss: 1.8915, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 256, Loss: 1.9426, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 257, Loss: 1.8824, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 258, Loss: 1.8446, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 259, Loss: 1.8180, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 260, Loss: 1.8662, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 261, Loss: 1.8107, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 262, Loss: 1.7582, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 263, Loss: 1.7285, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 264, Loss: 1.6661, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 265, Loss: 1.6356, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 266, Loss: 1.6488, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 267, Loss: 1.6939, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 268, Loss: 1.5905, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 269, Loss: 1.5249, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 270, Loss: 1.6061, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 271, Loss: 1.5027, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 272, Loss: 1.5284, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 273, Loss: 1.3402, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 274, Loss: 1.5492, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 275, Loss: 1.5303, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 276, Loss: 1.5445, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 277, Loss: 1.4104, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 278, Loss: 1.4918, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 279, Loss: 1.4273, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 280, Loss: 1.4187, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 281, Loss: 1.3625, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 282, Loss: 1.4270, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 283, Loss: 1.4230, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 284, Loss: 1.3779, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 285, Loss: 1.3382, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 286, Loss: 1.3253, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 287, Loss: 1.4523, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 288, Loss: 1.3545, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 289, Loss: 1.3514, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 290, Loss: 1.3631, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 291, Loss: 1.2812, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 292, Loss: 1.3228, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 293, Loss: 1.3449, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 294, Loss: 1.4055, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 295, Loss: 1.2899, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 296, Loss: 1.3960, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 297, Loss: 1.4548, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 298, Loss: 1.2579, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 299, Loss: 1.3927, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 300, Loss: 1.3351, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 301, Loss: 1.3161, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 302, Loss: 1.3560, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 303, Loss: 1.4285, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 304, Loss: 1.3129, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 305, Loss: 1.3207, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 306, Loss: 1.3133, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 307, Loss: 1.3595, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 308, Loss: 1.2398, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 309, Loss: 1.2574, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 310, Loss: 1.2968, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 311, Loss: 1.4603, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 312, Loss: 1.2792, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 313, Loss: 1.3213, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 314, Loss: 1.3193, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 315, Loss: 1.2626, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 316, Loss: 1.3511, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 317, Loss: 1.3304, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 318, Loss: 1.2561, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 319, Loss: 1.2765, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 320, Loss: 1.3133, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 321, Loss: 1.3391, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 322, Loss: 1.3310, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 323, Loss: 1.2925, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 324, Loss: 1.2241, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 325, Loss: 1.2950, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 326, Loss: 1.3130, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 327, Loss: 1.2814, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 328, Loss: 1.2658, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 329, Loss: 1.3199, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 330, Loss: 1.2960, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 331, Loss: 1.3084, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 332, Loss: 1.3005, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 333, Loss: 1.2910, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 334, Loss: 1.3332, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 335, Loss: 1.2843, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 336, Loss: 1.3715, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 337, Loss: 1.3092, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 338, Loss: 1.2669, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 339, Loss: 1.2885, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 340, Loss: 1.2902, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 341, Loss: 1.2522, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 342, Loss: 1.3034, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 343, Loss: 1.2690, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 344, Loss: 1.1906, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 345, Loss: 1.2013, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 346, Loss: 1.2946, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 347, Loss: 1.2188, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 348, Loss: 1.2055, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 349, Loss: 1.1816, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 350, Loss: 1.2465, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 351, Loss: 1.2571, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 352, Loss: 1.1736, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 353, Loss: 1.2212, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 354, Loss: 1.3244, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 355, Loss: 1.1395, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 356, Loss: 1.1888, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 357, Loss: 1.1748, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 358, Loss: 1.1873, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 359, Loss: 1.2257, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 360, Loss: 1.2016, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 361, Loss: 1.2576, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 362, Loss: 1.2085, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 363, Loss: 1.1885, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 364, Loss: 1.2457, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 365, Loss: 1.1445, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 366, Loss: 1.2719, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 367, Loss: 1.3238, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 368, Loss: 1.0849, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 369, Loss: 1.1607, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 370, Loss: 1.2296, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 371, Loss: 1.3981, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 372, Loss: 1.2530, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 373, Loss: 1.1713, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 374, Loss: 1.5772, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 375, Loss: 1.2530, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 376, Loss: 1.1798, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 377, Loss: 1.2900, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 378, Loss: 1.2487, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 379, Loss: 1.2280, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 380, Loss: 1.1066, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 381, Loss: 1.1887, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 382, Loss: 1.3142, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 383, Loss: 1.1562, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 384, Loss: 1.1815, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 385, Loss: 1.1462, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 386, Loss: 1.1750, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 387, Loss: 1.2775, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 388, Loss: 1.1033, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 389, Loss: 1.2328, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 390, Loss: 1.1899, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 391, Loss: 1.1767, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 392, Loss: 1.0789, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 393, Loss: 1.0864, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 394, Loss: 1.1338, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 395, Loss: 1.3048, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 396, Loss: 1.1392, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 397, Loss: 1.0221, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 398, Loss: 1.1944, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 399, Loss: 1.1796, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 400, Loss: 1.0257, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 401, Loss: 1.0570, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 402, Loss: 1.0887, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 403, Loss: 1.1383, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 404, Loss: 1.0390, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 405, Loss: 0.9967, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 406, Loss: 1.0590, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 407, Loss: 1.1057, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 408, Loss: 0.9355, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 409, Loss: 1.0003, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 410, Loss: 0.9526, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 411, Loss: 1.0403, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 412, Loss: 1.0273, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 413, Loss: 1.0671, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 414, Loss: 1.0009, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 415, Loss: 1.0882, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 416, Loss: 0.9577, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 417, Loss: 1.0378, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 418, Loss: 0.8989, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 419, Loss: 0.9502, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 420, Loss: 1.0308, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 421, Loss: 0.8998, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 422, Loss: 0.9276, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 423, Loss: 0.8810, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 424, Loss: 0.9272, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 425, Loss: 0.9614, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 426, Loss: 0.8399, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 427, Loss: 1.0201, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 428, Loss: 0.8566, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 429, Loss: 0.9246, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 430, Loss: 0.8430, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 431, Loss: 0.9972, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 432, Loss: 0.9563, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 433, Loss: 0.9115, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 434, Loss: 0.8979, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 435, Loss: 0.9001, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 436, Loss: 0.8500, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 437, Loss: 0.9318, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 438, Loss: 1.0037, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 439, Loss: 0.9016, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 440, Loss: 0.8403, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 441, Loss: 0.9823, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 442, Loss: 0.8824, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 443, Loss: 0.8898, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 444, Loss: 0.8781, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 445, Loss: 1.0778, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 446, Loss: 0.8047, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 447, Loss: 0.9080, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 448, Loss: 0.9409, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 449, Loss: 0.8490, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 450, Loss: 0.8635, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 451, Loss: 0.9051, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 452, Loss: 0.9715, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 453, Loss: 0.8892, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 454, Loss: 0.8172, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 455, Loss: 0.8825, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 456, Loss: 0.8826, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 457, Loss: 0.8240, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 458, Loss: 0.8230, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 459, Loss: 0.8064, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 460, Loss: 0.8142, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 461, Loss: 0.7568, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 462, Loss: 0.7759, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 463, Loss: 0.8533, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 464, Loss: 0.7551, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 465, Loss: 0.7710, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 466, Loss: 0.8433, Train 0.3837, Val 0.3692\n",
      "Fold: 3, Epoch: 467, Loss: 0.8741, Train 0.3837, Val 0.3692\n",
      "Fold: 3, Epoch: 468, Loss: 0.7333, Train 0.3837, Val 0.3692\n",
      "Fold: 3, Epoch: 469, Loss: 0.7939, Train 0.3886, Val 0.3692\n",
      "Fold: 3, Epoch: 470, Loss: 0.8099, Train 0.3911, Val 0.3692\n",
      "Fold: 3, Epoch: 471, Loss: 0.9436, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 472, Loss: 0.8227, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 473, Loss: 0.7552, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 474, Loss: 0.9477, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 475, Loss: 0.8625, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 476, Loss: 0.7605, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 477, Loss: 0.8409, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 478, Loss: 0.8309, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 479, Loss: 0.8038, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 480, Loss: 0.7649, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 481, Loss: 0.7921, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 482, Loss: 0.7006, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 483, Loss: 0.7623, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 484, Loss: 0.7592, Train 0.3837, Val 0.3692\n",
      "Fold: 3, Epoch: 485, Loss: 0.7252, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 486, Loss: 0.6679, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 487, Loss: 0.7101, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 488, Loss: 0.6964, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 489, Loss: 0.8103, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 490, Loss: 0.7119, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 491, Loss: 0.6253, Train 0.4183, Val 0.3846\n",
      "Fold: 3, Epoch: 492, Loss: 0.9005, Train 0.6460, Val 0.5692\n",
      "Fold: 3, Epoch: 493, Loss: 0.9991, Train 0.5322, Val 0.4462\n",
      "Fold: 3, Epoch: 494, Loss: 0.7429, Train 0.4183, Val 0.4000\n",
      "Fold: 3, Epoch: 495, Loss: 0.6184, Train 0.4431, Val 0.4308\n",
      "Fold: 3, Epoch: 496, Loss: 0.7328, Train 0.3515, Val 0.3538\n",
      "Fold: 3, Epoch: 497, Loss: 0.7793, Train 0.2624, Val 0.2308\n",
      "Fold: 3, Epoch: 498, Loss: 0.6595, Train 0.2723, Val 0.2462\n",
      "Fold: 3, Epoch: 499, Loss: 0.7587, Train 0.6040, Val 0.5846\n",
      "Fold: 3, Epoch: 500, Loss: 0.7684, Train 0.5842, Val 0.4923\n",
      "Fold: 3, Epoch: 501, Loss: 0.9762, Train 0.3837, Val 0.3385\n",
      "Fold: 3, Epoch: 502, Loss: 0.6544, Train 0.3292, Val 0.3077\n",
      "Fold: 3, Epoch: 503, Loss: 0.6472, Train 0.3936, Val 0.4154\n",
      "Fold: 3, Epoch: 504, Loss: 0.6488, Train 0.4728, Val 0.4462\n",
      "Fold: 3, Epoch: 505, Loss: 0.8846, Train 0.4777, Val 0.4615\n",
      "Fold: 3, Epoch: 506, Loss: 0.6277, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 507, Loss: 0.6178, Train 0.7450, Val 0.6769\n",
      "Fold: 3, Epoch: 508, Loss: 0.7400, Train 0.7723, Val 0.7077\n",
      "Fold: 3, Epoch: 509, Loss: 0.7438, Train 0.7351, Val 0.6462\n",
      "Fold: 3, Epoch: 510, Loss: 0.6719, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 511, Loss: 0.5502, Train 0.4703, Val 0.4615\n",
      "Fold: 3, Epoch: 512, Loss: 0.7090, Train 0.4505, Val 0.4154\n",
      "Fold: 3, Epoch: 513, Loss: 0.6489, Train 0.4134, Val 0.4000\n",
      "Fold: 3, Epoch: 514, Loss: 0.5961, Train 0.3490, Val 0.3692\n",
      "Fold: 3, Epoch: 515, Loss: 0.5686, Train 0.3589, Val 0.3692\n",
      "Fold: 3, Epoch: 516, Loss: 0.6387, Train 0.4505, Val 0.4308\n",
      "Fold: 3, Epoch: 517, Loss: 0.5807, Train 0.4629, Val 0.4462\n",
      "Fold: 3, Epoch: 518, Loss: 0.6060, Train 0.4208, Val 0.4308\n",
      "Fold: 3, Epoch: 519, Loss: 0.6042, Train 0.3960, Val 0.3846\n",
      "Fold: 3, Epoch: 520, Loss: 0.5709, Train 0.4035, Val 0.4000\n",
      "Fold: 3, Epoch: 521, Loss: 0.7084, Train 0.4530, Val 0.4462\n",
      "Fold: 3, Epoch: 522, Loss: 0.6360, Train 0.4876, Val 0.4615\n",
      "Fold: 3, Epoch: 523, Loss: 0.6330, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 524, Loss: 0.5471, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 525, Loss: 0.5264, Train 0.4950, Val 0.4615\n",
      "Fold: 3, Epoch: 526, Loss: 0.4523, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 527, Loss: 0.5901, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 528, Loss: 0.5289, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 529, Loss: 0.5579, Train 0.4926, Val 0.4615\n",
      "Fold: 3, Epoch: 530, Loss: 0.5751, Train 0.4926, Val 0.4615\n",
      "Fold: 3, Epoch: 531, Loss: 0.5468, Train 0.4926, Val 0.4615\n",
      "Fold: 3, Epoch: 532, Loss: 0.4626, Train 0.4827, Val 0.4615\n",
      "Fold: 3, Epoch: 533, Loss: 0.5811, Train 0.4703, Val 0.4462\n",
      "Fold: 3, Epoch: 534, Loss: 0.5313, Train 0.4134, Val 0.4000\n",
      "Fold: 3, Epoch: 535, Loss: 0.5854, Train 0.2946, Val 0.2000\n",
      "Fold: 3, Epoch: 536, Loss: 0.6055, Train 0.3391, Val 0.2769\n",
      "Fold: 3, Epoch: 537, Loss: 0.4820, Train 0.3812, Val 0.3231\n",
      "Fold: 3, Epoch: 538, Loss: 0.5167, Train 0.4084, Val 0.3692\n",
      "Fold: 3, Epoch: 539, Loss: 0.5272, Train 0.3465, Val 0.2923\n",
      "Fold: 3, Epoch: 540, Loss: 0.4369, Train 0.2772, Val 0.2000\n",
      "Fold: 3, Epoch: 541, Loss: 0.5536, Train 0.1955, Val 0.1538\n",
      "Fold: 3, Epoch: 542, Loss: 0.5522, Train 0.1460, Val 0.1077\n",
      "Fold: 3, Epoch: 543, Loss: 0.5656, Train 0.1361, Val 0.1077\n",
      "Fold: 3, Epoch: 544, Loss: 0.6059, Train 0.1584, Val 0.1538\n",
      "Fold: 3, Epoch: 545, Loss: 0.5901, Train 0.1559, Val 0.1385\n",
      "Fold: 3, Epoch: 546, Loss: 0.6215, Train 0.1807, Val 0.1538\n",
      "Fold: 3, Epoch: 547, Loss: 0.6407, Train 0.2203, Val 0.2154\n",
      "Fold: 3, Epoch: 548, Loss: 0.4486, Train 0.4678, Val 0.4462\n",
      "Fold: 3, Epoch: 549, Loss: 0.6487, Train 0.4901, Val 0.4615\n",
      "Fold: 3, Epoch: 550, Loss: 0.6017, Train 0.5941, Val 0.5077\n",
      "Fold: 3, Epoch: 551, Loss: 0.5760, Train 0.7525, Val 0.6769\n",
      "Fold: 3, Epoch: 552, Loss: 0.5611, Train 0.7871, Val 0.6923\n",
      "Fold: 3, Epoch: 553, Loss: 0.5262, Train 0.7871, Val 0.6923\n",
      "Fold: 3, Epoch: 554, Loss: 0.7195, Train 0.6386, Val 0.5538\n",
      "Fold: 3, Epoch: 555, Loss: 0.4142, Train 0.4827, Val 0.4615\n",
      "Fold: 3, Epoch: 556, Loss: 0.5612, Train 0.4455, Val 0.4308\n",
      "Fold: 3, Epoch: 557, Loss: 0.5140, Train 0.3713, Val 0.3385\n",
      "Fold: 3, Epoch: 558, Loss: 0.5279, Train 0.3045, Val 0.2308\n",
      "Fold: 3, Epoch: 559, Loss: 0.5250, Train 0.3292, Val 0.2615\n",
      "Fold: 3, Epoch: 560, Loss: 0.5448, Train 0.4579, Val 0.4308\n",
      "Fold: 3, Epoch: 561, Loss: 0.4796, Train 0.4827, Val 0.4615\n",
      "Fold: 3, Epoch: 562, Loss: 0.4198, Train 0.4851, Val 0.4615\n",
      "Fold: 3, Epoch: 563, Loss: 0.4834, Train 0.5000, Val 0.4615\n",
      "Fold: 3, Epoch: 564, Loss: 0.5019, Train 0.5446, Val 0.4923\n",
      "Fold: 3, Epoch: 565, Loss: 0.4474, Train 0.5025, Val 0.4615\n",
      "Fold: 3, Epoch: 566, Loss: 0.4776, Train 0.4851, Val 0.4615\n",
      "Fold: 3, Epoch: 567, Loss: 0.5040, Train 0.4851, Val 0.4615\n",
      "Fold: 3, Epoch: 568, Loss: 0.5521, Train 0.4851, Val 0.4615\n",
      "Fold: 3, Epoch: 569, Loss: 0.4828, Train 0.4777, Val 0.4615\n",
      "Fold: 3, Epoch: 570, Loss: 0.4171, Train 0.4653, Val 0.4462\n",
      "Fold: 3, Epoch: 571, Loss: 0.5820, Train 0.4208, Val 0.4154\n",
      "Fold: 3, Epoch: 572, Loss: 0.3742, Train 0.3094, Val 0.2769\n",
      "Fold: 3, Epoch: 573, Loss: 0.5287, Train 0.3540, Val 0.3077\n",
      "Fold: 3, Epoch: 574, Loss: 0.5824, Train 0.4926, Val 0.4615\n",
      "Fold: 3, Epoch: 575, Loss: 0.5624, Train 0.4950, Val 0.4615\n",
      "Fold: 3, Epoch: 576, Loss: 0.5333, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 577, Loss: 0.5393, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 578, Loss: 0.5836, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 579, Loss: 0.4386, Train 0.4678, Val 0.4462\n",
      "Fold: 3, Epoch: 580, Loss: 0.3911, Train 0.3738, Val 0.3385\n",
      "Fold: 3, Epoch: 581, Loss: 0.5087, Train 0.2153, Val 0.1538\n",
      "Fold: 3, Epoch: 582, Loss: 0.4557, Train 0.2030, Val 0.1231\n",
      "Fold: 3, Epoch: 583, Loss: 0.5199, Train 0.4678, Val 0.3538\n",
      "Fold: 3, Epoch: 584, Loss: 0.4323, Train 0.4851, Val 0.3692\n",
      "Fold: 3, Epoch: 585, Loss: 0.6459, Train 0.4208, Val 0.2923\n",
      "Fold: 3, Epoch: 586, Loss: 0.4795, Train 0.3911, Val 0.3692\n",
      "Fold: 3, Epoch: 587, Loss: 0.4186, Train 0.4381, Val 0.4615\n",
      "Fold: 3, Epoch: 588, Loss: 0.6628, Train 0.4703, Val 0.4615\n",
      "Fold: 3, Epoch: 589, Loss: 0.4769, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 590, Loss: 0.4759, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 591, Loss: 0.5045, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 592, Loss: 0.5020, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 593, Loss: 0.4507, Train 0.7178, Val 0.6308\n",
      "Fold: 3, Epoch: 594, Loss: 0.5853, Train 0.7277, Val 0.6154\n",
      "Fold: 3, Epoch: 595, Loss: 0.4882, Train 0.7351, Val 0.6308\n",
      "Fold: 3, Epoch: 596, Loss: 0.5392, Train 0.7203, Val 0.6308\n",
      "Fold: 3, Epoch: 597, Loss: 0.5466, Train 0.6262, Val 0.5385\n",
      "Fold: 3, Epoch: 598, Loss: 0.5610, Train 0.5817, Val 0.5077\n",
      "Fold: 3, Epoch: 599, Loss: 0.4487, Train 0.5000, Val 0.4615\n",
      "Fold: 3, Epoch: 600, Loss: 0.4432, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 601, Loss: 0.4826, Train 0.3663, Val 0.3231\n",
      "Fold: 3, Epoch: 602, Loss: 0.5711, Train 0.2129, Val 0.1692\n",
      "Fold: 3, Epoch: 603, Loss: 0.7125, Train 0.2277, Val 0.1846\n",
      "Fold: 3, Epoch: 604, Loss: 0.4792, Train 0.4332, Val 0.4308\n",
      "Fold: 3, Epoch: 605, Loss: 0.4077, Train 0.4950, Val 0.4615\n",
      "Fold: 3, Epoch: 606, Loss: 0.4854, Train 0.5421, Val 0.4769\n",
      "Fold: 3, Epoch: 607, Loss: 0.5206, Train 0.5743, Val 0.5231\n",
      "Fold: 3, Epoch: 608, Loss: 0.4392, Train 0.5743, Val 0.5231\n",
      "Fold: 3, Epoch: 609, Loss: 0.4046, Train 0.5767, Val 0.5231\n",
      "Fold: 3, Epoch: 610, Loss: 0.4000, Train 0.4728, Val 0.4615\n",
      "Fold: 3, Epoch: 611, Loss: 0.4947, Train 0.2500, Val 0.2308\n",
      "Fold: 3, Epoch: 612, Loss: 0.4605, Train 0.2475, Val 0.2615\n",
      "Fold: 3, Epoch: 613, Loss: 0.3932, Train 0.2302, Val 0.1692\n",
      "Fold: 3, Epoch: 614, Loss: 0.5136, Train 0.2178, Val 0.1692\n",
      "Fold: 3, Epoch: 615, Loss: 0.3739, Train 0.1931, Val 0.1538\n",
      "Fold: 3, Epoch: 616, Loss: 0.4795, Train 0.1832, Val 0.1385\n",
      "Fold: 3, Epoch: 617, Loss: 0.4311, Train 0.1832, Val 0.1385\n",
      "Fold: 3, Epoch: 618, Loss: 0.4246, Train 0.1708, Val 0.1231\n",
      "Fold: 3, Epoch: 619, Loss: 0.4750, Train 0.1708, Val 0.1231\n",
      "Fold: 3, Epoch: 620, Loss: 0.6266, Train 0.1832, Val 0.1538\n",
      "Fold: 3, Epoch: 621, Loss: 0.4952, Train 0.2079, Val 0.1846\n",
      "Fold: 3, Epoch: 622, Loss: 0.4219, Train 0.3837, Val 0.3231\n",
      "Fold: 3, Epoch: 623, Loss: 0.4634, Train 0.5792, Val 0.4769\n",
      "Fold: 3, Epoch: 624, Loss: 0.4679, Train 0.7970, Val 0.7077\n",
      "Fold: 3, Epoch: 625, Loss: 0.4344, Train 0.7970, Val 0.7077\n",
      "Fold: 3, Epoch: 626, Loss: 0.4926, Train 0.7847, Val 0.6923\n",
      "Fold: 3, Epoch: 627, Loss: 0.5322, Train 0.7426, Val 0.6462\n",
      "Fold: 3, Epoch: 628, Loss: 0.5407, Train 0.5866, Val 0.4923\n",
      "Fold: 3, Epoch: 629, Loss: 0.3540, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 630, Loss: 0.4520, Train 0.4728, Val 0.4615\n",
      "Fold: 3, Epoch: 631, Loss: 0.4143, Train 0.3144, Val 0.2615\n",
      "Fold: 3, Epoch: 632, Loss: 0.5116, Train 0.2550, Val 0.1846\n",
      "Fold: 3, Epoch: 633, Loss: 0.4794, Train 0.4109, Val 0.3846\n",
      "Fold: 3, Epoch: 634, Loss: 0.4807, Train 0.4752, Val 0.4615\n",
      "Fold: 3, Epoch: 635, Loss: 0.4962, Train 0.4876, Val 0.4769\n",
      "Fold: 3, Epoch: 636, Loss: 0.4411, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 637, Loss: 0.4764, Train 0.4827, Val 0.4615\n",
      "Fold: 3, Epoch: 638, Loss: 0.4394, Train 0.4901, Val 0.4615\n",
      "Fold: 3, Epoch: 639, Loss: 0.4805, Train 0.5149, Val 0.4615\n",
      "Fold: 3, Epoch: 640, Loss: 0.4270, Train 0.6634, Val 0.5846\n",
      "Fold: 3, Epoch: 641, Loss: 0.4126, Train 0.6807, Val 0.6308\n",
      "Fold: 3, Epoch: 642, Loss: 0.4025, Train 0.6485, Val 0.5692\n",
      "Fold: 3, Epoch: 643, Loss: 0.4885, Train 0.4851, Val 0.4462\n",
      "Fold: 3, Epoch: 644, Loss: 0.4818, Train 0.4604, Val 0.4462\n",
      "Fold: 3, Epoch: 645, Loss: 0.4410, Train 0.4455, Val 0.4462\n",
      "Fold: 3, Epoch: 646, Loss: 0.3763, Train 0.4505, Val 0.4462\n",
      "Fold: 3, Epoch: 647, Loss: 0.3931, Train 0.4431, Val 0.4308\n",
      "Fold: 3, Epoch: 648, Loss: 0.4768, Train 0.4703, Val 0.4308\n",
      "Fold: 3, Epoch: 649, Loss: 0.4066, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 650, Loss: 0.5059, Train 0.5891, Val 0.5385\n",
      "Fold: 3, Epoch: 651, Loss: 0.4664, Train 0.6881, Val 0.6308\n",
      "Fold: 3, Epoch: 652, Loss: 0.4064, Train 0.6040, Val 0.4769\n",
      "Fold: 3, Epoch: 653, Loss: 0.3712, Train 0.5322, Val 0.4154\n",
      "Fold: 3, Epoch: 654, Loss: 0.4312, Train 0.5272, Val 0.4615\n",
      "Fold: 3, Epoch: 655, Loss: 0.4157, Train 0.4777, Val 0.4462\n",
      "Fold: 3, Epoch: 656, Loss: 0.3084, Train 0.2946, Val 0.3231\n",
      "Fold: 3, Epoch: 657, Loss: 0.4870, Train 0.2500, Val 0.2769\n",
      "Fold: 3, Epoch: 658, Loss: 0.3088, Train 0.2921, Val 0.3385\n",
      "Fold: 3, Epoch: 659, Loss: 0.4134, Train 0.4406, Val 0.4615\n",
      "Fold: 3, Epoch: 660, Loss: 0.4082, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 661, Loss: 0.4680, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 662, Loss: 0.4748, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 663, Loss: 0.4669, Train 0.5000, Val 0.4615\n",
      "Fold: 3, Epoch: 664, Loss: 0.3787, Train 0.4926, Val 0.4615\n",
      "Fold: 3, Epoch: 665, Loss: 0.4736, Train 0.4876, Val 0.4615\n",
      "Fold: 3, Epoch: 666, Loss: 0.3794, Train 0.4728, Val 0.4615\n",
      "Fold: 3, Epoch: 667, Loss: 0.4481, Train 0.4752, Val 0.4615\n",
      "Fold: 3, Epoch: 668, Loss: 0.5116, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 669, Loss: 0.4347, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 670, Loss: 0.3352, Train 0.4876, Val 0.4615\n",
      "Fold: 3, Epoch: 671, Loss: 0.5001, Train 0.4876, Val 0.4615\n",
      "Fold: 3, Epoch: 672, Loss: 0.3742, Train 0.4876, Val 0.4615\n",
      "Fold: 3, Epoch: 673, Loss: 0.3965, Train 0.5446, Val 0.4615\n",
      "Fold: 3, Epoch: 674, Loss: 0.4574, Train 0.7327, Val 0.6308\n",
      "Fold: 3, Epoch: 675, Loss: 0.4377, Train 0.7450, Val 0.6308\n",
      "Fold: 3, Epoch: 676, Loss: 0.3526, Train 0.6782, Val 0.6000\n",
      "Fold: 3, Epoch: 677, Loss: 0.4446, Train 0.5124, Val 0.4769\n",
      "Fold: 3, Epoch: 678, Loss: 0.3720, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 679, Loss: 0.3402, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 680, Loss: 0.3915, Train 0.4876, Val 0.4769\n",
      "Fold: 3, Epoch: 681, Loss: 0.3638, Train 0.4827, Val 0.4769\n",
      "Fold: 3, Epoch: 682, Loss: 0.3150, Train 0.4752, Val 0.4615\n",
      "Fold: 3, Epoch: 683, Loss: 0.4109, Train 0.4728, Val 0.4615\n",
      "Fold: 3, Epoch: 684, Loss: 0.3203, Train 0.4703, Val 0.4615\n",
      "Fold: 3, Epoch: 685, Loss: 0.3717, Train 0.4728, Val 0.4615\n",
      "Fold: 3, Epoch: 686, Loss: 0.3648, Train 0.4901, Val 0.4615\n",
      "Fold: 3, Epoch: 687, Loss: 0.3266, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 688, Loss: 0.3258, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 689, Loss: 0.3824, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 690, Loss: 0.5966, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 691, Loss: 0.4590, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 692, Loss: 0.3107, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 693, Loss: 0.3046, Train 0.5817, Val 0.5231\n",
      "Fold: 3, Epoch: 694, Loss: 0.4482, Train 0.5371, Val 0.4615\n",
      "Fold: 3, Epoch: 695, Loss: 0.3140, Train 0.5000, Val 0.4615\n",
      "Fold: 3, Epoch: 696, Loss: 0.4637, Train 0.4901, Val 0.4615\n",
      "Fold: 3, Epoch: 697, Loss: 0.3609, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 698, Loss: 0.3759, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 699, Loss: 0.4549, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 700, Loss: 0.3907, Train 0.4777, Val 0.4615\n",
      "Fold: 3, Epoch: 701, Loss: 0.4072, Train 0.4851, Val 0.4769\n",
      "Fold: 3, Epoch: 702, Loss: 0.3623, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 703, Loss: 0.4217, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 704, Loss: 0.3431, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 705, Loss: 0.4076, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 706, Loss: 0.3582, Train 0.4876, Val 0.4769\n",
      "Fold: 3, Epoch: 707, Loss: 0.3770, Train 0.4827, Val 0.4769\n",
      "Fold: 3, Epoch: 708, Loss: 0.3888, Train 0.4579, Val 0.4308\n",
      "Fold: 3, Epoch: 709, Loss: 0.3092, Train 0.4851, Val 0.4769\n",
      "Fold: 3, Epoch: 710, Loss: 0.3685, Train 0.4876, Val 0.4769\n",
      "Fold: 3, Epoch: 711, Loss: 0.2838, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 712, Loss: 0.2828, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 713, Loss: 0.3658, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 714, Loss: 0.4296, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 715, Loss: 0.3512, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 716, Loss: 0.3402, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 717, Loss: 0.3379, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 718, Loss: 0.5024, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 719, Loss: 0.4476, Train 0.5124, Val 0.4769\n",
      "Fold: 3, Epoch: 720, Loss: 0.4127, Train 0.5619, Val 0.5077\n",
      "Fold: 3, Epoch: 721, Loss: 0.5210, Train 0.5124, Val 0.4769\n",
      "Fold: 3, Epoch: 722, Loss: 0.3153, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 723, Loss: 0.3567, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 724, Loss: 0.3711, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 725, Loss: 0.3587, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 726, Loss: 0.3933, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 727, Loss: 0.4741, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 728, Loss: 0.3923, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 729, Loss: 0.3173, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 730, Loss: 0.3393, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 731, Loss: 0.4539, Train 0.4876, Val 0.4769\n",
      "Fold: 3, Epoch: 732, Loss: 0.4492, Train 0.4876, Val 0.4769\n",
      "Fold: 3, Epoch: 733, Loss: 0.3857, Train 0.4876, Val 0.4769\n",
      "Fold: 3, Epoch: 734, Loss: 0.4841, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 735, Loss: 0.4472, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 736, Loss: 0.3330, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 737, Loss: 0.3494, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 738, Loss: 0.3280, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 739, Loss: 0.3315, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 740, Loss: 0.3822, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 741, Loss: 0.2678, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 742, Loss: 0.3899, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 743, Loss: 0.3140, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 744, Loss: 0.3484, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 745, Loss: 0.3931, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 746, Loss: 0.4530, Train 0.4802, Val 0.4769\n",
      "Fold: 3, Epoch: 747, Loss: 0.4067, Train 0.4752, Val 0.4615\n",
      "Fold: 3, Epoch: 748, Loss: 0.4373, Train 0.4728, Val 0.4615\n",
      "Fold: 3, Epoch: 749, Loss: 0.3783, Train 0.4752, Val 0.4615\n",
      "Fold: 3, Epoch: 750, Loss: 0.3640, Train 0.4802, Val 0.4615\n",
      "Fold: 3, Epoch: 751, Loss: 0.3164, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 752, Loss: 0.3894, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 753, Loss: 0.4283, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 754, Loss: 0.3518, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 755, Loss: 0.4838, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 756, Loss: 0.5348, Train 0.4901, Val 0.4769\n",
      "Fold: 3, Epoch: 757, Loss: 0.3407, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 758, Loss: 0.3189, Train 0.5198, Val 0.4769\n",
      "Fold: 3, Epoch: 759, Loss: 0.3542, Train 0.5347, Val 0.4923\n",
      "Fold: 3, Epoch: 760, Loss: 0.3614, Train 0.5223, Val 0.4769\n",
      "Fold: 3, Epoch: 761, Loss: 0.4105, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 762, Loss: 0.3769, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 763, Loss: 0.3795, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 764, Loss: 0.4285, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 765, Loss: 0.3931, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 766, Loss: 0.3649, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 767, Loss: 0.2843, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 768, Loss: 0.3255, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 769, Loss: 0.3637, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 770, Loss: 0.3758, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 771, Loss: 0.3303, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 772, Loss: 0.3730, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 773, Loss: 0.4436, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 774, Loss: 0.4034, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 775, Loss: 0.3457, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 776, Loss: 0.3333, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 777, Loss: 0.3251, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 778, Loss: 0.4065, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 779, Loss: 0.3118, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 780, Loss: 0.3156, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 781, Loss: 0.3952, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 782, Loss: 0.3773, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 783, Loss: 0.3660, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 784, Loss: 0.3228, Train 0.5099, Val 0.5077\n",
      "Fold: 3, Epoch: 785, Loss: 0.2457, Train 0.5025, Val 0.5538\n",
      "Fold: 3, Epoch: 786, Loss: 0.3268, Train 0.3688, Val 0.4308\n",
      "Fold: 3, Epoch: 787, Loss: 0.3752, Train 0.2129, Val 0.2462\n",
      "Fold: 3, Epoch: 788, Loss: 0.2879, Train 0.2129, Val 0.2462\n",
      "Fold: 3, Epoch: 789, Loss: 0.3448, Train 0.2822, Val 0.3385\n",
      "Fold: 3, Epoch: 790, Loss: 0.3136, Train 0.3515, Val 0.4154\n",
      "Fold: 3, Epoch: 791, Loss: 0.3668, Train 0.3812, Val 0.4154\n",
      "Fold: 3, Epoch: 792, Loss: 0.5007, Train 0.4851, Val 0.5538\n",
      "Fold: 3, Epoch: 793, Loss: 0.2610, Train 0.5124, Val 0.5231\n",
      "Fold: 3, Epoch: 794, Loss: 0.3349, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 795, Loss: 0.2789, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 796, Loss: 0.3333, Train 0.4950, Val 0.4615\n",
      "Fold: 3, Epoch: 797, Loss: 0.3944, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 798, Loss: 0.3654, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 799, Loss: 0.3104, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 800, Loss: 0.2970, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 801, Loss: 0.4422, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 802, Loss: 0.3598, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 803, Loss: 0.3882, Train 0.5149, Val 0.4769\n",
      "Fold: 3, Epoch: 804, Loss: 0.4153, Train 0.5173, Val 0.4769\n",
      "Fold: 3, Epoch: 805, Loss: 0.4430, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 806, Loss: 0.3878, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 807, Loss: 0.4576, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 808, Loss: 0.3849, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 809, Loss: 0.4710, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 810, Loss: 0.3365, Train 0.4950, Val 0.4615\n",
      "Fold: 3, Epoch: 811, Loss: 0.2666, Train 0.4901, Val 0.4615\n",
      "Fold: 3, Epoch: 812, Loss: 0.3448, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 813, Loss: 0.3144, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 814, Loss: 0.4332, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 815, Loss: 0.2826, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 816, Loss: 0.3704, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 817, Loss: 0.3896, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 818, Loss: 0.3910, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 819, Loss: 0.3652, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 820, Loss: 0.3093, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 821, Loss: 0.4041, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 822, Loss: 0.3201, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 823, Loss: 0.2916, Train 0.4827, Val 0.4615\n",
      "Fold: 3, Epoch: 824, Loss: 0.3591, Train 0.4777, Val 0.4462\n",
      "Fold: 3, Epoch: 825, Loss: 0.3636, Train 0.4777, Val 0.4769\n",
      "Fold: 3, Epoch: 826, Loss: 0.3998, Train 0.4901, Val 0.4923\n",
      "Fold: 3, Epoch: 827, Loss: 0.2962, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 828, Loss: 0.4612, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 829, Loss: 0.2714, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 830, Loss: 0.4405, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 831, Loss: 0.2961, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 832, Loss: 0.3598, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 833, Loss: 0.2735, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 834, Loss: 0.3106, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 835, Loss: 0.3371, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 836, Loss: 0.2939, Train 0.5025, Val 0.5077\n",
      "Fold: 3, Epoch: 837, Loss: 0.3359, Train 0.5000, Val 0.5077\n",
      "Fold: 3, Epoch: 838, Loss: 0.3441, Train 0.5198, Val 0.5231\n",
      "Fold: 3, Epoch: 839, Loss: 0.2977, Train 0.5198, Val 0.5231\n",
      "Fold: 3, Epoch: 840, Loss: 0.3090, Train 0.5099, Val 0.5077\n",
      "Fold: 3, Epoch: 841, Loss: 0.4140, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 842, Loss: 0.4301, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 843, Loss: 0.3109, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 844, Loss: 0.3693, Train 0.5099, Val 0.4923\n",
      "Fold: 3, Epoch: 845, Loss: 0.3408, Train 0.5149, Val 0.5231\n",
      "Fold: 3, Epoch: 846, Loss: 0.3659, Train 0.5099, Val 0.4923\n",
      "Fold: 3, Epoch: 847, Loss: 0.4192, Train 0.5050, Val 0.4923\n",
      "Fold: 3, Epoch: 848, Loss: 0.3437, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 849, Loss: 0.3229, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 850, Loss: 0.4193, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 851, Loss: 0.2319, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 852, Loss: 0.2952, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 853, Loss: 0.2807, Train 0.4926, Val 0.4769\n",
      "Fold: 3, Epoch: 854, Loss: 0.3175, Train 0.4653, Val 0.4462\n",
      "Fold: 3, Epoch: 855, Loss: 0.3738, Train 0.4728, Val 0.4615\n",
      "Fold: 3, Epoch: 856, Loss: 0.3315, Train 0.4777, Val 0.4769\n",
      "Fold: 3, Epoch: 857, Loss: 0.3313, Train 0.5050, Val 0.5077\n",
      "Fold: 3, Epoch: 858, Loss: 0.2873, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 859, Loss: 0.3677, Train 0.5173, Val 0.5077\n",
      "Fold: 3, Epoch: 860, Loss: 0.2382, Train 0.5272, Val 0.5231\n",
      "Fold: 3, Epoch: 861, Loss: 0.3875, Train 0.5248, Val 0.5231\n",
      "Fold: 3, Epoch: 862, Loss: 0.2719, Train 0.5198, Val 0.5231\n",
      "Fold: 3, Epoch: 863, Loss: 0.3168, Train 0.5074, Val 0.5231\n",
      "Fold: 3, Epoch: 864, Loss: 0.3174, Train 0.4876, Val 0.5077\n",
      "Fold: 3, Epoch: 865, Loss: 0.3241, Train 0.4876, Val 0.5077\n",
      "Fold: 3, Epoch: 866, Loss: 0.3901, Train 0.5149, Val 0.5231\n",
      "Fold: 3, Epoch: 867, Loss: 0.2994, Train 0.5149, Val 0.5077\n",
      "Fold: 3, Epoch: 868, Loss: 0.3194, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 869, Loss: 0.2825, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 870, Loss: 0.3604, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 871, Loss: 0.3891, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 872, Loss: 0.2971, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 873, Loss: 0.3171, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 874, Loss: 0.3652, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 875, Loss: 0.3451, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 876, Loss: 0.3833, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 877, Loss: 0.2003, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 878, Loss: 0.4225, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 879, Loss: 0.2536, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 880, Loss: 0.3519, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 881, Loss: 0.2927, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 882, Loss: 0.3302, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 883, Loss: 0.3004, Train 0.4678, Val 0.4615\n",
      "Fold: 3, Epoch: 884, Loss: 0.2838, Train 0.4554, Val 0.4308\n",
      "Fold: 3, Epoch: 885, Loss: 0.3298, Train 0.4480, Val 0.4154\n",
      "Fold: 3, Epoch: 886, Loss: 0.3770, Train 0.4752, Val 0.4769\n",
      "Fold: 3, Epoch: 887, Loss: 0.2695, Train 0.4802, Val 0.4923\n",
      "Fold: 3, Epoch: 888, Loss: 0.3102, Train 0.4950, Val 0.5385\n",
      "Fold: 3, Epoch: 889, Loss: 0.2831, Train 0.5124, Val 0.5385\n",
      "Fold: 3, Epoch: 890, Loss: 0.2870, Train 0.4926, Val 0.5385\n",
      "Fold: 3, Epoch: 891, Loss: 0.3500, Train 0.4728, Val 0.5231\n",
      "Fold: 3, Epoch: 892, Loss: 0.3149, Train 0.4728, Val 0.4923\n",
      "Fold: 3, Epoch: 893, Loss: 0.2831, Train 0.4703, Val 0.4769\n",
      "Fold: 3, Epoch: 894, Loss: 0.2993, Train 0.4381, Val 0.4462\n",
      "Fold: 3, Epoch: 895, Loss: 0.2984, Train 0.4752, Val 0.4615\n",
      "Fold: 3, Epoch: 896, Loss: 0.3364, Train 0.4282, Val 0.4000\n",
      "Fold: 3, Epoch: 897, Loss: 0.2726, Train 0.3861, Val 0.3846\n",
      "Fold: 3, Epoch: 898, Loss: 0.2785, Train 0.3564, Val 0.3692\n",
      "Fold: 3, Epoch: 899, Loss: 0.2858, Train 0.3317, Val 0.3385\n",
      "Fold: 3, Epoch: 900, Loss: 0.3532, Train 0.3490, Val 0.3692\n",
      "Fold: 3, Epoch: 901, Loss: 0.3689, Train 0.3441, Val 0.3231\n",
      "Fold: 3, Epoch: 902, Loss: 0.2456, Train 0.3589, Val 0.3538\n",
      "Fold: 3, Epoch: 903, Loss: 0.3705, Train 0.2797, Val 0.2615\n",
      "Fold: 3, Epoch: 904, Loss: 0.2987, Train 0.2649, Val 0.2615\n",
      "Fold: 3, Epoch: 905, Loss: 0.3985, Train 0.2550, Val 0.2462\n",
      "Fold: 3, Epoch: 906, Loss: 0.2595, Train 0.2748, Val 0.2615\n",
      "Fold: 3, Epoch: 907, Loss: 0.3130, Train 0.3020, Val 0.2615\n",
      "Fold: 3, Epoch: 908, Loss: 0.3530, Train 0.4480, Val 0.4308\n",
      "Fold: 3, Epoch: 909, Loss: 0.3845, Train 0.4901, Val 0.4923\n",
      "Fold: 3, Epoch: 910, Loss: 0.4561, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 911, Loss: 0.3500, Train 0.5099, Val 0.4769\n",
      "Fold: 3, Epoch: 912, Loss: 0.4425, Train 0.5050, Val 0.4769\n",
      "Fold: 3, Epoch: 913, Loss: 0.3331, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 914, Loss: 0.3108, Train 0.4728, Val 0.4462\n",
      "Fold: 3, Epoch: 915, Loss: 0.3687, Train 0.4406, Val 0.4154\n",
      "Fold: 3, Epoch: 916, Loss: 0.3405, Train 0.3614, Val 0.3692\n",
      "Fold: 3, Epoch: 917, Loss: 0.3884, Train 0.3465, Val 0.3692\n",
      "Fold: 3, Epoch: 918, Loss: 0.2868, Train 0.3589, Val 0.3692\n",
      "Fold: 3, Epoch: 919, Loss: 0.2710, Train 0.3762, Val 0.3846\n",
      "Fold: 3, Epoch: 920, Loss: 0.3646, Train 0.4208, Val 0.4308\n",
      "Fold: 3, Epoch: 921, Loss: 0.3463, Train 0.4653, Val 0.4308\n",
      "Fold: 3, Epoch: 922, Loss: 0.3220, Train 0.4827, Val 0.4615\n",
      "Fold: 3, Epoch: 923, Loss: 0.2428, Train 0.4827, Val 0.4615\n",
      "Fold: 3, Epoch: 924, Loss: 0.2842, Train 0.4678, Val 0.4308\n",
      "Fold: 3, Epoch: 925, Loss: 0.2914, Train 0.4282, Val 0.4154\n",
      "Fold: 3, Epoch: 926, Loss: 0.2775, Train 0.3515, Val 0.3385\n",
      "Fold: 3, Epoch: 927, Loss: 0.3846, Train 0.2426, Val 0.2462\n",
      "Fold: 3, Epoch: 928, Loss: 0.2952, Train 0.2153, Val 0.2154\n",
      "Fold: 3, Epoch: 929, Loss: 0.2562, Train 0.2129, Val 0.2154\n",
      "Fold: 3, Epoch: 930, Loss: 0.3792, Train 0.2129, Val 0.2154\n",
      "Fold: 3, Epoch: 931, Loss: 0.2694, Train 0.2921, Val 0.2923\n",
      "Fold: 3, Epoch: 932, Loss: 0.2654, Train 0.3688, Val 0.4000\n",
      "Fold: 3, Epoch: 933, Loss: 0.2769, Train 0.4332, Val 0.4769\n",
      "Fold: 3, Epoch: 934, Loss: 0.2866, Train 0.4554, Val 0.4308\n",
      "Fold: 3, Epoch: 935, Loss: 0.3183, Train 0.4752, Val 0.4615\n",
      "Fold: 3, Epoch: 936, Loss: 0.2920, Train 0.5025, Val 0.5077\n",
      "Fold: 3, Epoch: 937, Loss: 0.2867, Train 0.5149, Val 0.4923\n",
      "Fold: 3, Epoch: 938, Loss: 0.3334, Train 0.5099, Val 0.4769\n",
      "Fold: 3, Epoch: 939, Loss: 0.3108, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 940, Loss: 0.2844, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 941, Loss: 0.3135, Train 0.4802, Val 0.4462\n",
      "Fold: 3, Epoch: 942, Loss: 0.2623, Train 0.4579, Val 0.4154\n",
      "Fold: 3, Epoch: 943, Loss: 0.3611, Train 0.4332, Val 0.3846\n",
      "Fold: 3, Epoch: 944, Loss: 0.2218, Train 0.4257, Val 0.3846\n",
      "Fold: 3, Epoch: 945, Loss: 0.2854, Train 0.4035, Val 0.4154\n",
      "Fold: 3, Epoch: 946, Loss: 0.2071, Train 0.3688, Val 0.3846\n",
      "Fold: 3, Epoch: 947, Loss: 0.2937, Train 0.4109, Val 0.4308\n",
      "Fold: 3, Epoch: 948, Loss: 0.2367, Train 0.4703, Val 0.4615\n",
      "Fold: 3, Epoch: 949, Loss: 0.3316, Train 0.4975, Val 0.5077\n",
      "Fold: 3, Epoch: 950, Loss: 0.2794, Train 0.5173, Val 0.5077\n",
      "Fold: 3, Epoch: 951, Loss: 0.2485, Train 0.5149, Val 0.4769\n",
      "Fold: 3, Epoch: 952, Loss: 0.3880, Train 0.5149, Val 0.4769\n",
      "Fold: 3, Epoch: 953, Loss: 0.2646, Train 0.5149, Val 0.4769\n",
      "Fold: 3, Epoch: 954, Loss: 0.3498, Train 0.5223, Val 0.4923\n",
      "Fold: 3, Epoch: 955, Loss: 0.3498, Train 0.5173, Val 0.5077\n",
      "Fold: 3, Epoch: 956, Loss: 0.2454, Train 0.5000, Val 0.5077\n",
      "Fold: 3, Epoch: 957, Loss: 0.3402, Train 0.4703, Val 0.4769\n",
      "Fold: 3, Epoch: 958, Loss: 0.3518, Train 0.4926, Val 0.5077\n",
      "Fold: 3, Epoch: 959, Loss: 0.2562, Train 0.5050, Val 0.5231\n",
      "Fold: 3, Epoch: 960, Loss: 0.2638, Train 0.5198, Val 0.5231\n",
      "Fold: 3, Epoch: 961, Loss: 0.3247, Train 0.5173, Val 0.5077\n",
      "Fold: 3, Epoch: 962, Loss: 0.3289, Train 0.5149, Val 0.5077\n",
      "Fold: 3, Epoch: 963, Loss: 0.4332, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 964, Loss: 0.3465, Train 0.4827, Val 0.4769\n",
      "Fold: 3, Epoch: 965, Loss: 0.3888, Train 0.4950, Val 0.4769\n",
      "Fold: 3, Epoch: 966, Loss: 0.3041, Train 0.5025, Val 0.4769\n",
      "Fold: 3, Epoch: 967, Loss: 0.2422, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 968, Loss: 0.3236, Train 0.5173, Val 0.5077\n",
      "Fold: 3, Epoch: 969, Loss: 0.2240, Train 0.5322, Val 0.5231\n",
      "Fold: 3, Epoch: 970, Loss: 0.3011, Train 0.5347, Val 0.5538\n",
      "Fold: 3, Epoch: 971, Loss: 0.2452, Train 0.5396, Val 0.5692\n",
      "Fold: 3, Epoch: 972, Loss: 0.4202, Train 0.5347, Val 0.5692\n",
      "Fold: 3, Epoch: 973, Loss: 0.4001, Train 0.5050, Val 0.5385\n",
      "Fold: 3, Epoch: 974, Loss: 0.2522, Train 0.4876, Val 0.5077\n",
      "Fold: 3, Epoch: 975, Loss: 0.3659, Train 0.4356, Val 0.4615\n",
      "Fold: 3, Epoch: 976, Loss: 0.3301, Train 0.3960, Val 0.4308\n",
      "Fold: 3, Epoch: 977, Loss: 0.3140, Train 0.4035, Val 0.4615\n",
      "Fold: 3, Epoch: 978, Loss: 0.3024, Train 0.4604, Val 0.4615\n",
      "Fold: 3, Epoch: 979, Loss: 0.2844, Train 0.4926, Val 0.5077\n",
      "Fold: 3, Epoch: 980, Loss: 0.3634, Train 0.5173, Val 0.5231\n",
      "Fold: 3, Epoch: 981, Loss: 0.2982, Train 0.5099, Val 0.4769\n",
      "Fold: 3, Epoch: 982, Loss: 0.3970, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 983, Loss: 0.2809, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 984, Loss: 0.2798, Train 0.4975, Val 0.4769\n",
      "Fold: 3, Epoch: 985, Loss: 0.3608, Train 0.5000, Val 0.4769\n",
      "Fold: 3, Epoch: 986, Loss: 0.2930, Train 0.5074, Val 0.4769\n",
      "Fold: 3, Epoch: 987, Loss: 0.2904, Train 0.5124, Val 0.4923\n",
      "Fold: 3, Epoch: 988, Loss: 0.3229, Train 0.5248, Val 0.5231\n",
      "Fold: 3, Epoch: 989, Loss: 0.2484, Train 0.5248, Val 0.5231\n",
      "Fold: 3, Epoch: 990, Loss: 0.3428, Train 0.5198, Val 0.5385\n",
      "Fold: 3, Epoch: 991, Loss: 0.2771, Train 0.5074, Val 0.5385\n",
      "Fold: 3, Epoch: 992, Loss: 0.2389, Train 0.4950, Val 0.5077\n",
      "Fold: 3, Epoch: 993, Loss: 0.4338, Train 0.4579, Val 0.4615\n",
      "Fold: 3, Epoch: 994, Loss: 0.2462, Train 0.3837, Val 0.4308\n",
      "Fold: 3, Epoch: 995, Loss: 0.3358, Train 0.2153, Val 0.2000\n",
      "Fold: 3, Epoch: 996, Loss: 0.2759, Train 0.1683, Val 0.1538\n",
      "Fold: 3, Epoch: 997, Loss: 0.2998, Train 0.1683, Val 0.1385\n",
      "Fold: 3, Epoch: 998, Loss: 0.2857, Train 0.1708, Val 0.1385\n",
      "Fold: 3, Epoch: 999, Loss: 0.2827, Train 0.1955, Val 0.1692\n",
      "Fold: 4, Epoch: 001, Loss: 2.6318, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 002, Loss: 5.4608, Train 0.1337, Val 0.1719\n",
      "Fold: 4, Epoch: 003, Loss: 3.3197, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 004, Loss: 2.9164, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 005, Loss: 3.7918, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 006, Loss: 3.1001, Train 0.1089, Val 0.0781\n",
      "Fold: 4, Epoch: 007, Loss: 2.4963, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 008, Loss: 2.7575, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 009, Loss: 2.9815, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 010, Loss: 2.5297, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 011, Loss: 2.3259, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 012, Loss: 2.3408, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 013, Loss: 2.5021, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 014, Loss: 2.5161, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 015, Loss: 2.2968, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 016, Loss: 2.1752, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 017, Loss: 2.3291, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 018, Loss: 2.4578, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 019, Loss: 2.5711, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 020, Loss: 2.2673, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 021, Loss: 2.1503, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 022, Loss: 2.2636, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 023, Loss: 2.3391, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 024, Loss: 2.3789, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 025, Loss: 2.1973, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 026, Loss: 2.1572, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 027, Loss: 2.1695, Train 0.3094, Val 0.3906\n",
      "Fold: 4, Epoch: 028, Loss: 2.2490, Train 0.3119, Val 0.3906\n",
      "Fold: 4, Epoch: 029, Loss: 2.2210, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 030, Loss: 2.1450, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 031, Loss: 2.1368, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 032, Loss: 2.1534, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 033, Loss: 2.1862, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 034, Loss: 2.1714, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 035, Loss: 2.1464, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 036, Loss: 2.1654, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 037, Loss: 2.1537, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 038, Loss: 2.1500, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 039, Loss: 2.1518, Train 0.3762, Val 0.2969\n",
      "Fold: 4, Epoch: 040, Loss: 2.2179, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 041, Loss: 2.2135, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 042, Loss: 2.1526, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 043, Loss: 2.1710, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 044, Loss: 2.1775, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 045, Loss: 2.1908, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 046, Loss: 2.1347, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 047, Loss: 2.1585, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 048, Loss: 2.1494, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 049, Loss: 2.1791, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 050, Loss: 2.1631, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 051, Loss: 2.1470, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 052, Loss: 2.1383, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 053, Loss: 2.1695, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 054, Loss: 2.1656, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 055, Loss: 2.1460, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 056, Loss: 2.1267, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 057, Loss: 2.1492, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 058, Loss: 2.1751, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 059, Loss: 2.1951, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 060, Loss: 2.1252, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 061, Loss: 2.1378, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 062, Loss: 2.1312, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 063, Loss: 2.2055, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 064, Loss: 2.1587, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 065, Loss: 2.1270, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 066, Loss: 2.1466, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 067, Loss: 2.1530, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 068, Loss: 2.2037, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 069, Loss: 2.1273, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 070, Loss: 2.1449, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 071, Loss: 2.1267, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 072, Loss: 2.1247, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 073, Loss: 2.1469, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 074, Loss: 2.1378, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 075, Loss: 2.1295, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 076, Loss: 2.1374, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 077, Loss: 2.1283, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 078, Loss: 2.1597, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 079, Loss: 2.1317, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 080, Loss: 2.1469, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 081, Loss: 2.1513, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 082, Loss: 2.1424, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 083, Loss: 2.1392, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 084, Loss: 2.1235, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 085, Loss: 2.1375, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 086, Loss: 2.1751, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 087, Loss: 2.1238, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 088, Loss: 2.1490, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 089, Loss: 2.1505, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 090, Loss: 2.1269, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 091, Loss: 2.1303, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 092, Loss: 2.1493, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 093, Loss: 2.1171, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 094, Loss: 2.1266, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 095, Loss: 2.1325, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 096, Loss: 2.1378, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 097, Loss: 2.1374, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 098, Loss: 2.1592, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 099, Loss: 2.1174, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 100, Loss: 2.1493, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 101, Loss: 2.1423, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 102, Loss: 2.1723, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 103, Loss: 2.1346, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 104, Loss: 2.1454, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 105, Loss: 2.1320, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 106, Loss: 2.1293, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 107, Loss: 2.1486, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 108, Loss: 2.1240, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 109, Loss: 2.1154, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 110, Loss: 2.1326, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 111, Loss: 2.1249, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 112, Loss: 2.1363, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 113, Loss: 2.1147, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 114, Loss: 2.1259, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 115, Loss: 2.1163, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 116, Loss: 2.1076, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 117, Loss: 2.1289, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 118, Loss: 2.1220, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 119, Loss: 2.1286, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 120, Loss: 2.1260, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 121, Loss: 2.1032, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 122, Loss: 2.1148, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 123, Loss: 2.1276, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 124, Loss: 2.1214, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 125, Loss: 2.1164, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 126, Loss: 2.1167, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 127, Loss: 2.1144, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 128, Loss: 2.1424, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 129, Loss: 2.1236, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 130, Loss: 2.1407, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 131, Loss: 2.1005, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 132, Loss: 2.1050, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 133, Loss: 2.1089, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 134, Loss: 2.1204, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 135, Loss: 2.1277, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 136, Loss: 2.1121, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 137, Loss: 2.1139, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 138, Loss: 2.1335, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 139, Loss: 2.1008, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 140, Loss: 2.1213, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 141, Loss: 2.1077, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 142, Loss: 2.1068, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 143, Loss: 2.1019, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 144, Loss: 2.1104, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 145, Loss: 2.1097, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 146, Loss: 2.1203, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 147, Loss: 2.0968, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 148, Loss: 2.1269, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 149, Loss: 2.1266, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 150, Loss: 2.1385, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 151, Loss: 2.1103, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 152, Loss: 2.1875, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 153, Loss: 2.1298, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 154, Loss: 2.0890, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 155, Loss: 2.1239, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 156, Loss: 2.1418, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 157, Loss: 2.1438, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 158, Loss: 2.1184, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 159, Loss: 2.1476, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 160, Loss: 2.1292, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 161, Loss: 2.1219, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 162, Loss: 2.1002, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 163, Loss: 2.1256, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 164, Loss: 2.1014, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 165, Loss: 2.1078, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 166, Loss: 2.1057, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 167, Loss: 2.0954, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 168, Loss: 2.1025, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 169, Loss: 2.1082, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 170, Loss: 2.0983, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 171, Loss: 2.1092, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 172, Loss: 2.1027, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 173, Loss: 2.1188, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 174, Loss: 2.1067, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 175, Loss: 2.1072, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 176, Loss: 2.1024, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 177, Loss: 2.0984, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 178, Loss: 2.0951, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 179, Loss: 2.1023, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 180, Loss: 2.1184, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 181, Loss: 2.0928, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 182, Loss: 2.0938, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 183, Loss: 2.0992, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 184, Loss: 2.1385, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 185, Loss: 2.1040, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 186, Loss: 2.1048, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 187, Loss: 2.1162, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 188, Loss: 2.1257, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 189, Loss: 2.0915, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 190, Loss: 2.1498, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 191, Loss: 2.0987, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 192, Loss: 2.0882, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 193, Loss: 2.1236, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 194, Loss: 2.1292, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 195, Loss: 2.1204, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 196, Loss: 2.0997, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 197, Loss: 2.1109, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 198, Loss: 2.0858, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 199, Loss: 2.0980, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 200, Loss: 2.0969, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 201, Loss: 2.0931, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 202, Loss: 2.1004, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 203, Loss: 2.1182, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 204, Loss: 2.0999, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 205, Loss: 2.1125, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 206, Loss: 2.0766, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 207, Loss: 2.1086, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 208, Loss: 2.0969, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 209, Loss: 2.1076, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 210, Loss: 2.0772, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 211, Loss: 2.1069, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 212, Loss: 2.1581, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 213, Loss: 2.0731, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 214, Loss: 2.1602, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 215, Loss: 2.1152, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 216, Loss: 2.0705, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 217, Loss: 2.0830, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 218, Loss: 2.1336, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 219, Loss: 2.0909, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 220, Loss: 2.1010, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 221, Loss: 2.1152, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 222, Loss: 2.0919, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 223, Loss: 2.0763, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 224, Loss: 2.0909, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 225, Loss: 2.1109, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 226, Loss: 2.0881, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 227, Loss: 2.0397, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 228, Loss: 2.0691, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 229, Loss: 2.0916, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 230, Loss: 2.0352, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 231, Loss: 2.0451, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 232, Loss: 2.0550, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 233, Loss: 2.0623, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 234, Loss: 2.0525, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 235, Loss: 2.0198, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 236, Loss: 2.0577, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 237, Loss: 2.0308, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 238, Loss: 2.0151, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 239, Loss: 2.0281, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 240, Loss: 2.0328, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 241, Loss: 2.0135, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 242, Loss: 2.0377, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 243, Loss: 2.0460, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 244, Loss: 2.0096, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 245, Loss: 2.0514, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 246, Loss: 1.9762, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 247, Loss: 1.9892, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 248, Loss: 1.9956, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 249, Loss: 1.9819, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 250, Loss: 1.9377, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 251, Loss: 1.9190, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 252, Loss: 1.9194, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 253, Loss: 1.9396, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 254, Loss: 1.8907, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 255, Loss: 1.8915, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 256, Loss: 1.9426, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 257, Loss: 1.8824, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 258, Loss: 1.8446, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 259, Loss: 1.8180, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 260, Loss: 1.8662, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 261, Loss: 1.8107, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 262, Loss: 1.7582, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 263, Loss: 1.7285, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 264, Loss: 1.6661, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 265, Loss: 1.6356, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 266, Loss: 1.6488, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 267, Loss: 1.6939, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 268, Loss: 1.5905, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 269, Loss: 1.5249, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 270, Loss: 1.6061, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 271, Loss: 1.5027, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 272, Loss: 1.5284, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 273, Loss: 1.3402, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 274, Loss: 1.5492, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 275, Loss: 1.5303, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 276, Loss: 1.5445, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 277, Loss: 1.4104, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 278, Loss: 1.4918, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 279, Loss: 1.4273, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 280, Loss: 1.4187, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 281, Loss: 1.3625, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 282, Loss: 1.4270, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 283, Loss: 1.4230, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 284, Loss: 1.3779, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 285, Loss: 1.3382, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 286, Loss: 1.3253, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 287, Loss: 1.4523, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 288, Loss: 1.3545, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 289, Loss: 1.3514, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 290, Loss: 1.3631, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 291, Loss: 1.2812, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 292, Loss: 1.3228, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 293, Loss: 1.3449, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 294, Loss: 1.4055, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 295, Loss: 1.2899, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 296, Loss: 1.3960, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 297, Loss: 1.4548, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 298, Loss: 1.2579, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 299, Loss: 1.3927, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 300, Loss: 1.3351, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 301, Loss: 1.3161, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 302, Loss: 1.3560, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 303, Loss: 1.4285, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 304, Loss: 1.3129, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 305, Loss: 1.3207, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 306, Loss: 1.3133, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 307, Loss: 1.3595, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 308, Loss: 1.2398, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 309, Loss: 1.2574, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 310, Loss: 1.2968, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 311, Loss: 1.4603, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 312, Loss: 1.2792, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 313, Loss: 1.3213, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 314, Loss: 1.3193, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 315, Loss: 1.2626, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 316, Loss: 1.3511, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 317, Loss: 1.3304, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 318, Loss: 1.2561, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 319, Loss: 1.2765, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 320, Loss: 1.3133, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 321, Loss: 1.3391, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 322, Loss: 1.3310, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 323, Loss: 1.2925, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 324, Loss: 1.2241, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 325, Loss: 1.2950, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 326, Loss: 1.3130, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 327, Loss: 1.2814, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 328, Loss: 1.2658, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 329, Loss: 1.3199, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 330, Loss: 1.2960, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 331, Loss: 1.3084, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 332, Loss: 1.3005, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 333, Loss: 1.2910, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 334, Loss: 1.3332, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 335, Loss: 1.2843, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 336, Loss: 1.3715, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 337, Loss: 1.3092, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 338, Loss: 1.2669, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 339, Loss: 1.2885, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 340, Loss: 1.2902, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 341, Loss: 1.2522, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 342, Loss: 1.3034, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 343, Loss: 1.2690, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 344, Loss: 1.1906, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 345, Loss: 1.2013, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 346, Loss: 1.2946, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 347, Loss: 1.2188, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 348, Loss: 1.2055, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 349, Loss: 1.1816, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 350, Loss: 1.2465, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 351, Loss: 1.2571, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 352, Loss: 1.1736, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 353, Loss: 1.2212, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 354, Loss: 1.3244, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 355, Loss: 1.1395, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 356, Loss: 1.1888, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 357, Loss: 1.1748, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 358, Loss: 1.1873, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 359, Loss: 1.2257, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 360, Loss: 1.2016, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 361, Loss: 1.2576, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 362, Loss: 1.2085, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 363, Loss: 1.1885, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 364, Loss: 1.2457, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 365, Loss: 1.1445, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 366, Loss: 1.2719, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 367, Loss: 1.3238, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 368, Loss: 1.0849, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 369, Loss: 1.1607, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 370, Loss: 1.2296, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 371, Loss: 1.3981, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 372, Loss: 1.2530, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 373, Loss: 1.1713, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 374, Loss: 1.5772, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 375, Loss: 1.2530, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 376, Loss: 1.1798, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 377, Loss: 1.2900, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 378, Loss: 1.2487, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 379, Loss: 1.2280, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 380, Loss: 1.1066, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 381, Loss: 1.1887, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 382, Loss: 1.3142, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 383, Loss: 1.1562, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 384, Loss: 1.1815, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 385, Loss: 1.1462, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 386, Loss: 1.1750, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 387, Loss: 1.2775, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 388, Loss: 1.1033, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 389, Loss: 1.2328, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 390, Loss: 1.1899, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 391, Loss: 1.1767, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 392, Loss: 1.0789, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 393, Loss: 1.0864, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 394, Loss: 1.1338, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 395, Loss: 1.3048, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 396, Loss: 1.1392, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 397, Loss: 1.0221, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 398, Loss: 1.1944, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 399, Loss: 1.1796, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 400, Loss: 1.0257, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 401, Loss: 1.0570, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 402, Loss: 1.0887, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 403, Loss: 1.1383, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 404, Loss: 1.0390, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 405, Loss: 0.9967, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 406, Loss: 1.0590, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 407, Loss: 1.1057, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 408, Loss: 0.9355, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 409, Loss: 1.0003, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 410, Loss: 0.9526, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 411, Loss: 1.0403, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 412, Loss: 1.0273, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 413, Loss: 1.0671, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 414, Loss: 1.0009, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 415, Loss: 1.0882, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 416, Loss: 0.9577, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 417, Loss: 1.0378, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 418, Loss: 0.8989, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 419, Loss: 0.9502, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 420, Loss: 1.0308, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 421, Loss: 0.8998, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 422, Loss: 0.9276, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 423, Loss: 0.8810, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 424, Loss: 0.9272, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 425, Loss: 0.9614, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 426, Loss: 0.8399, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 427, Loss: 1.0201, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 428, Loss: 0.8566, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 429, Loss: 0.9246, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 430, Loss: 0.8430, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 431, Loss: 0.9972, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 432, Loss: 0.9563, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 433, Loss: 0.9115, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 434, Loss: 0.8979, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 435, Loss: 0.9001, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 436, Loss: 0.8500, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 437, Loss: 0.9318, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 438, Loss: 1.0037, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 439, Loss: 0.9016, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 440, Loss: 0.8403, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 441, Loss: 0.9823, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 442, Loss: 0.8824, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 443, Loss: 0.8898, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 444, Loss: 0.8781, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 445, Loss: 1.0778, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 446, Loss: 0.8047, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 447, Loss: 0.9080, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 448, Loss: 0.9409, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 449, Loss: 0.8490, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 450, Loss: 0.8635, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 451, Loss: 0.9051, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 452, Loss: 0.9715, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 453, Loss: 0.8892, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 454, Loss: 0.8172, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 455, Loss: 0.8825, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 456, Loss: 0.8826, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 457, Loss: 0.8240, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 458, Loss: 0.8230, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 459, Loss: 0.8064, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 460, Loss: 0.8142, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 461, Loss: 0.7568, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 462, Loss: 0.7759, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 463, Loss: 0.8533, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 464, Loss: 0.7551, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 465, Loss: 0.7710, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 466, Loss: 0.8433, Train 0.3837, Val 0.2969\n",
      "Fold: 4, Epoch: 467, Loss: 0.8741, Train 0.3837, Val 0.2969\n",
      "Fold: 4, Epoch: 468, Loss: 0.7333, Train 0.3837, Val 0.2969\n",
      "Fold: 4, Epoch: 469, Loss: 0.7939, Train 0.3886, Val 0.2969\n",
      "Fold: 4, Epoch: 470, Loss: 0.8099, Train 0.3911, Val 0.2969\n",
      "Fold: 4, Epoch: 471, Loss: 0.9436, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 472, Loss: 0.8227, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 473, Loss: 0.7552, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 474, Loss: 0.9477, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 475, Loss: 0.8625, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 476, Loss: 0.7605, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 477, Loss: 0.8409, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 478, Loss: 0.8309, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 479, Loss: 0.8038, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 480, Loss: 0.7649, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 481, Loss: 0.7921, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 482, Loss: 0.7006, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 483, Loss: 0.7623, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 484, Loss: 0.7592, Train 0.3837, Val 0.2969\n",
      "Fold: 4, Epoch: 485, Loss: 0.7252, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 486, Loss: 0.6679, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 487, Loss: 0.7101, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 488, Loss: 0.6964, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 489, Loss: 0.8103, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 490, Loss: 0.7119, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 491, Loss: 0.6253, Train 0.4183, Val 0.3281\n",
      "Fold: 4, Epoch: 492, Loss: 0.9005, Train 0.6460, Val 0.6875\n",
      "Fold: 4, Epoch: 493, Loss: 0.9991, Train 0.5322, Val 0.4844\n",
      "Fold: 4, Epoch: 494, Loss: 0.7429, Train 0.4183, Val 0.2969\n",
      "Fold: 4, Epoch: 495, Loss: 0.6184, Train 0.4431, Val 0.3594\n",
      "Fold: 4, Epoch: 496, Loss: 0.7328, Train 0.3515, Val 0.2344\n",
      "Fold: 4, Epoch: 497, Loss: 0.7793, Train 0.2624, Val 0.1562\n",
      "Fold: 4, Epoch: 498, Loss: 0.6595, Train 0.2723, Val 0.1719\n",
      "Fold: 4, Epoch: 499, Loss: 0.7587, Train 0.6040, Val 0.5469\n",
      "Fold: 4, Epoch: 500, Loss: 0.7684, Train 0.5842, Val 0.5469\n",
      "Fold: 4, Epoch: 501, Loss: 0.9762, Train 0.3837, Val 0.2500\n",
      "Fold: 4, Epoch: 502, Loss: 0.6544, Train 0.3292, Val 0.2344\n",
      "Fold: 4, Epoch: 503, Loss: 0.6472, Train 0.3936, Val 0.2656\n",
      "Fold: 4, Epoch: 504, Loss: 0.6488, Train 0.4728, Val 0.3750\n",
      "Fold: 4, Epoch: 505, Loss: 0.8846, Train 0.4777, Val 0.3750\n",
      "Fold: 4, Epoch: 506, Loss: 0.6277, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 507, Loss: 0.6178, Train 0.7450, Val 0.7344\n",
      "Fold: 4, Epoch: 508, Loss: 0.7400, Train 0.7723, Val 0.7500\n",
      "Fold: 4, Epoch: 509, Loss: 0.7438, Train 0.7351, Val 0.7188\n",
      "Fold: 4, Epoch: 510, Loss: 0.6719, Train 0.4802, Val 0.3750\n",
      "Fold: 4, Epoch: 511, Loss: 0.5502, Train 0.4703, Val 0.3750\n",
      "Fold: 4, Epoch: 512, Loss: 0.7090, Train 0.4505, Val 0.3594\n",
      "Fold: 4, Epoch: 513, Loss: 0.6489, Train 0.4134, Val 0.2969\n",
      "Fold: 4, Epoch: 514, Loss: 0.5961, Train 0.3490, Val 0.2500\n",
      "Fold: 4, Epoch: 515, Loss: 0.5686, Train 0.3589, Val 0.2344\n",
      "Fold: 4, Epoch: 516, Loss: 0.6387, Train 0.4505, Val 0.3594\n",
      "Fold: 4, Epoch: 517, Loss: 0.5807, Train 0.4629, Val 0.3594\n",
      "Fold: 4, Epoch: 518, Loss: 0.6060, Train 0.4208, Val 0.2656\n",
      "Fold: 4, Epoch: 519, Loss: 0.6042, Train 0.3960, Val 0.2656\n",
      "Fold: 4, Epoch: 520, Loss: 0.5709, Train 0.4035, Val 0.2656\n",
      "Fold: 4, Epoch: 521, Loss: 0.7084, Train 0.4530, Val 0.3281\n",
      "Fold: 4, Epoch: 522, Loss: 0.6360, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 523, Loss: 0.6330, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 524, Loss: 0.5471, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 525, Loss: 0.5264, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 526, Loss: 0.4523, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 527, Loss: 0.5901, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 528, Loss: 0.5289, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 529, Loss: 0.5579, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 530, Loss: 0.5751, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 531, Loss: 0.5468, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 532, Loss: 0.4626, Train 0.4827, Val 0.3750\n",
      "Fold: 4, Epoch: 533, Loss: 0.5811, Train 0.4703, Val 0.3750\n",
      "Fold: 4, Epoch: 534, Loss: 0.5313, Train 0.4134, Val 0.2344\n",
      "Fold: 4, Epoch: 535, Loss: 0.5854, Train 0.2946, Val 0.1719\n",
      "Fold: 4, Epoch: 536, Loss: 0.6055, Train 0.3391, Val 0.2031\n",
      "Fold: 4, Epoch: 537, Loss: 0.4820, Train 0.3812, Val 0.2500\n",
      "Fold: 4, Epoch: 538, Loss: 0.5167, Train 0.4084, Val 0.2500\n",
      "Fold: 4, Epoch: 539, Loss: 0.5272, Train 0.3465, Val 0.2188\n",
      "Fold: 4, Epoch: 540, Loss: 0.4369, Train 0.2772, Val 0.1719\n",
      "Fold: 4, Epoch: 541, Loss: 0.5536, Train 0.1955, Val 0.0938\n",
      "Fold: 4, Epoch: 542, Loss: 0.5522, Train 0.1460, Val 0.0938\n",
      "Fold: 4, Epoch: 543, Loss: 0.5656, Train 0.1361, Val 0.0938\n",
      "Fold: 4, Epoch: 544, Loss: 0.6059, Train 0.1584, Val 0.0938\n",
      "Fold: 4, Epoch: 545, Loss: 0.5901, Train 0.1559, Val 0.0938\n",
      "Fold: 4, Epoch: 546, Loss: 0.6215, Train 0.1807, Val 0.0938\n",
      "Fold: 4, Epoch: 547, Loss: 0.6407, Train 0.2203, Val 0.1094\n",
      "Fold: 4, Epoch: 548, Loss: 0.4486, Train 0.4678, Val 0.3750\n",
      "Fold: 4, Epoch: 549, Loss: 0.6487, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 550, Loss: 0.6017, Train 0.5941, Val 0.5156\n",
      "Fold: 4, Epoch: 551, Loss: 0.5760, Train 0.7525, Val 0.7344\n",
      "Fold: 4, Epoch: 552, Loss: 0.5611, Train 0.7871, Val 0.7656\n",
      "Fold: 4, Epoch: 553, Loss: 0.5262, Train 0.7871, Val 0.7656\n",
      "Fold: 4, Epoch: 554, Loss: 0.7195, Train 0.6386, Val 0.5625\n",
      "Fold: 4, Epoch: 555, Loss: 0.4142, Train 0.4827, Val 0.3750\n",
      "Fold: 4, Epoch: 556, Loss: 0.5612, Train 0.4455, Val 0.3438\n",
      "Fold: 4, Epoch: 557, Loss: 0.5140, Train 0.3713, Val 0.2500\n",
      "Fold: 4, Epoch: 558, Loss: 0.5279, Train 0.3045, Val 0.2031\n",
      "Fold: 4, Epoch: 559, Loss: 0.5250, Train 0.3292, Val 0.2188\n",
      "Fold: 4, Epoch: 560, Loss: 0.5448, Train 0.4579, Val 0.3594\n",
      "Fold: 4, Epoch: 561, Loss: 0.4796, Train 0.4827, Val 0.3750\n",
      "Fold: 4, Epoch: 562, Loss: 0.4198, Train 0.4851, Val 0.3750\n",
      "Fold: 4, Epoch: 563, Loss: 0.4834, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 564, Loss: 0.5019, Train 0.5446, Val 0.4062\n",
      "Fold: 4, Epoch: 565, Loss: 0.4474, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 566, Loss: 0.4776, Train 0.4851, Val 0.3750\n",
      "Fold: 4, Epoch: 567, Loss: 0.5040, Train 0.4851, Val 0.3750\n",
      "Fold: 4, Epoch: 568, Loss: 0.5521, Train 0.4851, Val 0.3750\n",
      "Fold: 4, Epoch: 569, Loss: 0.4828, Train 0.4777, Val 0.3750\n",
      "Fold: 4, Epoch: 570, Loss: 0.4171, Train 0.4653, Val 0.3438\n",
      "Fold: 4, Epoch: 571, Loss: 0.5820, Train 0.4208, Val 0.2812\n",
      "Fold: 4, Epoch: 572, Loss: 0.3742, Train 0.3094, Val 0.1875\n",
      "Fold: 4, Epoch: 573, Loss: 0.5287, Train 0.3540, Val 0.2188\n",
      "Fold: 4, Epoch: 574, Loss: 0.5824, Train 0.4926, Val 0.3438\n",
      "Fold: 4, Epoch: 575, Loss: 0.5624, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 576, Loss: 0.5333, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 577, Loss: 0.5393, Train 0.4802, Val 0.3750\n",
      "Fold: 4, Epoch: 578, Loss: 0.5836, Train 0.4802, Val 0.3750\n",
      "Fold: 4, Epoch: 579, Loss: 0.4386, Train 0.4678, Val 0.3750\n",
      "Fold: 4, Epoch: 580, Loss: 0.3911, Train 0.3738, Val 0.2500\n",
      "Fold: 4, Epoch: 581, Loss: 0.5087, Train 0.2153, Val 0.1094\n",
      "Fold: 4, Epoch: 582, Loss: 0.4557, Train 0.2030, Val 0.1094\n",
      "Fold: 4, Epoch: 583, Loss: 0.5199, Train 0.4678, Val 0.4688\n",
      "Fold: 4, Epoch: 584, Loss: 0.4323, Train 0.4851, Val 0.4844\n",
      "Fold: 4, Epoch: 585, Loss: 0.6459, Train 0.4208, Val 0.3281\n",
      "Fold: 4, Epoch: 586, Loss: 0.4795, Train 0.3911, Val 0.2500\n",
      "Fold: 4, Epoch: 587, Loss: 0.4186, Train 0.4381, Val 0.2969\n",
      "Fold: 4, Epoch: 588, Loss: 0.6628, Train 0.4703, Val 0.3750\n",
      "Fold: 4, Epoch: 589, Loss: 0.4769, Train 0.4802, Val 0.3750\n",
      "Fold: 4, Epoch: 590, Loss: 0.4759, Train 0.4802, Val 0.3594\n",
      "Fold: 4, Epoch: 591, Loss: 0.5045, Train 0.4901, Val 0.3594\n",
      "Fold: 4, Epoch: 592, Loss: 0.5020, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 593, Loss: 0.4507, Train 0.7178, Val 0.6875\n",
      "Fold: 4, Epoch: 594, Loss: 0.5853, Train 0.7277, Val 0.7188\n",
      "Fold: 4, Epoch: 595, Loss: 0.4882, Train 0.7351, Val 0.7344\n",
      "Fold: 4, Epoch: 596, Loss: 0.5392, Train 0.7203, Val 0.7188\n",
      "Fold: 4, Epoch: 597, Loss: 0.5466, Train 0.6262, Val 0.5469\n",
      "Fold: 4, Epoch: 598, Loss: 0.5610, Train 0.5817, Val 0.4375\n",
      "Fold: 4, Epoch: 599, Loss: 0.4487, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 600, Loss: 0.4432, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 601, Loss: 0.4826, Train 0.3663, Val 0.2500\n",
      "Fold: 4, Epoch: 602, Loss: 0.5711, Train 0.2129, Val 0.1250\n",
      "Fold: 4, Epoch: 603, Loss: 0.7125, Train 0.2277, Val 0.1406\n",
      "Fold: 4, Epoch: 604, Loss: 0.4792, Train 0.4332, Val 0.2500\n",
      "Fold: 4, Epoch: 605, Loss: 0.4077, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 606, Loss: 0.4854, Train 0.5421, Val 0.4219\n",
      "Fold: 4, Epoch: 607, Loss: 0.5206, Train 0.5743, Val 0.4531\n",
      "Fold: 4, Epoch: 608, Loss: 0.4392, Train 0.5743, Val 0.4531\n",
      "Fold: 4, Epoch: 609, Loss: 0.4046, Train 0.5767, Val 0.4531\n",
      "Fold: 4, Epoch: 610, Loss: 0.4000, Train 0.4728, Val 0.3438\n",
      "Fold: 4, Epoch: 611, Loss: 0.4947, Train 0.2500, Val 0.1094\n",
      "Fold: 4, Epoch: 612, Loss: 0.4605, Train 0.2475, Val 0.1094\n",
      "Fold: 4, Epoch: 613, Loss: 0.3932, Train 0.2302, Val 0.1094\n",
      "Fold: 4, Epoch: 614, Loss: 0.5136, Train 0.2178, Val 0.1094\n",
      "Fold: 4, Epoch: 615, Loss: 0.3739, Train 0.1931, Val 0.1094\n",
      "Fold: 4, Epoch: 616, Loss: 0.4795, Train 0.1832, Val 0.1094\n",
      "Fold: 4, Epoch: 617, Loss: 0.4311, Train 0.1832, Val 0.1094\n",
      "Fold: 4, Epoch: 618, Loss: 0.4246, Train 0.1708, Val 0.0938\n",
      "Fold: 4, Epoch: 619, Loss: 0.4750, Train 0.1708, Val 0.0938\n",
      "Fold: 4, Epoch: 620, Loss: 0.6266, Train 0.1832, Val 0.0938\n",
      "Fold: 4, Epoch: 621, Loss: 0.4952, Train 0.2079, Val 0.1250\n",
      "Fold: 4, Epoch: 622, Loss: 0.4219, Train 0.3837, Val 0.3750\n",
      "Fold: 4, Epoch: 623, Loss: 0.4634, Train 0.5792, Val 0.5625\n",
      "Fold: 4, Epoch: 624, Loss: 0.4679, Train 0.7970, Val 0.7656\n",
      "Fold: 4, Epoch: 625, Loss: 0.4344, Train 0.7970, Val 0.7656\n",
      "Fold: 4, Epoch: 626, Loss: 0.4926, Train 0.7847, Val 0.7656\n",
      "Fold: 4, Epoch: 627, Loss: 0.5322, Train 0.7426, Val 0.7344\n",
      "Fold: 4, Epoch: 628, Loss: 0.5407, Train 0.5866, Val 0.4688\n",
      "Fold: 4, Epoch: 629, Loss: 0.3540, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 630, Loss: 0.4520, Train 0.4728, Val 0.3594\n",
      "Fold: 4, Epoch: 631, Loss: 0.4143, Train 0.3144, Val 0.1719\n",
      "Fold: 4, Epoch: 632, Loss: 0.5116, Train 0.2550, Val 0.1719\n",
      "Fold: 4, Epoch: 633, Loss: 0.4794, Train 0.4109, Val 0.2656\n",
      "Fold: 4, Epoch: 634, Loss: 0.4807, Train 0.4752, Val 0.3594\n",
      "Fold: 4, Epoch: 635, Loss: 0.4962, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 636, Loss: 0.4411, Train 0.4802, Val 0.3750\n",
      "Fold: 4, Epoch: 637, Loss: 0.4764, Train 0.4827, Val 0.3750\n",
      "Fold: 4, Epoch: 638, Loss: 0.4394, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 639, Loss: 0.4805, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 640, Loss: 0.4270, Train 0.6634, Val 0.5938\n",
      "Fold: 4, Epoch: 641, Loss: 0.4126, Train 0.6807, Val 0.6250\n",
      "Fold: 4, Epoch: 642, Loss: 0.4025, Train 0.6485, Val 0.5469\n",
      "Fold: 4, Epoch: 643, Loss: 0.4885, Train 0.4851, Val 0.3906\n",
      "Fold: 4, Epoch: 644, Loss: 0.4818, Train 0.4604, Val 0.3438\n",
      "Fold: 4, Epoch: 645, Loss: 0.4410, Train 0.4455, Val 0.3125\n",
      "Fold: 4, Epoch: 646, Loss: 0.3763, Train 0.4505, Val 0.3438\n",
      "Fold: 4, Epoch: 647, Loss: 0.3931, Train 0.4431, Val 0.3281\n",
      "Fold: 4, Epoch: 648, Loss: 0.4768, Train 0.4703, Val 0.3594\n",
      "Fold: 4, Epoch: 649, Loss: 0.4066, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 650, Loss: 0.5059, Train 0.5891, Val 0.4844\n",
      "Fold: 4, Epoch: 651, Loss: 0.4664, Train 0.6881, Val 0.6562\n",
      "Fold: 4, Epoch: 652, Loss: 0.4064, Train 0.6040, Val 0.5625\n",
      "Fold: 4, Epoch: 653, Loss: 0.3712, Train 0.5322, Val 0.5000\n",
      "Fold: 4, Epoch: 654, Loss: 0.4312, Train 0.5272, Val 0.4844\n",
      "Fold: 4, Epoch: 655, Loss: 0.4157, Train 0.4777, Val 0.4219\n",
      "Fold: 4, Epoch: 656, Loss: 0.3084, Train 0.2946, Val 0.2031\n",
      "Fold: 4, Epoch: 657, Loss: 0.4870, Train 0.2500, Val 0.1562\n",
      "Fold: 4, Epoch: 658, Loss: 0.3088, Train 0.2921, Val 0.1719\n",
      "Fold: 4, Epoch: 659, Loss: 0.4134, Train 0.4406, Val 0.2812\n",
      "Fold: 4, Epoch: 660, Loss: 0.4082, Train 0.4950, Val 0.3594\n",
      "Fold: 4, Epoch: 661, Loss: 0.4680, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 662, Loss: 0.4748, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 663, Loss: 0.4669, Train 0.5000, Val 0.3594\n",
      "Fold: 4, Epoch: 664, Loss: 0.3787, Train 0.4926, Val 0.3594\n",
      "Fold: 4, Epoch: 665, Loss: 0.4736, Train 0.4876, Val 0.3594\n",
      "Fold: 4, Epoch: 666, Loss: 0.3794, Train 0.4728, Val 0.3438\n",
      "Fold: 4, Epoch: 667, Loss: 0.4481, Train 0.4752, Val 0.3594\n",
      "Fold: 4, Epoch: 668, Loss: 0.5116, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 669, Loss: 0.4347, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 670, Loss: 0.3352, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 671, Loss: 0.5001, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 672, Loss: 0.3742, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 673, Loss: 0.3965, Train 0.5446, Val 0.4531\n",
      "Fold: 4, Epoch: 674, Loss: 0.4574, Train 0.7327, Val 0.7188\n",
      "Fold: 4, Epoch: 675, Loss: 0.4377, Train 0.7450, Val 0.7344\n",
      "Fold: 4, Epoch: 676, Loss: 0.3526, Train 0.6782, Val 0.6094\n",
      "Fold: 4, Epoch: 677, Loss: 0.4446, Train 0.5124, Val 0.3750\n",
      "Fold: 4, Epoch: 678, Loss: 0.3720, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 679, Loss: 0.3402, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 680, Loss: 0.3915, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 681, Loss: 0.3638, Train 0.4827, Val 0.3594\n",
      "Fold: 4, Epoch: 682, Loss: 0.3150, Train 0.4752, Val 0.3594\n",
      "Fold: 4, Epoch: 683, Loss: 0.4109, Train 0.4728, Val 0.3438\n",
      "Fold: 4, Epoch: 684, Loss: 0.3203, Train 0.4703, Val 0.3594\n",
      "Fold: 4, Epoch: 685, Loss: 0.3717, Train 0.4728, Val 0.3594\n",
      "Fold: 4, Epoch: 686, Loss: 0.3648, Train 0.4901, Val 0.3906\n",
      "Fold: 4, Epoch: 687, Loss: 0.3266, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 688, Loss: 0.3258, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 689, Loss: 0.3824, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 690, Loss: 0.5966, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 691, Loss: 0.4590, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 692, Loss: 0.3107, Train 0.5050, Val 0.3750\n",
      "Fold: 4, Epoch: 693, Loss: 0.3046, Train 0.5817, Val 0.5156\n",
      "Fold: 4, Epoch: 694, Loss: 0.4482, Train 0.5371, Val 0.4531\n",
      "Fold: 4, Epoch: 695, Loss: 0.3140, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 696, Loss: 0.4637, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 697, Loss: 0.3609, Train 0.4802, Val 0.3594\n",
      "Fold: 4, Epoch: 698, Loss: 0.3759, Train 0.4802, Val 0.3594\n",
      "Fold: 4, Epoch: 699, Loss: 0.4549, Train 0.4802, Val 0.3594\n",
      "Fold: 4, Epoch: 700, Loss: 0.3907, Train 0.4777, Val 0.3594\n",
      "Fold: 4, Epoch: 701, Loss: 0.4072, Train 0.4851, Val 0.3594\n",
      "Fold: 4, Epoch: 702, Loss: 0.3623, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 703, Loss: 0.4217, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 704, Loss: 0.3431, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 705, Loss: 0.4076, Train 0.4926, Val 0.3594\n",
      "Fold: 4, Epoch: 706, Loss: 0.3582, Train 0.4876, Val 0.3594\n",
      "Fold: 4, Epoch: 707, Loss: 0.3770, Train 0.4827, Val 0.3750\n",
      "Fold: 4, Epoch: 708, Loss: 0.3888, Train 0.4579, Val 0.3750\n",
      "Fold: 4, Epoch: 709, Loss: 0.3092, Train 0.4851, Val 0.3750\n",
      "Fold: 4, Epoch: 710, Loss: 0.3685, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 711, Loss: 0.2838, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 712, Loss: 0.2828, Train 0.4926, Val 0.3906\n",
      "Fold: 4, Epoch: 713, Loss: 0.3658, Train 0.4901, Val 0.3906\n",
      "Fold: 4, Epoch: 714, Loss: 0.4296, Train 0.4926, Val 0.3906\n",
      "Fold: 4, Epoch: 715, Loss: 0.3512, Train 0.4901, Val 0.3906\n",
      "Fold: 4, Epoch: 716, Loss: 0.3402, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 717, Loss: 0.3379, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 718, Loss: 0.5024, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 719, Loss: 0.4476, Train 0.5124, Val 0.3750\n",
      "Fold: 4, Epoch: 720, Loss: 0.4127, Train 0.5619, Val 0.4688\n",
      "Fold: 4, Epoch: 721, Loss: 0.5210, Train 0.5124, Val 0.3750\n",
      "Fold: 4, Epoch: 722, Loss: 0.3153, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 723, Loss: 0.3567, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 724, Loss: 0.3711, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 725, Loss: 0.3587, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 726, Loss: 0.3933, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 727, Loss: 0.4741, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 728, Loss: 0.3923, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 729, Loss: 0.3173, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 730, Loss: 0.3393, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 731, Loss: 0.4539, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 732, Loss: 0.4492, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 733, Loss: 0.3857, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 734, Loss: 0.4841, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 735, Loss: 0.4472, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 736, Loss: 0.3330, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 737, Loss: 0.3494, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 738, Loss: 0.3280, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 739, Loss: 0.3315, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 740, Loss: 0.3822, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 741, Loss: 0.2678, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 742, Loss: 0.3899, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 743, Loss: 0.3140, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 744, Loss: 0.3484, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 745, Loss: 0.3931, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 746, Loss: 0.4530, Train 0.4802, Val 0.3594\n",
      "Fold: 4, Epoch: 747, Loss: 0.4067, Train 0.4752, Val 0.3594\n",
      "Fold: 4, Epoch: 748, Loss: 0.4373, Train 0.4728, Val 0.3594\n",
      "Fold: 4, Epoch: 749, Loss: 0.3783, Train 0.4752, Val 0.3594\n",
      "Fold: 4, Epoch: 750, Loss: 0.3640, Train 0.4802, Val 0.3750\n",
      "Fold: 4, Epoch: 751, Loss: 0.3164, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 752, Loss: 0.3894, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 753, Loss: 0.4283, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 754, Loss: 0.3518, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 755, Loss: 0.4838, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 756, Loss: 0.5348, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 757, Loss: 0.3407, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 758, Loss: 0.3189, Train 0.5198, Val 0.4219\n",
      "Fold: 4, Epoch: 759, Loss: 0.3542, Train 0.5347, Val 0.4219\n",
      "Fold: 4, Epoch: 760, Loss: 0.3614, Train 0.5223, Val 0.4375\n",
      "Fold: 4, Epoch: 761, Loss: 0.4105, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 762, Loss: 0.3769, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 763, Loss: 0.3795, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 764, Loss: 0.4285, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 765, Loss: 0.3931, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 766, Loss: 0.3649, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 767, Loss: 0.2843, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 768, Loss: 0.3255, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 769, Loss: 0.3637, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 770, Loss: 0.3758, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 771, Loss: 0.3303, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 772, Loss: 0.3730, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 773, Loss: 0.4436, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 774, Loss: 0.4034, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 775, Loss: 0.3457, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 776, Loss: 0.3333, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 777, Loss: 0.3251, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 778, Loss: 0.4065, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 779, Loss: 0.3118, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 780, Loss: 0.3156, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 781, Loss: 0.3952, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 782, Loss: 0.3773, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 783, Loss: 0.3660, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 784, Loss: 0.3228, Train 0.5099, Val 0.3906\n",
      "Fold: 4, Epoch: 785, Loss: 0.2457, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 786, Loss: 0.3268, Train 0.3688, Val 0.3125\n",
      "Fold: 4, Epoch: 787, Loss: 0.3752, Train 0.2129, Val 0.1406\n",
      "Fold: 4, Epoch: 788, Loss: 0.2879, Train 0.2129, Val 0.1406\n",
      "Fold: 4, Epoch: 789, Loss: 0.3448, Train 0.2822, Val 0.2344\n",
      "Fold: 4, Epoch: 790, Loss: 0.3136, Train 0.3515, Val 0.2812\n",
      "Fold: 4, Epoch: 791, Loss: 0.3668, Train 0.3812, Val 0.3281\n",
      "Fold: 4, Epoch: 792, Loss: 0.5007, Train 0.4851, Val 0.3906\n",
      "Fold: 4, Epoch: 793, Loss: 0.2610, Train 0.5124, Val 0.4062\n",
      "Fold: 4, Epoch: 794, Loss: 0.3349, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 795, Loss: 0.2789, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 796, Loss: 0.3333, Train 0.4950, Val 0.3750\n",
      "Fold: 4, Epoch: 797, Loss: 0.3944, Train 0.5050, Val 0.3750\n",
      "Fold: 4, Epoch: 798, Loss: 0.3654, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 799, Loss: 0.3104, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 800, Loss: 0.2970, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 801, Loss: 0.4422, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 802, Loss: 0.3598, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 803, Loss: 0.3882, Train 0.5149, Val 0.3906\n",
      "Fold: 4, Epoch: 804, Loss: 0.4153, Train 0.5173, Val 0.3906\n",
      "Fold: 4, Epoch: 805, Loss: 0.4430, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 806, Loss: 0.3878, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 807, Loss: 0.4576, Train 0.4926, Val 0.3906\n",
      "Fold: 4, Epoch: 808, Loss: 0.3849, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 809, Loss: 0.4710, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 810, Loss: 0.3365, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 811, Loss: 0.2666, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 812, Loss: 0.3448, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 813, Loss: 0.3144, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 814, Loss: 0.4332, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 815, Loss: 0.2826, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 816, Loss: 0.3704, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 817, Loss: 0.3896, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 818, Loss: 0.3910, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 819, Loss: 0.3652, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 820, Loss: 0.3093, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 821, Loss: 0.4041, Train 0.4926, Val 0.3906\n",
      "Fold: 4, Epoch: 822, Loss: 0.3201, Train 0.4926, Val 0.3906\n",
      "Fold: 4, Epoch: 823, Loss: 0.2916, Train 0.4827, Val 0.3906\n",
      "Fold: 4, Epoch: 824, Loss: 0.3591, Train 0.4777, Val 0.3906\n",
      "Fold: 4, Epoch: 825, Loss: 0.3636, Train 0.4777, Val 0.3906\n",
      "Fold: 4, Epoch: 826, Loss: 0.3998, Train 0.4901, Val 0.3906\n",
      "Fold: 4, Epoch: 827, Loss: 0.2962, Train 0.5025, Val 0.4062\n",
      "Fold: 4, Epoch: 828, Loss: 0.4612, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 829, Loss: 0.2714, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 830, Loss: 0.4405, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 831, Loss: 0.2961, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 832, Loss: 0.3598, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 833, Loss: 0.2735, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 834, Loss: 0.3106, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 835, Loss: 0.3371, Train 0.5074, Val 0.4062\n",
      "Fold: 4, Epoch: 836, Loss: 0.2939, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 837, Loss: 0.3359, Train 0.5000, Val 0.3750\n",
      "Fold: 4, Epoch: 838, Loss: 0.3441, Train 0.5198, Val 0.4062\n",
      "Fold: 4, Epoch: 839, Loss: 0.2977, Train 0.5198, Val 0.4062\n",
      "Fold: 4, Epoch: 840, Loss: 0.3090, Train 0.5099, Val 0.4062\n",
      "Fold: 4, Epoch: 841, Loss: 0.4140, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 842, Loss: 0.4301, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 843, Loss: 0.3109, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 844, Loss: 0.3693, Train 0.5099, Val 0.3906\n",
      "Fold: 4, Epoch: 845, Loss: 0.3408, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 846, Loss: 0.3659, Train 0.5099, Val 0.4062\n",
      "Fold: 4, Epoch: 847, Loss: 0.4192, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 848, Loss: 0.3437, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 849, Loss: 0.3229, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 850, Loss: 0.4193, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 851, Loss: 0.2319, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 852, Loss: 0.2952, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 853, Loss: 0.2807, Train 0.4926, Val 0.3906\n",
      "Fold: 4, Epoch: 854, Loss: 0.3175, Train 0.4653, Val 0.3750\n",
      "Fold: 4, Epoch: 855, Loss: 0.3738, Train 0.4728, Val 0.3906\n",
      "Fold: 4, Epoch: 856, Loss: 0.3315, Train 0.4777, Val 0.3750\n",
      "Fold: 4, Epoch: 857, Loss: 0.3313, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 858, Loss: 0.2873, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 859, Loss: 0.3677, Train 0.5173, Val 0.3906\n",
      "Fold: 4, Epoch: 860, Loss: 0.2382, Train 0.5272, Val 0.4062\n",
      "Fold: 4, Epoch: 861, Loss: 0.3875, Train 0.5248, Val 0.4062\n",
      "Fold: 4, Epoch: 862, Loss: 0.2719, Train 0.5198, Val 0.4062\n",
      "Fold: 4, Epoch: 863, Loss: 0.3168, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 864, Loss: 0.3174, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 865, Loss: 0.3241, Train 0.4876, Val 0.3750\n",
      "Fold: 4, Epoch: 866, Loss: 0.3901, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 867, Loss: 0.2994, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 868, Loss: 0.3194, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 869, Loss: 0.2825, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 870, Loss: 0.3604, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 871, Loss: 0.3891, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 872, Loss: 0.2971, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 873, Loss: 0.3171, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 874, Loss: 0.3652, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 875, Loss: 0.3451, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 876, Loss: 0.3833, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 877, Loss: 0.2003, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 878, Loss: 0.4225, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 879, Loss: 0.2536, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 880, Loss: 0.3519, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 881, Loss: 0.2927, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 882, Loss: 0.3302, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 883, Loss: 0.3004, Train 0.4678, Val 0.3750\n",
      "Fold: 4, Epoch: 884, Loss: 0.2838, Train 0.4554, Val 0.3750\n",
      "Fold: 4, Epoch: 885, Loss: 0.3298, Train 0.4480, Val 0.3750\n",
      "Fold: 4, Epoch: 886, Loss: 0.3770, Train 0.4752, Val 0.3750\n",
      "Fold: 4, Epoch: 887, Loss: 0.2695, Train 0.4802, Val 0.3750\n",
      "Fold: 4, Epoch: 888, Loss: 0.3102, Train 0.4950, Val 0.3594\n",
      "Fold: 4, Epoch: 889, Loss: 0.2831, Train 0.5124, Val 0.4062\n",
      "Fold: 4, Epoch: 890, Loss: 0.2870, Train 0.4926, Val 0.4062\n",
      "Fold: 4, Epoch: 891, Loss: 0.3500, Train 0.4728, Val 0.4062\n",
      "Fold: 4, Epoch: 892, Loss: 0.3149, Train 0.4728, Val 0.4219\n",
      "Fold: 4, Epoch: 893, Loss: 0.2831, Train 0.4703, Val 0.4219\n",
      "Fold: 4, Epoch: 894, Loss: 0.2993, Train 0.4381, Val 0.3438\n",
      "Fold: 4, Epoch: 895, Loss: 0.2984, Train 0.4752, Val 0.3906\n",
      "Fold: 4, Epoch: 896, Loss: 0.3364, Train 0.4282, Val 0.3438\n",
      "Fold: 4, Epoch: 897, Loss: 0.2726, Train 0.3861, Val 0.2812\n",
      "Fold: 4, Epoch: 898, Loss: 0.2785, Train 0.3564, Val 0.2656\n",
      "Fold: 4, Epoch: 899, Loss: 0.2858, Train 0.3317, Val 0.2188\n",
      "Fold: 4, Epoch: 900, Loss: 0.3532, Train 0.3490, Val 0.2344\n",
      "Fold: 4, Epoch: 901, Loss: 0.3689, Train 0.3441, Val 0.2656\n",
      "Fold: 4, Epoch: 902, Loss: 0.2456, Train 0.3589, Val 0.2969\n",
      "Fold: 4, Epoch: 903, Loss: 0.3705, Train 0.2797, Val 0.2344\n",
      "Fold: 4, Epoch: 904, Loss: 0.2987, Train 0.2649, Val 0.2188\n",
      "Fold: 4, Epoch: 905, Loss: 0.3985, Train 0.2550, Val 0.1719\n",
      "Fold: 4, Epoch: 906, Loss: 0.2595, Train 0.2748, Val 0.2031\n",
      "Fold: 4, Epoch: 907, Loss: 0.3130, Train 0.3020, Val 0.2188\n",
      "Fold: 4, Epoch: 908, Loss: 0.3530, Train 0.4480, Val 0.3906\n",
      "Fold: 4, Epoch: 909, Loss: 0.3845, Train 0.4901, Val 0.3906\n",
      "Fold: 4, Epoch: 910, Loss: 0.4561, Train 0.5074, Val 0.4062\n",
      "Fold: 4, Epoch: 911, Loss: 0.3500, Train 0.5099, Val 0.3906\n",
      "Fold: 4, Epoch: 912, Loss: 0.4425, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 913, Loss: 0.3331, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 914, Loss: 0.3108, Train 0.4728, Val 0.3750\n",
      "Fold: 4, Epoch: 915, Loss: 0.3687, Train 0.4406, Val 0.3750\n",
      "Fold: 4, Epoch: 916, Loss: 0.3405, Train 0.3614, Val 0.2812\n",
      "Fold: 4, Epoch: 917, Loss: 0.3884, Train 0.3465, Val 0.2656\n",
      "Fold: 4, Epoch: 918, Loss: 0.2868, Train 0.3589, Val 0.2812\n",
      "Fold: 4, Epoch: 919, Loss: 0.2710, Train 0.3762, Val 0.2812\n",
      "Fold: 4, Epoch: 920, Loss: 0.3646, Train 0.4208, Val 0.3438\n",
      "Fold: 4, Epoch: 921, Loss: 0.3463, Train 0.4653, Val 0.3906\n",
      "Fold: 4, Epoch: 922, Loss: 0.3220, Train 0.4827, Val 0.3906\n",
      "Fold: 4, Epoch: 923, Loss: 0.2428, Train 0.4827, Val 0.3906\n",
      "Fold: 4, Epoch: 924, Loss: 0.2842, Train 0.4678, Val 0.4062\n",
      "Fold: 4, Epoch: 925, Loss: 0.2914, Train 0.4282, Val 0.3281\n",
      "Fold: 4, Epoch: 926, Loss: 0.2775, Train 0.3515, Val 0.2656\n",
      "Fold: 4, Epoch: 927, Loss: 0.3846, Train 0.2426, Val 0.1719\n",
      "Fold: 4, Epoch: 928, Loss: 0.2952, Train 0.2153, Val 0.1562\n",
      "Fold: 4, Epoch: 929, Loss: 0.2562, Train 0.2129, Val 0.1406\n",
      "Fold: 4, Epoch: 930, Loss: 0.3792, Train 0.2129, Val 0.1406\n",
      "Fold: 4, Epoch: 931, Loss: 0.2694, Train 0.2921, Val 0.2031\n",
      "Fold: 4, Epoch: 932, Loss: 0.2654, Train 0.3688, Val 0.2812\n",
      "Fold: 4, Epoch: 933, Loss: 0.2769, Train 0.4332, Val 0.3438\n",
      "Fold: 4, Epoch: 934, Loss: 0.2866, Train 0.4554, Val 0.3906\n",
      "Fold: 4, Epoch: 935, Loss: 0.3183, Train 0.4752, Val 0.3906\n",
      "Fold: 4, Epoch: 936, Loss: 0.2920, Train 0.5025, Val 0.3906\n",
      "Fold: 4, Epoch: 937, Loss: 0.2867, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 938, Loss: 0.3334, Train 0.5099, Val 0.4062\n",
      "Fold: 4, Epoch: 939, Loss: 0.3108, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 940, Loss: 0.2844, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 941, Loss: 0.3135, Train 0.4802, Val 0.3750\n",
      "Fold: 4, Epoch: 942, Loss: 0.2623, Train 0.4579, Val 0.3750\n",
      "Fold: 4, Epoch: 943, Loss: 0.3611, Train 0.4332, Val 0.3594\n",
      "Fold: 4, Epoch: 944, Loss: 0.2218, Train 0.4257, Val 0.3750\n",
      "Fold: 4, Epoch: 945, Loss: 0.2854, Train 0.4035, Val 0.3281\n",
      "Fold: 4, Epoch: 946, Loss: 0.2071, Train 0.3688, Val 0.2812\n",
      "Fold: 4, Epoch: 947, Loss: 0.2937, Train 0.4109, Val 0.3281\n",
      "Fold: 4, Epoch: 948, Loss: 0.2367, Train 0.4703, Val 0.3906\n",
      "Fold: 4, Epoch: 949, Loss: 0.3316, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 950, Loss: 0.2794, Train 0.5173, Val 0.4062\n",
      "Fold: 4, Epoch: 951, Loss: 0.2485, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 952, Loss: 0.3880, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 953, Loss: 0.2646, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 954, Loss: 0.3498, Train 0.5223, Val 0.4062\n",
      "Fold: 4, Epoch: 955, Loss: 0.3498, Train 0.5173, Val 0.4062\n",
      "Fold: 4, Epoch: 956, Loss: 0.2454, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 957, Loss: 0.3402, Train 0.4703, Val 0.3906\n",
      "Fold: 4, Epoch: 958, Loss: 0.3518, Train 0.4926, Val 0.3906\n",
      "Fold: 4, Epoch: 959, Loss: 0.2562, Train 0.5050, Val 0.3906\n",
      "Fold: 4, Epoch: 960, Loss: 0.2638, Train 0.5198, Val 0.4219\n",
      "Fold: 4, Epoch: 961, Loss: 0.3247, Train 0.5173, Val 0.4062\n",
      "Fold: 4, Epoch: 962, Loss: 0.3289, Train 0.5149, Val 0.4062\n",
      "Fold: 4, Epoch: 963, Loss: 0.4332, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 964, Loss: 0.3465, Train 0.4827, Val 0.3750\n",
      "Fold: 4, Epoch: 965, Loss: 0.3888, Train 0.4950, Val 0.3906\n",
      "Fold: 4, Epoch: 966, Loss: 0.3041, Train 0.5025, Val 0.4062\n",
      "Fold: 4, Epoch: 967, Loss: 0.2422, Train 0.5074, Val 0.4062\n",
      "Fold: 4, Epoch: 968, Loss: 0.3236, Train 0.5173, Val 0.4062\n",
      "Fold: 4, Epoch: 969, Loss: 0.2240, Train 0.5322, Val 0.4219\n",
      "Fold: 4, Epoch: 970, Loss: 0.3011, Train 0.5347, Val 0.4219\n",
      "Fold: 4, Epoch: 971, Loss: 0.2452, Train 0.5396, Val 0.4219\n",
      "Fold: 4, Epoch: 972, Loss: 0.4202, Train 0.5347, Val 0.4219\n",
      "Fold: 4, Epoch: 973, Loss: 0.4001, Train 0.5050, Val 0.4062\n",
      "Fold: 4, Epoch: 974, Loss: 0.2522, Train 0.4876, Val 0.4062\n",
      "Fold: 4, Epoch: 975, Loss: 0.3659, Train 0.4356, Val 0.3750\n",
      "Fold: 4, Epoch: 976, Loss: 0.3301, Train 0.3960, Val 0.2969\n",
      "Fold: 4, Epoch: 977, Loss: 0.3140, Train 0.4035, Val 0.2656\n",
      "Fold: 4, Epoch: 978, Loss: 0.3024, Train 0.4604, Val 0.3750\n",
      "Fold: 4, Epoch: 979, Loss: 0.2844, Train 0.4926, Val 0.3906\n",
      "Fold: 4, Epoch: 980, Loss: 0.3634, Train 0.5173, Val 0.4062\n",
      "Fold: 4, Epoch: 981, Loss: 0.2982, Train 0.5099, Val 0.4062\n",
      "Fold: 4, Epoch: 982, Loss: 0.3970, Train 0.4975, Val 0.3906\n",
      "Fold: 4, Epoch: 983, Loss: 0.2809, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 984, Loss: 0.2798, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 985, Loss: 0.3608, Train 0.5000, Val 0.3906\n",
      "Fold: 4, Epoch: 986, Loss: 0.2930, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 987, Loss: 0.2904, Train 0.5124, Val 0.4062\n",
      "Fold: 4, Epoch: 988, Loss: 0.3229, Train 0.5248, Val 0.4062\n",
      "Fold: 4, Epoch: 989, Loss: 0.2484, Train 0.5248, Val 0.4219\n",
      "Fold: 4, Epoch: 990, Loss: 0.3428, Train 0.5198, Val 0.4219\n",
      "Fold: 4, Epoch: 991, Loss: 0.2771, Train 0.5074, Val 0.3906\n",
      "Fold: 4, Epoch: 992, Loss: 0.2389, Train 0.4950, Val 0.4062\n",
      "Fold: 4, Epoch: 993, Loss: 0.4338, Train 0.4579, Val 0.3750\n",
      "Fold: 4, Epoch: 994, Loss: 0.2462, Train 0.3837, Val 0.2500\n",
      "Fold: 4, Epoch: 995, Loss: 0.3358, Train 0.2153, Val 0.1250\n",
      "Fold: 4, Epoch: 996, Loss: 0.2759, Train 0.1683, Val 0.1250\n",
      "Fold: 4, Epoch: 997, Loss: 0.2998, Train 0.1683, Val 0.1250\n",
      "Fold: 4, Epoch: 998, Loss: 0.2857, Train 0.1708, Val 0.1406\n",
      "Fold: 4, Epoch: 999, Loss: 0.2827, Train 0.1955, Val 0.1406\n",
      "Fold: 5, Epoch: 001, Loss: 2.6318, Train 0.3144, Val 0.2656\n",
      "Fold: 5, Epoch: 002, Loss: 5.4608, Train 0.1337, Val 0.1094\n",
      "Fold: 5, Epoch: 003, Loss: 3.3197, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 004, Loss: 2.9164, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 005, Loss: 3.7918, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 006, Loss: 3.1001, Train 0.1089, Val 0.1719\n",
      "Fold: 5, Epoch: 007, Loss: 2.4963, Train 0.3144, Val 0.2656\n",
      "Fold: 5, Epoch: 008, Loss: 2.7575, Train 0.3144, Val 0.2656\n",
      "Fold: 5, Epoch: 009, Loss: 2.9815, Train 0.3144, Val 0.2656\n",
      "Fold: 5, Epoch: 010, Loss: 2.5297, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 011, Loss: 2.3259, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 012, Loss: 2.3408, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 013, Loss: 2.5021, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 014, Loss: 2.5161, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 015, Loss: 2.2968, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 016, Loss: 2.1752, Train 0.3144, Val 0.2656\n",
      "Fold: 5, Epoch: 017, Loss: 2.3291, Train 0.3144, Val 0.2656\n",
      "Fold: 5, Epoch: 018, Loss: 2.4578, Train 0.3144, Val 0.2656\n",
      "Fold: 5, Epoch: 019, Loss: 2.5711, Train 0.3144, Val 0.2656\n",
      "Fold: 5, Epoch: 020, Loss: 2.2673, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 021, Loss: 2.1503, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 022, Loss: 2.2636, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 023, Loss: 2.3391, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 024, Loss: 2.3789, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 025, Loss: 2.1973, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 026, Loss: 2.1572, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 027, Loss: 2.1695, Train 0.3094, Val 0.2656\n",
      "Fold: 5, Epoch: 028, Loss: 2.2490, Train 0.3119, Val 0.2656\n",
      "Fold: 5, Epoch: 029, Loss: 2.2210, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 030, Loss: 2.1450, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 031, Loss: 2.1368, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 032, Loss: 2.1534, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 033, Loss: 2.1862, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 034, Loss: 2.1714, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 035, Loss: 2.1464, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 036, Loss: 2.1654, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 037, Loss: 2.1537, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 038, Loss: 2.1500, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 039, Loss: 2.1518, Train 0.3762, Val 0.4062\n",
      "Fold: 5, Epoch: 040, Loss: 2.2179, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 041, Loss: 2.2135, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 042, Loss: 2.1526, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 043, Loss: 2.1710, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 044, Loss: 2.1775, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 045, Loss: 2.1908, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 046, Loss: 2.1347, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 047, Loss: 2.1585, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 048, Loss: 2.1494, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 049, Loss: 2.1791, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 050, Loss: 2.1631, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 051, Loss: 2.1470, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 052, Loss: 2.1383, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 053, Loss: 2.1695, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 054, Loss: 2.1656, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 055, Loss: 2.1460, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 056, Loss: 2.1267, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 057, Loss: 2.1492, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 058, Loss: 2.1751, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 059, Loss: 2.1951, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 060, Loss: 2.1252, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 061, Loss: 2.1378, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 062, Loss: 2.1312, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 063, Loss: 2.2055, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 064, Loss: 2.1587, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 065, Loss: 2.1270, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 066, Loss: 2.1466, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 067, Loss: 2.1530, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 068, Loss: 2.2037, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 069, Loss: 2.1273, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 070, Loss: 2.1449, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 071, Loss: 2.1267, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 072, Loss: 2.1247, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 073, Loss: 2.1469, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 074, Loss: 2.1378, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 075, Loss: 2.1295, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 076, Loss: 2.1374, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 077, Loss: 2.1283, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 078, Loss: 2.1597, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 079, Loss: 2.1317, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 080, Loss: 2.1469, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 081, Loss: 2.1513, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 082, Loss: 2.1424, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 083, Loss: 2.1392, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 084, Loss: 2.1235, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 085, Loss: 2.1375, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 086, Loss: 2.1751, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 087, Loss: 2.1238, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 088, Loss: 2.1490, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 089, Loss: 2.1505, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 090, Loss: 2.1269, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 091, Loss: 2.1303, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 092, Loss: 2.1493, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 093, Loss: 2.1171, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 094, Loss: 2.1266, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 095, Loss: 2.1325, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 096, Loss: 2.1378, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 097, Loss: 2.1374, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 098, Loss: 2.1592, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 099, Loss: 2.1174, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 100, Loss: 2.1493, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 101, Loss: 2.1423, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 102, Loss: 2.1723, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 103, Loss: 2.1346, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 104, Loss: 2.1454, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 105, Loss: 2.1320, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 106, Loss: 2.1293, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 107, Loss: 2.1486, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 108, Loss: 2.1240, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 109, Loss: 2.1154, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 110, Loss: 2.1326, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 111, Loss: 2.1249, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 112, Loss: 2.1363, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 113, Loss: 2.1147, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 114, Loss: 2.1259, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 115, Loss: 2.1163, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 116, Loss: 2.1076, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 117, Loss: 2.1289, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 118, Loss: 2.1220, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 119, Loss: 2.1286, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 120, Loss: 2.1260, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 121, Loss: 2.1032, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 122, Loss: 2.1148, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 123, Loss: 2.1276, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 124, Loss: 2.1214, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 125, Loss: 2.1164, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 126, Loss: 2.1167, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 127, Loss: 2.1144, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 128, Loss: 2.1424, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 129, Loss: 2.1236, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 130, Loss: 2.1407, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 131, Loss: 2.1005, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 132, Loss: 2.1050, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 133, Loss: 2.1089, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 134, Loss: 2.1204, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 135, Loss: 2.1277, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 136, Loss: 2.1121, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 137, Loss: 2.1139, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 138, Loss: 2.1335, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 139, Loss: 2.1008, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 140, Loss: 2.1213, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 141, Loss: 2.1077, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 142, Loss: 2.1068, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 143, Loss: 2.1019, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 144, Loss: 2.1104, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 145, Loss: 2.1097, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 146, Loss: 2.1203, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 147, Loss: 2.0968, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 148, Loss: 2.1269, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 149, Loss: 2.1266, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 150, Loss: 2.1385, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 151, Loss: 2.1103, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 152, Loss: 2.1875, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 153, Loss: 2.1298, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 154, Loss: 2.0890, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 155, Loss: 2.1239, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 156, Loss: 2.1418, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 157, Loss: 2.1438, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 158, Loss: 2.1184, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 159, Loss: 2.1476, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 160, Loss: 2.1292, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 161, Loss: 2.1219, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 162, Loss: 2.1002, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 163, Loss: 2.1256, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 164, Loss: 2.1014, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 165, Loss: 2.1078, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 166, Loss: 2.1057, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 167, Loss: 2.0954, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 168, Loss: 2.1025, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 169, Loss: 2.1082, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 170, Loss: 2.0983, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 171, Loss: 2.1092, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 172, Loss: 2.1027, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 173, Loss: 2.1188, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 174, Loss: 2.1067, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 175, Loss: 2.1072, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 176, Loss: 2.1024, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 177, Loss: 2.0984, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 178, Loss: 2.0951, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 179, Loss: 2.1023, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 180, Loss: 2.1184, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 181, Loss: 2.0928, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 182, Loss: 2.0938, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 183, Loss: 2.0992, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 184, Loss: 2.1385, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 185, Loss: 2.1040, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 186, Loss: 2.1048, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 187, Loss: 2.1162, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 188, Loss: 2.1257, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 189, Loss: 2.0915, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 190, Loss: 2.1498, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 191, Loss: 2.0987, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 192, Loss: 2.0882, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 193, Loss: 2.1236, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 194, Loss: 2.1292, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 195, Loss: 2.1204, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 196, Loss: 2.0997, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 197, Loss: 2.1109, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 198, Loss: 2.0858, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 199, Loss: 2.0980, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 200, Loss: 2.0969, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 201, Loss: 2.0931, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 202, Loss: 2.1004, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 203, Loss: 2.1182, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 204, Loss: 2.0999, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 205, Loss: 2.1125, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 206, Loss: 2.0766, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 207, Loss: 2.1086, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 208, Loss: 2.0969, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 209, Loss: 2.1076, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 210, Loss: 2.0772, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 211, Loss: 2.1069, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 212, Loss: 2.1581, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 213, Loss: 2.0731, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 214, Loss: 2.1602, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 215, Loss: 2.1152, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 216, Loss: 2.0705, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 217, Loss: 2.0830, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 218, Loss: 2.1336, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 219, Loss: 2.0909, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 220, Loss: 2.1010, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 221, Loss: 2.1152, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 222, Loss: 2.0919, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 223, Loss: 2.0763, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 224, Loss: 2.0909, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 225, Loss: 2.1109, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 226, Loss: 2.0881, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 227, Loss: 2.0397, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 228, Loss: 2.0691, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 229, Loss: 2.0916, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 230, Loss: 2.0352, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 231, Loss: 2.0451, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 232, Loss: 2.0550, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 233, Loss: 2.0623, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 234, Loss: 2.0525, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 235, Loss: 2.0198, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 236, Loss: 2.0577, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 237, Loss: 2.0308, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 238, Loss: 2.0151, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 239, Loss: 2.0281, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 240, Loss: 2.0328, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 241, Loss: 2.0135, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 242, Loss: 2.0377, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 243, Loss: 2.0460, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 244, Loss: 2.0096, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 245, Loss: 2.0514, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 246, Loss: 1.9762, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 247, Loss: 1.9892, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 248, Loss: 1.9956, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 249, Loss: 1.9819, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 250, Loss: 1.9377, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 251, Loss: 1.9190, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 252, Loss: 1.9194, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 253, Loss: 1.9396, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 254, Loss: 1.8907, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 255, Loss: 1.8915, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 256, Loss: 1.9426, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 257, Loss: 1.8824, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 258, Loss: 1.8446, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 259, Loss: 1.8180, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 260, Loss: 1.8662, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 261, Loss: 1.8107, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 262, Loss: 1.7582, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 263, Loss: 1.7285, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 264, Loss: 1.6661, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 265, Loss: 1.6356, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 266, Loss: 1.6488, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 267, Loss: 1.6939, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 268, Loss: 1.5905, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 269, Loss: 1.5249, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 270, Loss: 1.6061, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 271, Loss: 1.5027, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 272, Loss: 1.5284, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 273, Loss: 1.3402, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 274, Loss: 1.5492, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 275, Loss: 1.5303, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 276, Loss: 1.5445, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 277, Loss: 1.4104, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 278, Loss: 1.4918, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 279, Loss: 1.4273, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 280, Loss: 1.4187, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 281, Loss: 1.3625, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 282, Loss: 1.4270, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 283, Loss: 1.4230, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 284, Loss: 1.3779, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 285, Loss: 1.3382, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 286, Loss: 1.3253, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 287, Loss: 1.4523, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 288, Loss: 1.3545, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 289, Loss: 1.3514, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 290, Loss: 1.3631, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 291, Loss: 1.2812, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 292, Loss: 1.3228, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 293, Loss: 1.3449, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 294, Loss: 1.4055, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 295, Loss: 1.2899, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 296, Loss: 1.3960, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 297, Loss: 1.4548, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 298, Loss: 1.2579, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 299, Loss: 1.3927, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 300, Loss: 1.3351, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 301, Loss: 1.3161, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 302, Loss: 1.3560, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 303, Loss: 1.4285, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 304, Loss: 1.3129, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 305, Loss: 1.3207, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 306, Loss: 1.3133, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 307, Loss: 1.3595, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 308, Loss: 1.2398, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 309, Loss: 1.2574, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 310, Loss: 1.2968, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 311, Loss: 1.4603, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 312, Loss: 1.2792, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 313, Loss: 1.3213, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 314, Loss: 1.3193, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 315, Loss: 1.2626, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 316, Loss: 1.3511, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 317, Loss: 1.3304, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 318, Loss: 1.2561, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 319, Loss: 1.2765, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 320, Loss: 1.3133, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 321, Loss: 1.3391, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 322, Loss: 1.3310, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 323, Loss: 1.2925, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 324, Loss: 1.2241, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 325, Loss: 1.2950, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 326, Loss: 1.3130, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 327, Loss: 1.2814, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 328, Loss: 1.2658, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 329, Loss: 1.3199, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 330, Loss: 1.2960, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 331, Loss: 1.3084, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 332, Loss: 1.3005, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 333, Loss: 1.2910, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 334, Loss: 1.3332, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 335, Loss: 1.2843, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 336, Loss: 1.3715, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 337, Loss: 1.3092, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 338, Loss: 1.2669, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 339, Loss: 1.2885, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 340, Loss: 1.2902, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 341, Loss: 1.2522, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 342, Loss: 1.3034, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 343, Loss: 1.2690, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 344, Loss: 1.1906, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 345, Loss: 1.2013, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 346, Loss: 1.2946, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 347, Loss: 1.2188, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 348, Loss: 1.2055, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 349, Loss: 1.1816, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 350, Loss: 1.2465, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 351, Loss: 1.2571, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 352, Loss: 1.1736, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 353, Loss: 1.2212, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 354, Loss: 1.3244, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 355, Loss: 1.1395, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 356, Loss: 1.1888, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 357, Loss: 1.1748, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 358, Loss: 1.1873, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 359, Loss: 1.2257, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 360, Loss: 1.2016, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 361, Loss: 1.2576, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 362, Loss: 1.2085, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 363, Loss: 1.1885, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 364, Loss: 1.2457, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 365, Loss: 1.1445, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 366, Loss: 1.2719, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 367, Loss: 1.3238, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 368, Loss: 1.0849, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 369, Loss: 1.1607, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 370, Loss: 1.2296, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 371, Loss: 1.3981, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 372, Loss: 1.2530, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 373, Loss: 1.1713, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 374, Loss: 1.5772, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 375, Loss: 1.2530, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 376, Loss: 1.1798, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 377, Loss: 1.2900, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 378, Loss: 1.2487, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 379, Loss: 1.2280, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 380, Loss: 1.1066, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 381, Loss: 1.1887, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 382, Loss: 1.3142, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 383, Loss: 1.1562, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 384, Loss: 1.1815, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 385, Loss: 1.1462, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 386, Loss: 1.1750, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 387, Loss: 1.2775, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 388, Loss: 1.1033, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 389, Loss: 1.2328, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 390, Loss: 1.1899, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 391, Loss: 1.1767, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 392, Loss: 1.0789, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 393, Loss: 1.0864, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 394, Loss: 1.1338, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 395, Loss: 1.3048, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 396, Loss: 1.1392, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 397, Loss: 1.0221, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 398, Loss: 1.1944, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 399, Loss: 1.1796, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 400, Loss: 1.0257, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 401, Loss: 1.0570, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 402, Loss: 1.0887, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 403, Loss: 1.1383, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 404, Loss: 1.0390, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 405, Loss: 0.9967, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 406, Loss: 1.0590, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 407, Loss: 1.1057, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 408, Loss: 0.9355, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 409, Loss: 1.0003, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 410, Loss: 0.9526, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 411, Loss: 1.0403, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 412, Loss: 1.0273, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 413, Loss: 1.0671, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 414, Loss: 1.0009, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 415, Loss: 1.0882, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 416, Loss: 0.9577, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 417, Loss: 1.0378, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 418, Loss: 0.8989, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 419, Loss: 0.9502, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 420, Loss: 1.0308, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 421, Loss: 0.8998, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 422, Loss: 0.9276, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 423, Loss: 0.8810, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 424, Loss: 0.9272, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 425, Loss: 0.9614, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 426, Loss: 0.8399, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 427, Loss: 1.0201, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 428, Loss: 0.8566, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 429, Loss: 0.9246, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 430, Loss: 0.8430, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 431, Loss: 0.9972, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 432, Loss: 0.9563, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 433, Loss: 0.9115, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 434, Loss: 0.8979, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 435, Loss: 0.9001, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 436, Loss: 0.8500, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 437, Loss: 0.9318, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 438, Loss: 1.0037, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 439, Loss: 0.9016, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 440, Loss: 0.8403, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 441, Loss: 0.9823, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 442, Loss: 0.8824, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 443, Loss: 0.8898, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 444, Loss: 0.8781, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 445, Loss: 1.0778, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 446, Loss: 0.8047, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 447, Loss: 0.9080, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 448, Loss: 0.9409, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 449, Loss: 0.8490, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 450, Loss: 0.8635, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 451, Loss: 0.9051, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 452, Loss: 0.9715, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 453, Loss: 0.8892, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 454, Loss: 0.8172, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 455, Loss: 0.8825, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 456, Loss: 0.8826, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 457, Loss: 0.8240, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 458, Loss: 0.8230, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 459, Loss: 0.8064, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 460, Loss: 0.8142, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 461, Loss: 0.7568, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 462, Loss: 0.7759, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 463, Loss: 0.8533, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 464, Loss: 0.7551, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 465, Loss: 0.7710, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 466, Loss: 0.8433, Train 0.3837, Val 0.4062\n",
      "Fold: 5, Epoch: 467, Loss: 0.8741, Train 0.3837, Val 0.4062\n",
      "Fold: 5, Epoch: 468, Loss: 0.7333, Train 0.3837, Val 0.4062\n",
      "Fold: 5, Epoch: 469, Loss: 0.7939, Train 0.3886, Val 0.4062\n",
      "Fold: 5, Epoch: 470, Loss: 0.8099, Train 0.3911, Val 0.4062\n",
      "Fold: 5, Epoch: 471, Loss: 0.9436, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 472, Loss: 0.8227, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 473, Loss: 0.7552, Train 0.3787, Val 0.4062\n",
      "Fold: 5, Epoch: 474, Loss: 0.9477, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 475, Loss: 0.8625, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 476, Loss: 0.7605, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 477, Loss: 0.8409, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 478, Loss: 0.8309, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 479, Loss: 0.8038, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 480, Loss: 0.7649, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 481, Loss: 0.7921, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 482, Loss: 0.7006, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 483, Loss: 0.7623, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 484, Loss: 0.7592, Train 0.3837, Val 0.4062\n",
      "Fold: 5, Epoch: 485, Loss: 0.7252, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 486, Loss: 0.6679, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 487, Loss: 0.7101, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 488, Loss: 0.6964, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 489, Loss: 0.8103, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 490, Loss: 0.7119, Train 0.3812, Val 0.4062\n",
      "Fold: 5, Epoch: 491, Loss: 0.6253, Train 0.4183, Val 0.4219\n",
      "Fold: 5, Epoch: 492, Loss: 0.9005, Train 0.6460, Val 0.6406\n",
      "Fold: 5, Epoch: 493, Loss: 0.9991, Train 0.5322, Val 0.5469\n",
      "Fold: 5, Epoch: 494, Loss: 0.7429, Train 0.4183, Val 0.4844\n",
      "Fold: 5, Epoch: 495, Loss: 0.6184, Train 0.4431, Val 0.5312\n",
      "Fold: 5, Epoch: 496, Loss: 0.7328, Train 0.3515, Val 0.3594\n",
      "Fold: 5, Epoch: 497, Loss: 0.7793, Train 0.2624, Val 0.2188\n",
      "Fold: 5, Epoch: 498, Loss: 0.6595, Train 0.2723, Val 0.1875\n",
      "Fold: 5, Epoch: 499, Loss: 0.7587, Train 0.6040, Val 0.5000\n",
      "Fold: 5, Epoch: 500, Loss: 0.7684, Train 0.5842, Val 0.5312\n",
      "Fold: 5, Epoch: 501, Loss: 0.9762, Train 0.3837, Val 0.3906\n",
      "Fold: 5, Epoch: 502, Loss: 0.6544, Train 0.3292, Val 0.3125\n",
      "Fold: 5, Epoch: 503, Loss: 0.6472, Train 0.3936, Val 0.4062\n",
      "Fold: 5, Epoch: 504, Loss: 0.6488, Train 0.4728, Val 0.5781\n",
      "Fold: 5, Epoch: 505, Loss: 0.8846, Train 0.4777, Val 0.5781\n",
      "Fold: 5, Epoch: 506, Loss: 0.6277, Train 0.5000, Val 0.5312\n",
      "Fold: 5, Epoch: 507, Loss: 0.6178, Train 0.7450, Val 0.7344\n",
      "Fold: 5, Epoch: 508, Loss: 0.7400, Train 0.7723, Val 0.7969\n",
      "Fold: 5, Epoch: 509, Loss: 0.7438, Train 0.7351, Val 0.7344\n",
      "Fold: 5, Epoch: 510, Loss: 0.6719, Train 0.4802, Val 0.5625\n",
      "Fold: 5, Epoch: 511, Loss: 0.5502, Train 0.4703, Val 0.5469\n",
      "Fold: 5, Epoch: 512, Loss: 0.7090, Train 0.4505, Val 0.5156\n",
      "Fold: 5, Epoch: 513, Loss: 0.6489, Train 0.4134, Val 0.4844\n",
      "Fold: 5, Epoch: 514, Loss: 0.5961, Train 0.3490, Val 0.3438\n",
      "Fold: 5, Epoch: 515, Loss: 0.5686, Train 0.3589, Val 0.3281\n",
      "Fold: 5, Epoch: 516, Loss: 0.6387, Train 0.4505, Val 0.4844\n",
      "Fold: 5, Epoch: 517, Loss: 0.5807, Train 0.4629, Val 0.5156\n",
      "Fold: 5, Epoch: 518, Loss: 0.6060, Train 0.4208, Val 0.4531\n",
      "Fold: 5, Epoch: 519, Loss: 0.6042, Train 0.3960, Val 0.4062\n",
      "Fold: 5, Epoch: 520, Loss: 0.5709, Train 0.4035, Val 0.4219\n",
      "Fold: 5, Epoch: 521, Loss: 0.7084, Train 0.4530, Val 0.5625\n",
      "Fold: 5, Epoch: 522, Loss: 0.6360, Train 0.4876, Val 0.5781\n",
      "Fold: 5, Epoch: 523, Loss: 0.6330, Train 0.4975, Val 0.5781\n",
      "Fold: 5, Epoch: 524, Loss: 0.5471, Train 0.4975, Val 0.5781\n",
      "Fold: 5, Epoch: 525, Loss: 0.5264, Train 0.4950, Val 0.5781\n",
      "Fold: 5, Epoch: 526, Loss: 0.4523, Train 0.4975, Val 0.5781\n",
      "Fold: 5, Epoch: 527, Loss: 0.5901, Train 0.4975, Val 0.5781\n",
      "Fold: 5, Epoch: 528, Loss: 0.5289, Train 0.4975, Val 0.5781\n",
      "Fold: 5, Epoch: 529, Loss: 0.5579, Train 0.4926, Val 0.5781\n",
      "Fold: 5, Epoch: 530, Loss: 0.5751, Train 0.4926, Val 0.5781\n",
      "Fold: 5, Epoch: 531, Loss: 0.5468, Train 0.4926, Val 0.5781\n",
      "Fold: 5, Epoch: 532, Loss: 0.4626, Train 0.4827, Val 0.5781\n",
      "Fold: 5, Epoch: 533, Loss: 0.5811, Train 0.4703, Val 0.5781\n",
      "Fold: 5, Epoch: 534, Loss: 0.5313, Train 0.4134, Val 0.4531\n",
      "Fold: 5, Epoch: 535, Loss: 0.5854, Train 0.2946, Val 0.2812\n",
      "Fold: 5, Epoch: 536, Loss: 0.6055, Train 0.3391, Val 0.3281\n",
      "Fold: 5, Epoch: 537, Loss: 0.4820, Train 0.3812, Val 0.3750\n",
      "Fold: 5, Epoch: 538, Loss: 0.5167, Train 0.4084, Val 0.4219\n",
      "Fold: 5, Epoch: 539, Loss: 0.5272, Train 0.3465, Val 0.3438\n",
      "Fold: 5, Epoch: 540, Loss: 0.4369, Train 0.2772, Val 0.2812\n",
      "Fold: 5, Epoch: 541, Loss: 0.5536, Train 0.1955, Val 0.2031\n",
      "Fold: 5, Epoch: 542, Loss: 0.5522, Train 0.1460, Val 0.2031\n",
      "Fold: 5, Epoch: 543, Loss: 0.5656, Train 0.1361, Val 0.2031\n",
      "Fold: 5, Epoch: 544, Loss: 0.6059, Train 0.1584, Val 0.2031\n",
      "Fold: 5, Epoch: 545, Loss: 0.5901, Train 0.1559, Val 0.2031\n",
      "Fold: 5, Epoch: 546, Loss: 0.6215, Train 0.1807, Val 0.2031\n",
      "Fold: 5, Epoch: 547, Loss: 0.6407, Train 0.2203, Val 0.2344\n",
      "Fold: 5, Epoch: 548, Loss: 0.4486, Train 0.4678, Val 0.5781\n",
      "Fold: 5, Epoch: 549, Loss: 0.6487, Train 0.4901, Val 0.5781\n",
      "Fold: 5, Epoch: 550, Loss: 0.6017, Train 0.5941, Val 0.6406\n",
      "Fold: 5, Epoch: 551, Loss: 0.5760, Train 0.7525, Val 0.7812\n",
      "Fold: 5, Epoch: 552, Loss: 0.5611, Train 0.7871, Val 0.8281\n",
      "Fold: 5, Epoch: 553, Loss: 0.5262, Train 0.7871, Val 0.8281\n",
      "Fold: 5, Epoch: 554, Loss: 0.7195, Train 0.6386, Val 0.7188\n",
      "Fold: 5, Epoch: 555, Loss: 0.4142, Train 0.4827, Val 0.5781\n",
      "Fold: 5, Epoch: 556, Loss: 0.5612, Train 0.4455, Val 0.5312\n",
      "Fold: 5, Epoch: 557, Loss: 0.5140, Train 0.3713, Val 0.3906\n",
      "Fold: 5, Epoch: 558, Loss: 0.5279, Train 0.3045, Val 0.2969\n",
      "Fold: 5, Epoch: 559, Loss: 0.5250, Train 0.3292, Val 0.3438\n",
      "Fold: 5, Epoch: 560, Loss: 0.5448, Train 0.4579, Val 0.5469\n",
      "Fold: 5, Epoch: 561, Loss: 0.4796, Train 0.4827, Val 0.5781\n",
      "Fold: 5, Epoch: 562, Loss: 0.4198, Train 0.4851, Val 0.5781\n",
      "Fold: 5, Epoch: 563, Loss: 0.4834, Train 0.5000, Val 0.5781\n",
      "Fold: 5, Epoch: 564, Loss: 0.5019, Train 0.5446, Val 0.6094\n",
      "Fold: 5, Epoch: 565, Loss: 0.4474, Train 0.5025, Val 0.5781\n",
      "Fold: 5, Epoch: 566, Loss: 0.4776, Train 0.4851, Val 0.5781\n",
      "Fold: 5, Epoch: 567, Loss: 0.5040, Train 0.4851, Val 0.5781\n",
      "Fold: 5, Epoch: 568, Loss: 0.5521, Train 0.4851, Val 0.5781\n",
      "Fold: 5, Epoch: 569, Loss: 0.4828, Train 0.4777, Val 0.5781\n",
      "Fold: 5, Epoch: 570, Loss: 0.4171, Train 0.4653, Val 0.5781\n",
      "Fold: 5, Epoch: 571, Loss: 0.5820, Train 0.4208, Val 0.4844\n",
      "Fold: 5, Epoch: 572, Loss: 0.3742, Train 0.3094, Val 0.3281\n",
      "Fold: 5, Epoch: 573, Loss: 0.5287, Train 0.3540, Val 0.3750\n",
      "Fold: 5, Epoch: 574, Loss: 0.5824, Train 0.4926, Val 0.5938\n",
      "Fold: 5, Epoch: 575, Loss: 0.5624, Train 0.4950, Val 0.5781\n",
      "Fold: 5, Epoch: 576, Loss: 0.5333, Train 0.4975, Val 0.5781\n",
      "Fold: 5, Epoch: 577, Loss: 0.5393, Train 0.4802, Val 0.5781\n",
      "Fold: 5, Epoch: 578, Loss: 0.5836, Train 0.4802, Val 0.5781\n",
      "Fold: 5, Epoch: 579, Loss: 0.4386, Train 0.4678, Val 0.5781\n",
      "Fold: 5, Epoch: 580, Loss: 0.3911, Train 0.3738, Val 0.4062\n",
      "Fold: 5, Epoch: 581, Loss: 0.5087, Train 0.2153, Val 0.2500\n",
      "Fold: 5, Epoch: 582, Loss: 0.4557, Train 0.2030, Val 0.2188\n",
      "Fold: 5, Epoch: 583, Loss: 0.5199, Train 0.4678, Val 0.4531\n",
      "Fold: 5, Epoch: 584, Loss: 0.4323, Train 0.4851, Val 0.4688\n",
      "Fold: 5, Epoch: 585, Loss: 0.6459, Train 0.4208, Val 0.4375\n",
      "Fold: 5, Epoch: 586, Loss: 0.4795, Train 0.3911, Val 0.4219\n",
      "Fold: 5, Epoch: 587, Loss: 0.4186, Train 0.4381, Val 0.5312\n",
      "Fold: 5, Epoch: 588, Loss: 0.6628, Train 0.4703, Val 0.6094\n",
      "Fold: 5, Epoch: 589, Loss: 0.4769, Train 0.4802, Val 0.6094\n",
      "Fold: 5, Epoch: 590, Loss: 0.4759, Train 0.4802, Val 0.6094\n",
      "Fold: 5, Epoch: 591, Loss: 0.5045, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 592, Loss: 0.5020, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 593, Loss: 0.4507, Train 0.7178, Val 0.7812\n",
      "Fold: 5, Epoch: 594, Loss: 0.5853, Train 0.7277, Val 0.7812\n",
      "Fold: 5, Epoch: 595, Loss: 0.4882, Train 0.7351, Val 0.7812\n",
      "Fold: 5, Epoch: 596, Loss: 0.5392, Train 0.7203, Val 0.7812\n",
      "Fold: 5, Epoch: 597, Loss: 0.5466, Train 0.6262, Val 0.6719\n",
      "Fold: 5, Epoch: 598, Loss: 0.5610, Train 0.5817, Val 0.6406\n",
      "Fold: 5, Epoch: 599, Loss: 0.4487, Train 0.5000, Val 0.5781\n",
      "Fold: 5, Epoch: 600, Loss: 0.4432, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 601, Loss: 0.4826, Train 0.3663, Val 0.3906\n",
      "Fold: 5, Epoch: 602, Loss: 0.5711, Train 0.2129, Val 0.2344\n",
      "Fold: 5, Epoch: 603, Loss: 0.7125, Train 0.2277, Val 0.2344\n",
      "Fold: 5, Epoch: 604, Loss: 0.4792, Train 0.4332, Val 0.5000\n",
      "Fold: 5, Epoch: 605, Loss: 0.4077, Train 0.4950, Val 0.5781\n",
      "Fold: 5, Epoch: 606, Loss: 0.4854, Train 0.5421, Val 0.5938\n",
      "Fold: 5, Epoch: 607, Loss: 0.5206, Train 0.5743, Val 0.6406\n",
      "Fold: 5, Epoch: 608, Loss: 0.4392, Train 0.5743, Val 0.6406\n",
      "Fold: 5, Epoch: 609, Loss: 0.4046, Train 0.5767, Val 0.6406\n",
      "Fold: 5, Epoch: 610, Loss: 0.4000, Train 0.4728, Val 0.5156\n",
      "Fold: 5, Epoch: 611, Loss: 0.4947, Train 0.2500, Val 0.2656\n",
      "Fold: 5, Epoch: 612, Loss: 0.4605, Train 0.2475, Val 0.2500\n",
      "Fold: 5, Epoch: 613, Loss: 0.3932, Train 0.2302, Val 0.2500\n",
      "Fold: 5, Epoch: 614, Loss: 0.5136, Train 0.2178, Val 0.2344\n",
      "Fold: 5, Epoch: 615, Loss: 0.3739, Train 0.1931, Val 0.2031\n",
      "Fold: 5, Epoch: 616, Loss: 0.4795, Train 0.1832, Val 0.2188\n",
      "Fold: 5, Epoch: 617, Loss: 0.4311, Train 0.1832, Val 0.2188\n",
      "Fold: 5, Epoch: 618, Loss: 0.4246, Train 0.1708, Val 0.2188\n",
      "Fold: 5, Epoch: 619, Loss: 0.4750, Train 0.1708, Val 0.2031\n",
      "Fold: 5, Epoch: 620, Loss: 0.6266, Train 0.1832, Val 0.2031\n",
      "Fold: 5, Epoch: 621, Loss: 0.4952, Train 0.2079, Val 0.2031\n",
      "Fold: 5, Epoch: 622, Loss: 0.4219, Train 0.3837, Val 0.3594\n",
      "Fold: 5, Epoch: 623, Loss: 0.4634, Train 0.5792, Val 0.5469\n",
      "Fold: 5, Epoch: 624, Loss: 0.4679, Train 0.7970, Val 0.8750\n",
      "Fold: 5, Epoch: 625, Loss: 0.4344, Train 0.7970, Val 0.8438\n",
      "Fold: 5, Epoch: 626, Loss: 0.4926, Train 0.7847, Val 0.8125\n",
      "Fold: 5, Epoch: 627, Loss: 0.5322, Train 0.7426, Val 0.7812\n",
      "Fold: 5, Epoch: 628, Loss: 0.5407, Train 0.5866, Val 0.6719\n",
      "Fold: 5, Epoch: 629, Loss: 0.3540, Train 0.4975, Val 0.5781\n",
      "Fold: 5, Epoch: 630, Loss: 0.4520, Train 0.4728, Val 0.5938\n",
      "Fold: 5, Epoch: 631, Loss: 0.4143, Train 0.3144, Val 0.3594\n",
      "Fold: 5, Epoch: 632, Loss: 0.5116, Train 0.2550, Val 0.2969\n",
      "Fold: 5, Epoch: 633, Loss: 0.4794, Train 0.4109, Val 0.4688\n",
      "Fold: 5, Epoch: 634, Loss: 0.4807, Train 0.4752, Val 0.5938\n",
      "Fold: 5, Epoch: 635, Loss: 0.4962, Train 0.4876, Val 0.5938\n",
      "Fold: 5, Epoch: 636, Loss: 0.4411, Train 0.4802, Val 0.5625\n",
      "Fold: 5, Epoch: 637, Loss: 0.4764, Train 0.4827, Val 0.5781\n",
      "Fold: 5, Epoch: 638, Loss: 0.4394, Train 0.4901, Val 0.5781\n",
      "Fold: 5, Epoch: 639, Loss: 0.4805, Train 0.5149, Val 0.5781\n",
      "Fold: 5, Epoch: 640, Loss: 0.4270, Train 0.6634, Val 0.7500\n",
      "Fold: 5, Epoch: 641, Loss: 0.4126, Train 0.6807, Val 0.7500\n",
      "Fold: 5, Epoch: 642, Loss: 0.4025, Train 0.6485, Val 0.7344\n",
      "Fold: 5, Epoch: 643, Loss: 0.4885, Train 0.4851, Val 0.5781\n",
      "Fold: 5, Epoch: 644, Loss: 0.4818, Train 0.4604, Val 0.5469\n",
      "Fold: 5, Epoch: 645, Loss: 0.4410, Train 0.4455, Val 0.5469\n",
      "Fold: 5, Epoch: 646, Loss: 0.3763, Train 0.4505, Val 0.5469\n",
      "Fold: 5, Epoch: 647, Loss: 0.3931, Train 0.4431, Val 0.5469\n",
      "Fold: 5, Epoch: 648, Loss: 0.4768, Train 0.4703, Val 0.5781\n",
      "Fold: 5, Epoch: 649, Loss: 0.4066, Train 0.4975, Val 0.5781\n",
      "Fold: 5, Epoch: 650, Loss: 0.5059, Train 0.5891, Val 0.6875\n",
      "Fold: 5, Epoch: 651, Loss: 0.4664, Train 0.6881, Val 0.6875\n",
      "Fold: 5, Epoch: 652, Loss: 0.4064, Train 0.6040, Val 0.5938\n",
      "Fold: 5, Epoch: 653, Loss: 0.3712, Train 0.5322, Val 0.4844\n",
      "Fold: 5, Epoch: 654, Loss: 0.4312, Train 0.5272, Val 0.4844\n",
      "Fold: 5, Epoch: 655, Loss: 0.4157, Train 0.4777, Val 0.4688\n",
      "Fold: 5, Epoch: 656, Loss: 0.3084, Train 0.2946, Val 0.3125\n",
      "Fold: 5, Epoch: 657, Loss: 0.4870, Train 0.2500, Val 0.2969\n",
      "Fold: 5, Epoch: 658, Loss: 0.3088, Train 0.2921, Val 0.3281\n",
      "Fold: 5, Epoch: 659, Loss: 0.4134, Train 0.4406, Val 0.5000\n",
      "Fold: 5, Epoch: 660, Loss: 0.4082, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 661, Loss: 0.4680, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 662, Loss: 0.4748, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 663, Loss: 0.4669, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 664, Loss: 0.3787, Train 0.4926, Val 0.5938\n",
      "Fold: 5, Epoch: 665, Loss: 0.4736, Train 0.4876, Val 0.5938\n",
      "Fold: 5, Epoch: 666, Loss: 0.3794, Train 0.4728, Val 0.5938\n",
      "Fold: 5, Epoch: 667, Loss: 0.4481, Train 0.4752, Val 0.5938\n",
      "Fold: 5, Epoch: 668, Loss: 0.5116, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 669, Loss: 0.4347, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 670, Loss: 0.3352, Train 0.4876, Val 0.5781\n",
      "Fold: 5, Epoch: 671, Loss: 0.5001, Train 0.4876, Val 0.5781\n",
      "Fold: 5, Epoch: 672, Loss: 0.3742, Train 0.4876, Val 0.5781\n",
      "Fold: 5, Epoch: 673, Loss: 0.3965, Train 0.5446, Val 0.6094\n",
      "Fold: 5, Epoch: 674, Loss: 0.4574, Train 0.7327, Val 0.7656\n",
      "Fold: 5, Epoch: 675, Loss: 0.4377, Train 0.7450, Val 0.7656\n",
      "Fold: 5, Epoch: 676, Loss: 0.3526, Train 0.6782, Val 0.7344\n",
      "Fold: 5, Epoch: 677, Loss: 0.4446, Train 0.5124, Val 0.5938\n",
      "Fold: 5, Epoch: 678, Loss: 0.3720, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 679, Loss: 0.3402, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 680, Loss: 0.3915, Train 0.4876, Val 0.6094\n",
      "Fold: 5, Epoch: 681, Loss: 0.3638, Train 0.4827, Val 0.6094\n",
      "Fold: 5, Epoch: 682, Loss: 0.3150, Train 0.4752, Val 0.5938\n",
      "Fold: 5, Epoch: 683, Loss: 0.4109, Train 0.4728, Val 0.5938\n",
      "Fold: 5, Epoch: 684, Loss: 0.3203, Train 0.4703, Val 0.5938\n",
      "Fold: 5, Epoch: 685, Loss: 0.3717, Train 0.4728, Val 0.5938\n",
      "Fold: 5, Epoch: 686, Loss: 0.3648, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 687, Loss: 0.3266, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 688, Loss: 0.3258, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 689, Loss: 0.3824, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 690, Loss: 0.5966, Train 0.4926, Val 0.5938\n",
      "Fold: 5, Epoch: 691, Loss: 0.4590, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 692, Loss: 0.3107, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 693, Loss: 0.3046, Train 0.5817, Val 0.6719\n",
      "Fold: 5, Epoch: 694, Loss: 0.4482, Train 0.5371, Val 0.6250\n",
      "Fold: 5, Epoch: 695, Loss: 0.3140, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 696, Loss: 0.4637, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 697, Loss: 0.3609, Train 0.4802, Val 0.6094\n",
      "Fold: 5, Epoch: 698, Loss: 0.3759, Train 0.4802, Val 0.6094\n",
      "Fold: 5, Epoch: 699, Loss: 0.4549, Train 0.4802, Val 0.6094\n",
      "Fold: 5, Epoch: 700, Loss: 0.3907, Train 0.4777, Val 0.6094\n",
      "Fold: 5, Epoch: 701, Loss: 0.4072, Train 0.4851, Val 0.6094\n",
      "Fold: 5, Epoch: 702, Loss: 0.3623, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 703, Loss: 0.4217, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 704, Loss: 0.3431, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 705, Loss: 0.4076, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 706, Loss: 0.3582, Train 0.4876, Val 0.6094\n",
      "Fold: 5, Epoch: 707, Loss: 0.3770, Train 0.4827, Val 0.5938\n",
      "Fold: 5, Epoch: 708, Loss: 0.3888, Train 0.4579, Val 0.5781\n",
      "Fold: 5, Epoch: 709, Loss: 0.3092, Train 0.4851, Val 0.6094\n",
      "Fold: 5, Epoch: 710, Loss: 0.3685, Train 0.4876, Val 0.6094\n",
      "Fold: 5, Epoch: 711, Loss: 0.2838, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 712, Loss: 0.2828, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 713, Loss: 0.3658, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 714, Loss: 0.4296, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 715, Loss: 0.3512, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 716, Loss: 0.3402, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 717, Loss: 0.3379, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 718, Loss: 0.5024, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 719, Loss: 0.4476, Train 0.5124, Val 0.6094\n",
      "Fold: 5, Epoch: 720, Loss: 0.4127, Train 0.5619, Val 0.6562\n",
      "Fold: 5, Epoch: 721, Loss: 0.5210, Train 0.5124, Val 0.6094\n",
      "Fold: 5, Epoch: 722, Loss: 0.3153, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 723, Loss: 0.3567, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 724, Loss: 0.3711, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 725, Loss: 0.3587, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 726, Loss: 0.3933, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 727, Loss: 0.4741, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 728, Loss: 0.3923, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 729, Loss: 0.3173, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 730, Loss: 0.3393, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 731, Loss: 0.4539, Train 0.4876, Val 0.6094\n",
      "Fold: 5, Epoch: 732, Loss: 0.4492, Train 0.4876, Val 0.6094\n",
      "Fold: 5, Epoch: 733, Loss: 0.3857, Train 0.4876, Val 0.6094\n",
      "Fold: 5, Epoch: 734, Loss: 0.4841, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 735, Loss: 0.4472, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 736, Loss: 0.3330, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 737, Loss: 0.3494, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 738, Loss: 0.3280, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 739, Loss: 0.3315, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 740, Loss: 0.3822, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 741, Loss: 0.2678, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 742, Loss: 0.3899, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 743, Loss: 0.3140, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 744, Loss: 0.3484, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 745, Loss: 0.3931, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 746, Loss: 0.4530, Train 0.4802, Val 0.6094\n",
      "Fold: 5, Epoch: 747, Loss: 0.4067, Train 0.4752, Val 0.6094\n",
      "Fold: 5, Epoch: 748, Loss: 0.4373, Train 0.4728, Val 0.5938\n",
      "Fold: 5, Epoch: 749, Loss: 0.3783, Train 0.4752, Val 0.5938\n",
      "Fold: 5, Epoch: 750, Loss: 0.3640, Train 0.4802, Val 0.6094\n",
      "Fold: 5, Epoch: 751, Loss: 0.3164, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 752, Loss: 0.3894, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 753, Loss: 0.4283, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 754, Loss: 0.3518, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 755, Loss: 0.4838, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 756, Loss: 0.5348, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 757, Loss: 0.3407, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 758, Loss: 0.3189, Train 0.5198, Val 0.6094\n",
      "Fold: 5, Epoch: 759, Loss: 0.3542, Train 0.5347, Val 0.6250\n",
      "Fold: 5, Epoch: 760, Loss: 0.3614, Train 0.5223, Val 0.6094\n",
      "Fold: 5, Epoch: 761, Loss: 0.4105, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 762, Loss: 0.3769, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 763, Loss: 0.3795, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 764, Loss: 0.4285, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 765, Loss: 0.3931, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 766, Loss: 0.3649, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 767, Loss: 0.2843, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 768, Loss: 0.3255, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 769, Loss: 0.3637, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 770, Loss: 0.3758, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 771, Loss: 0.3303, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 772, Loss: 0.3730, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 773, Loss: 0.4436, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 774, Loss: 0.4034, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 775, Loss: 0.3457, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 776, Loss: 0.3333, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 777, Loss: 0.3251, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 778, Loss: 0.4065, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 779, Loss: 0.3118, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 780, Loss: 0.3156, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 781, Loss: 0.3952, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 782, Loss: 0.3773, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 783, Loss: 0.3660, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 784, Loss: 0.3228, Train 0.5099, Val 0.6094\n",
      "Fold: 5, Epoch: 785, Loss: 0.2457, Train 0.5025, Val 0.6250\n",
      "Fold: 5, Epoch: 786, Loss: 0.3268, Train 0.3688, Val 0.5312\n",
      "Fold: 5, Epoch: 787, Loss: 0.3752, Train 0.2129, Val 0.2812\n",
      "Fold: 5, Epoch: 788, Loss: 0.2879, Train 0.2129, Val 0.2656\n",
      "Fold: 5, Epoch: 789, Loss: 0.3448, Train 0.2822, Val 0.4062\n",
      "Fold: 5, Epoch: 790, Loss: 0.3136, Train 0.3515, Val 0.5156\n",
      "Fold: 5, Epoch: 791, Loss: 0.3668, Train 0.3812, Val 0.5156\n",
      "Fold: 5, Epoch: 792, Loss: 0.5007, Train 0.4851, Val 0.5938\n",
      "Fold: 5, Epoch: 793, Loss: 0.2610, Train 0.5124, Val 0.6094\n",
      "Fold: 5, Epoch: 794, Loss: 0.3349, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 795, Loss: 0.2789, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 796, Loss: 0.3333, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 797, Loss: 0.3944, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 798, Loss: 0.3654, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 799, Loss: 0.3104, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 800, Loss: 0.2970, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 801, Loss: 0.4422, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 802, Loss: 0.3598, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 803, Loss: 0.3882, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 804, Loss: 0.4153, Train 0.5173, Val 0.6094\n",
      "Fold: 5, Epoch: 805, Loss: 0.4430, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 806, Loss: 0.3878, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 807, Loss: 0.4576, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 808, Loss: 0.3849, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 809, Loss: 0.4710, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 810, Loss: 0.3365, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 811, Loss: 0.2666, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 812, Loss: 0.3448, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 813, Loss: 0.3144, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 814, Loss: 0.4332, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 815, Loss: 0.2826, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 816, Loss: 0.3704, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 817, Loss: 0.3896, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 818, Loss: 0.3910, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 819, Loss: 0.3652, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 820, Loss: 0.3093, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 821, Loss: 0.4041, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 822, Loss: 0.3201, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 823, Loss: 0.2916, Train 0.4827, Val 0.6094\n",
      "Fold: 5, Epoch: 824, Loss: 0.3591, Train 0.4777, Val 0.5938\n",
      "Fold: 5, Epoch: 825, Loss: 0.3636, Train 0.4777, Val 0.5938\n",
      "Fold: 5, Epoch: 826, Loss: 0.3998, Train 0.4901, Val 0.6094\n",
      "Fold: 5, Epoch: 827, Loss: 0.2962, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 828, Loss: 0.4612, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 829, Loss: 0.2714, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 830, Loss: 0.4405, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 831, Loss: 0.2961, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 832, Loss: 0.3598, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 833, Loss: 0.2735, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 834, Loss: 0.3106, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 835, Loss: 0.3371, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 836, Loss: 0.2939, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 837, Loss: 0.3359, Train 0.5000, Val 0.5938\n",
      "Fold: 5, Epoch: 838, Loss: 0.3441, Train 0.5198, Val 0.6094\n",
      "Fold: 5, Epoch: 839, Loss: 0.2977, Train 0.5198, Val 0.6094\n",
      "Fold: 5, Epoch: 840, Loss: 0.3090, Train 0.5099, Val 0.6094\n",
      "Fold: 5, Epoch: 841, Loss: 0.4140, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 842, Loss: 0.4301, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 843, Loss: 0.3109, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 844, Loss: 0.3693, Train 0.5099, Val 0.6094\n",
      "Fold: 5, Epoch: 845, Loss: 0.3408, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 846, Loss: 0.3659, Train 0.5099, Val 0.6094\n",
      "Fold: 5, Epoch: 847, Loss: 0.4192, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 848, Loss: 0.3437, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 849, Loss: 0.3229, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 850, Loss: 0.4193, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 851, Loss: 0.2319, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 852, Loss: 0.2952, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 853, Loss: 0.2807, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 854, Loss: 0.3175, Train 0.4653, Val 0.5938\n",
      "Fold: 5, Epoch: 855, Loss: 0.3738, Train 0.4728, Val 0.5938\n",
      "Fold: 5, Epoch: 856, Loss: 0.3315, Train 0.4777, Val 0.5938\n",
      "Fold: 5, Epoch: 857, Loss: 0.3313, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 858, Loss: 0.2873, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 859, Loss: 0.3677, Train 0.5173, Val 0.6094\n",
      "Fold: 5, Epoch: 860, Loss: 0.2382, Train 0.5272, Val 0.6094\n",
      "Fold: 5, Epoch: 861, Loss: 0.3875, Train 0.5248, Val 0.6094\n",
      "Fold: 5, Epoch: 862, Loss: 0.2719, Train 0.5198, Val 0.6094\n",
      "Fold: 5, Epoch: 863, Loss: 0.3168, Train 0.5074, Val 0.5938\n",
      "Fold: 5, Epoch: 864, Loss: 0.3174, Train 0.4876, Val 0.5938\n",
      "Fold: 5, Epoch: 865, Loss: 0.3241, Train 0.4876, Val 0.5938\n",
      "Fold: 5, Epoch: 866, Loss: 0.3901, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 867, Loss: 0.2994, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 868, Loss: 0.3194, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 869, Loss: 0.2825, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 870, Loss: 0.3604, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 871, Loss: 0.3891, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 872, Loss: 0.2971, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 873, Loss: 0.3171, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 874, Loss: 0.3652, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 875, Loss: 0.3451, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 876, Loss: 0.3833, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 877, Loss: 0.2003, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 878, Loss: 0.4225, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 879, Loss: 0.2536, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 880, Loss: 0.3519, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 881, Loss: 0.2927, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 882, Loss: 0.3302, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 883, Loss: 0.3004, Train 0.4678, Val 0.5938\n",
      "Fold: 5, Epoch: 884, Loss: 0.2838, Train 0.4554, Val 0.5938\n",
      "Fold: 5, Epoch: 885, Loss: 0.3298, Train 0.4480, Val 0.5781\n",
      "Fold: 5, Epoch: 886, Loss: 0.3770, Train 0.4752, Val 0.5938\n",
      "Fold: 5, Epoch: 887, Loss: 0.2695, Train 0.4802, Val 0.5938\n",
      "Fold: 5, Epoch: 888, Loss: 0.3102, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 889, Loss: 0.2831, Train 0.5124, Val 0.6094\n",
      "Fold: 5, Epoch: 890, Loss: 0.2870, Train 0.4926, Val 0.5781\n",
      "Fold: 5, Epoch: 891, Loss: 0.3500, Train 0.4728, Val 0.5938\n",
      "Fold: 5, Epoch: 892, Loss: 0.3149, Train 0.4728, Val 0.5781\n",
      "Fold: 5, Epoch: 893, Loss: 0.2831, Train 0.4703, Val 0.5625\n",
      "Fold: 5, Epoch: 894, Loss: 0.2993, Train 0.4381, Val 0.5312\n",
      "Fold: 5, Epoch: 895, Loss: 0.2984, Train 0.4752, Val 0.5469\n",
      "Fold: 5, Epoch: 896, Loss: 0.3364, Train 0.4282, Val 0.5156\n",
      "Fold: 5, Epoch: 897, Loss: 0.2726, Train 0.3861, Val 0.4844\n",
      "Fold: 5, Epoch: 898, Loss: 0.2785, Train 0.3564, Val 0.4375\n",
      "Fold: 5, Epoch: 899, Loss: 0.2858, Train 0.3317, Val 0.4219\n",
      "Fold: 5, Epoch: 900, Loss: 0.3532, Train 0.3490, Val 0.4375\n",
      "Fold: 5, Epoch: 901, Loss: 0.3689, Train 0.3441, Val 0.4531\n",
      "Fold: 5, Epoch: 902, Loss: 0.2456, Train 0.3589, Val 0.4688\n",
      "Fold: 5, Epoch: 903, Loss: 0.3705, Train 0.2797, Val 0.4062\n",
      "Fold: 5, Epoch: 904, Loss: 0.2987, Train 0.2649, Val 0.3750\n",
      "Fold: 5, Epoch: 905, Loss: 0.3985, Train 0.2550, Val 0.3438\n",
      "Fold: 5, Epoch: 906, Loss: 0.2595, Train 0.2748, Val 0.3594\n",
      "Fold: 5, Epoch: 907, Loss: 0.3130, Train 0.3020, Val 0.3750\n",
      "Fold: 5, Epoch: 908, Loss: 0.3530, Train 0.4480, Val 0.5625\n",
      "Fold: 5, Epoch: 909, Loss: 0.3845, Train 0.4901, Val 0.5938\n",
      "Fold: 5, Epoch: 910, Loss: 0.4561, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 911, Loss: 0.3500, Train 0.5099, Val 0.6094\n",
      "Fold: 5, Epoch: 912, Loss: 0.4425, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 913, Loss: 0.3331, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 914, Loss: 0.3108, Train 0.4728, Val 0.5938\n",
      "Fold: 5, Epoch: 915, Loss: 0.3687, Train 0.4406, Val 0.5625\n",
      "Fold: 5, Epoch: 916, Loss: 0.3405, Train 0.3614, Val 0.4688\n",
      "Fold: 5, Epoch: 917, Loss: 0.3884, Train 0.3465, Val 0.4375\n",
      "Fold: 5, Epoch: 918, Loss: 0.2868, Train 0.3589, Val 0.4688\n",
      "Fold: 5, Epoch: 919, Loss: 0.2710, Train 0.3762, Val 0.4844\n",
      "Fold: 5, Epoch: 920, Loss: 0.3646, Train 0.4208, Val 0.5469\n",
      "Fold: 5, Epoch: 921, Loss: 0.3463, Train 0.4653, Val 0.5781\n",
      "Fold: 5, Epoch: 922, Loss: 0.3220, Train 0.4827, Val 0.5781\n",
      "Fold: 5, Epoch: 923, Loss: 0.2428, Train 0.4827, Val 0.5781\n",
      "Fold: 5, Epoch: 924, Loss: 0.2842, Train 0.4678, Val 0.5781\n",
      "Fold: 5, Epoch: 925, Loss: 0.2914, Train 0.4282, Val 0.5312\n",
      "Fold: 5, Epoch: 926, Loss: 0.2775, Train 0.3515, Val 0.4219\n",
      "Fold: 5, Epoch: 927, Loss: 0.3846, Train 0.2426, Val 0.2969\n",
      "Fold: 5, Epoch: 928, Loss: 0.2952, Train 0.2153, Val 0.2656\n",
      "Fold: 5, Epoch: 929, Loss: 0.2562, Train 0.2129, Val 0.2656\n",
      "Fold: 5, Epoch: 930, Loss: 0.3792, Train 0.2129, Val 0.2656\n",
      "Fold: 5, Epoch: 931, Loss: 0.2694, Train 0.2921, Val 0.3594\n",
      "Fold: 5, Epoch: 932, Loss: 0.2654, Train 0.3688, Val 0.4688\n",
      "Fold: 5, Epoch: 933, Loss: 0.2769, Train 0.4332, Val 0.5469\n",
      "Fold: 5, Epoch: 934, Loss: 0.2866, Train 0.4554, Val 0.5938\n",
      "Fold: 5, Epoch: 935, Loss: 0.3183, Train 0.4752, Val 0.5781\n",
      "Fold: 5, Epoch: 936, Loss: 0.2920, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 937, Loss: 0.2867, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 938, Loss: 0.3334, Train 0.5099, Val 0.6094\n",
      "Fold: 5, Epoch: 939, Loss: 0.3108, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 940, Loss: 0.2844, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 941, Loss: 0.3135, Train 0.4802, Val 0.5938\n",
      "Fold: 5, Epoch: 942, Loss: 0.2623, Train 0.4579, Val 0.5781\n",
      "Fold: 5, Epoch: 943, Loss: 0.3611, Train 0.4332, Val 0.5781\n",
      "Fold: 5, Epoch: 944, Loss: 0.2218, Train 0.4257, Val 0.5625\n",
      "Fold: 5, Epoch: 945, Loss: 0.2854, Train 0.4035, Val 0.5000\n",
      "Fold: 5, Epoch: 946, Loss: 0.2071, Train 0.3688, Val 0.4844\n",
      "Fold: 5, Epoch: 947, Loss: 0.2937, Train 0.4109, Val 0.5000\n",
      "Fold: 5, Epoch: 948, Loss: 0.2367, Train 0.4703, Val 0.5781\n",
      "Fold: 5, Epoch: 949, Loss: 0.3316, Train 0.4975, Val 0.5938\n",
      "Fold: 5, Epoch: 950, Loss: 0.2794, Train 0.5173, Val 0.6094\n",
      "Fold: 5, Epoch: 951, Loss: 0.2485, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 952, Loss: 0.3880, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 953, Loss: 0.2646, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 954, Loss: 0.3498, Train 0.5223, Val 0.6094\n",
      "Fold: 5, Epoch: 955, Loss: 0.3498, Train 0.5173, Val 0.6094\n",
      "Fold: 5, Epoch: 956, Loss: 0.2454, Train 0.5000, Val 0.5938\n",
      "Fold: 5, Epoch: 957, Loss: 0.3402, Train 0.4703, Val 0.5781\n",
      "Fold: 5, Epoch: 958, Loss: 0.3518, Train 0.4926, Val 0.5938\n",
      "Fold: 5, Epoch: 959, Loss: 0.2562, Train 0.5050, Val 0.5938\n",
      "Fold: 5, Epoch: 960, Loss: 0.2638, Train 0.5198, Val 0.6094\n",
      "Fold: 5, Epoch: 961, Loss: 0.3247, Train 0.5173, Val 0.6094\n",
      "Fold: 5, Epoch: 962, Loss: 0.3289, Train 0.5149, Val 0.6094\n",
      "Fold: 5, Epoch: 963, Loss: 0.4332, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 964, Loss: 0.3465, Train 0.4827, Val 0.5938\n",
      "Fold: 5, Epoch: 965, Loss: 0.3888, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 966, Loss: 0.3041, Train 0.5025, Val 0.6094\n",
      "Fold: 5, Epoch: 967, Loss: 0.2422, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 968, Loss: 0.3236, Train 0.5173, Val 0.6094\n",
      "Fold: 5, Epoch: 969, Loss: 0.2240, Train 0.5322, Val 0.6250\n",
      "Fold: 5, Epoch: 970, Loss: 0.3011, Train 0.5347, Val 0.6250\n",
      "Fold: 5, Epoch: 971, Loss: 0.2452, Train 0.5396, Val 0.6250\n",
      "Fold: 5, Epoch: 972, Loss: 0.4202, Train 0.5347, Val 0.6250\n",
      "Fold: 5, Epoch: 973, Loss: 0.4001, Train 0.5050, Val 0.6094\n",
      "Fold: 5, Epoch: 974, Loss: 0.2522, Train 0.4876, Val 0.6094\n",
      "Fold: 5, Epoch: 975, Loss: 0.3659, Train 0.4356, Val 0.5781\n",
      "Fold: 5, Epoch: 976, Loss: 0.3301, Train 0.3960, Val 0.5625\n",
      "Fold: 5, Epoch: 977, Loss: 0.3140, Train 0.4035, Val 0.5625\n",
      "Fold: 5, Epoch: 978, Loss: 0.3024, Train 0.4604, Val 0.5781\n",
      "Fold: 5, Epoch: 979, Loss: 0.2844, Train 0.4926, Val 0.6094\n",
      "Fold: 5, Epoch: 980, Loss: 0.3634, Train 0.5173, Val 0.6094\n",
      "Fold: 5, Epoch: 981, Loss: 0.2982, Train 0.5099, Val 0.6094\n",
      "Fold: 5, Epoch: 982, Loss: 0.3970, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 983, Loss: 0.2809, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 984, Loss: 0.2798, Train 0.4975, Val 0.6094\n",
      "Fold: 5, Epoch: 985, Loss: 0.3608, Train 0.5000, Val 0.6094\n",
      "Fold: 5, Epoch: 986, Loss: 0.2930, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 987, Loss: 0.2904, Train 0.5124, Val 0.6094\n",
      "Fold: 5, Epoch: 988, Loss: 0.3229, Train 0.5248, Val 0.6094\n",
      "Fold: 5, Epoch: 989, Loss: 0.2484, Train 0.5248, Val 0.6250\n",
      "Fold: 5, Epoch: 990, Loss: 0.3428, Train 0.5198, Val 0.6094\n",
      "Fold: 5, Epoch: 991, Loss: 0.2771, Train 0.5074, Val 0.6094\n",
      "Fold: 5, Epoch: 992, Loss: 0.2389, Train 0.4950, Val 0.6094\n",
      "Fold: 5, Epoch: 993, Loss: 0.4338, Train 0.4579, Val 0.5938\n",
      "Fold: 5, Epoch: 994, Loss: 0.2462, Train 0.3837, Val 0.5312\n",
      "Fold: 5, Epoch: 995, Loss: 0.3358, Train 0.2153, Val 0.2812\n",
      "Fold: 5, Epoch: 996, Loss: 0.2759, Train 0.1683, Val 0.2344\n",
      "Fold: 5, Epoch: 997, Loss: 0.2998, Train 0.1683, Val 0.2344\n",
      "Fold: 5, Epoch: 998, Loss: 0.2857, Train 0.1708, Val 0.2188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-14 22:29:51,701] Trial 0 finished with value: 0.24691358024691357 and parameters: {'hidden_channels': 22, 'heads': 12}. Best is trial 0 with value: 0.24691358024691357.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5, Epoch: 999, Loss: 0.2827, Train 0.1955, Val 0.2500\n",
      "Fold: 1, Epoch: 001, Loss: 2.7025, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 002, Loss: 5.0333, Train 0.3713, Val 0.3538\n",
      "Fold: 1, Epoch: 003, Loss: 3.0025, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 004, Loss: 4.0523, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 005, Loss: 3.8551, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 006, Loss: 3.9654, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 007, Loss: 2.7595, Train 0.3119, Val 0.3692\n",
      "Fold: 1, Epoch: 008, Loss: 2.3593, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 009, Loss: 2.5280, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 010, Loss: 2.8683, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 011, Loss: 2.7766, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 012, Loss: 3.1365, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 013, Loss: 2.9956, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 014, Loss: 3.0888, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 015, Loss: 2.5373, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 016, Loss: 2.3925, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 017, Loss: 2.4832, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 018, Loss: 2.5630, Train 0.3119, Val 0.3692\n",
      "Fold: 1, Epoch: 019, Loss: 2.5266, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 020, Loss: 2.2176, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 021, Loss: 2.3180, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 022, Loss: 2.4216, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 023, Loss: 2.4055, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 024, Loss: 2.4469, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 025, Loss: 2.3699, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 026, Loss: 2.4498, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 027, Loss: 2.4039, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 028, Loss: 2.6175, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 029, Loss: 2.2921, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 030, Loss: 2.2089, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 031, Loss: 2.2237, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 032, Loss: 2.3629, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 033, Loss: 2.2808, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 034, Loss: 2.2661, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 035, Loss: 2.2295, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 036, Loss: 2.2419, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 037, Loss: 2.3699, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 038, Loss: 2.2275, Train 0.3762, Val 0.3538\n",
      "Fold: 1, Epoch: 039, Loss: 2.2105, Train 0.1361, Val 0.1385\n",
      "Fold: 1, Epoch: 040, Loss: 2.2174, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 041, Loss: 2.1658, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 042, Loss: 2.2685, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 043, Loss: 2.1632, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 044, Loss: 2.1924, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 045, Loss: 2.2019, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 046, Loss: 2.2718, Train 0.3515, Val 0.3538\n",
      "Fold: 1, Epoch: 047, Loss: 2.1666, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 048, Loss: 2.2554, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 049, Loss: 2.1881, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 050, Loss: 2.1485, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 051, Loss: 2.1579, Train 0.1361, Val 0.1385\n",
      "Fold: 1, Epoch: 052, Loss: 2.2033, Train 0.1361, Val 0.1385\n",
      "Fold: 1, Epoch: 053, Loss: 2.2037, Train 0.1361, Val 0.1385\n",
      "Fold: 1, Epoch: 054, Loss: 2.1993, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 055, Loss: 2.1826, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 056, Loss: 2.1707, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 057, Loss: 2.1733, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 058, Loss: 2.2259, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 059, Loss: 2.2726, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 060, Loss: 2.1928, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 061, Loss: 2.1687, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 062, Loss: 2.1633, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 063, Loss: 2.2429, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 064, Loss: 2.2252, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 065, Loss: 2.2029, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 066, Loss: 2.1630, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 067, Loss: 2.1321, Train 0.1312, Val 0.1385\n",
      "Fold: 1, Epoch: 068, Loss: 2.1904, Train 0.1361, Val 0.1385\n",
      "Fold: 1, Epoch: 069, Loss: 2.2163, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 070, Loss: 2.1536, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 071, Loss: 2.1873, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 072, Loss: 2.2598, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 073, Loss: 2.1797, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 074, Loss: 2.1565, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 075, Loss: 2.1940, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 076, Loss: 2.1587, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 077, Loss: 2.1363, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 078, Loss: 2.1494, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 079, Loss: 2.1800, Train 0.3540, Val 0.3692\n",
      "Fold: 1, Epoch: 080, Loss: 2.1717, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 081, Loss: 2.1536, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 082, Loss: 2.1829, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 083, Loss: 2.1455, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 084, Loss: 2.1628, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 085, Loss: 2.1948, Train 0.3762, Val 0.3538\n",
      "Fold: 1, Epoch: 086, Loss: 2.1284, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 087, Loss: 2.1273, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 088, Loss: 2.1958, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 089, Loss: 2.1666, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 090, Loss: 2.1672, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 091, Loss: 2.1517, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 092, Loss: 2.1437, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 093, Loss: 2.1516, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 094, Loss: 2.1346, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 095, Loss: 2.1510, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 096, Loss: 2.1767, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 097, Loss: 2.1590, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 098, Loss: 2.1237, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 099, Loss: 2.1848, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 100, Loss: 2.1421, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 101, Loss: 2.1495, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 102, Loss: 2.2174, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 103, Loss: 2.1326, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 104, Loss: 2.1860, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 105, Loss: 2.1995, Train 0.3762, Val 0.3538\n",
      "Fold: 1, Epoch: 106, Loss: 2.1691, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 107, Loss: 2.1649, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 108, Loss: 2.1535, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 109, Loss: 2.1420, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 110, Loss: 2.1398, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 111, Loss: 2.1734, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 112, Loss: 2.2057, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 113, Loss: 2.1598, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 114, Loss: 2.1339, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 115, Loss: 2.1844, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 116, Loss: 2.2161, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 117, Loss: 2.1766, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 118, Loss: 2.2308, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 119, Loss: 2.1883, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 120, Loss: 2.1546, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 121, Loss: 2.1598, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 122, Loss: 2.1384, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 123, Loss: 2.1501, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 124, Loss: 2.1574, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 125, Loss: 2.1860, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 126, Loss: 2.1487, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 127, Loss: 2.2146, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 128, Loss: 2.1946, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 129, Loss: 2.1524, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 130, Loss: 2.1209, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 131, Loss: 2.1104, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 132, Loss: 2.1443, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 133, Loss: 2.1229, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 134, Loss: 2.1390, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 135, Loss: 2.1339, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 136, Loss: 2.1434, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 137, Loss: 2.1196, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 138, Loss: 2.1458, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 139, Loss: 2.1274, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 140, Loss: 2.1407, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 141, Loss: 2.1563, Train 0.3762, Val 0.3538\n",
      "Fold: 1, Epoch: 142, Loss: 2.1381, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 143, Loss: 2.1606, Train 0.3762, Val 0.3538\n",
      "Fold: 1, Epoch: 144, Loss: 2.1531, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 145, Loss: 2.1549, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 146, Loss: 2.1693, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 147, Loss: 2.1512, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 148, Loss: 2.1222, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 149, Loss: 2.1287, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 150, Loss: 2.1148, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 151, Loss: 2.1724, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 152, Loss: 2.1408, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 153, Loss: 2.1361, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 154, Loss: 2.1332, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 155, Loss: 2.1334, Train 0.3762, Val 0.3538\n",
      "Fold: 1, Epoch: 156, Loss: 2.1632, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 157, Loss: 2.1433, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 158, Loss: 2.1357, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 159, Loss: 2.1988, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 160, Loss: 2.1526, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 161, Loss: 2.1840, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 162, Loss: 2.2337, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 163, Loss: 2.1433, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 164, Loss: 2.1691, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 165, Loss: 2.1933, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 166, Loss: 2.1624, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 167, Loss: 2.1637, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 168, Loss: 2.1460, Train 0.3762, Val 0.3538\n",
      "Fold: 1, Epoch: 169, Loss: 2.1194, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 170, Loss: 2.1455, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 171, Loss: 2.1397, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 172, Loss: 2.1687, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 173, Loss: 2.1329, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 174, Loss: 2.1585, Train 0.3144, Val 0.3692\n",
      "Fold: 1, Epoch: 175, Loss: 2.2736, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 176, Loss: 2.1334, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 177, Loss: 2.1689, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 178, Loss: 2.2232, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 179, Loss: 2.1480, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 180, Loss: 2.1331, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 181, Loss: 2.1515, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 182, Loss: 2.2464, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 183, Loss: 2.1313, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 184, Loss: 2.1796, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 185, Loss: 2.2444, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 186, Loss: 2.1273, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 187, Loss: 2.1457, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 188, Loss: 2.1697, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 189, Loss: 2.1629, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 190, Loss: 2.1597, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 191, Loss: 2.1427, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 192, Loss: 2.2136, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 193, Loss: 2.1636, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 194, Loss: 2.1528, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 195, Loss: 2.1524, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 196, Loss: 2.1590, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 197, Loss: 2.1455, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 198, Loss: 2.1323, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 199, Loss: 2.1334, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 200, Loss: 2.1265, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 201, Loss: 2.1465, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 202, Loss: 2.1631, Train 0.3168, Val 0.3692\n",
      "Fold: 1, Epoch: 203, Loss: 2.1602, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 204, Loss: 2.1233, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 205, Loss: 2.1527, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 206, Loss: 2.1594, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 207, Loss: 2.1251, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 208, Loss: 2.1514, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 209, Loss: 2.1792, Train 0.3787, Val 0.3538\n",
      "Fold: 1, Epoch: 210, Loss: 2.1433, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 211, Loss: 2.1384, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 212, Loss: 2.1860, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 213, Loss: 2.1372, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 214, Loss: 2.1617, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 215, Loss: 2.1271, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 216, Loss: 2.1438, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 217, Loss: 2.1479, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 218, Loss: 2.1319, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 219, Loss: 2.1282, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 220, Loss: 2.1578, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 221, Loss: 2.1572, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 222, Loss: 2.1107, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 223, Loss: 2.1678, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 224, Loss: 2.1385, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 225, Loss: 2.1269, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 226, Loss: 2.1262, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 227, Loss: 2.1422, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 228, Loss: 2.1526, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 229, Loss: 2.1368, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 230, Loss: 2.1454, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 231, Loss: 2.1093, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 232, Loss: 2.1401, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 233, Loss: 2.1282, Train 0.4183, Val 0.3692\n",
      "Fold: 1, Epoch: 234, Loss: 2.1207, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 235, Loss: 2.1517, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 236, Loss: 2.1144, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 237, Loss: 2.1086, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 238, Loss: 2.1231, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 239, Loss: 2.1187, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 240, Loss: 2.1226, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 241, Loss: 2.1295, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 242, Loss: 2.1286, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 243, Loss: 2.1083, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 244, Loss: 2.1667, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 245, Loss: 2.1221, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 246, Loss: 2.1539, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 247, Loss: 2.1246, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 248, Loss: 2.1439, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 249, Loss: 2.1428, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 250, Loss: 2.1305, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 251, Loss: 2.1174, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 252, Loss: 2.1285, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 253, Loss: 2.1281, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 254, Loss: 2.1234, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 255, Loss: 2.1389, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 256, Loss: 2.1270, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 257, Loss: 2.1508, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 258, Loss: 2.0953, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 259, Loss: 2.1266, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 260, Loss: 2.1481, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 261, Loss: 2.1373, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 262, Loss: 2.1354, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 263, Loss: 2.1329, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 264, Loss: 2.1073, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 265, Loss: 2.1370, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 266, Loss: 2.1187, Train 0.3342, Val 0.3692\n",
      "Fold: 1, Epoch: 267, Loss: 2.1158, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 268, Loss: 2.1217, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 269, Loss: 2.1260, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 270, Loss: 2.1260, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 271, Loss: 2.1165, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 272, Loss: 2.1536, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 273, Loss: 2.1424, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 274, Loss: 2.1292, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 275, Loss: 2.1120, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 276, Loss: 2.1388, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 277, Loss: 2.1160, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 278, Loss: 2.1081, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 279, Loss: 2.1473, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 280, Loss: 2.1218, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 281, Loss: 2.1277, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 282, Loss: 2.1039, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 283, Loss: 2.1074, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 284, Loss: 2.1106, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 285, Loss: 2.1287, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 286, Loss: 2.1219, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 287, Loss: 2.1058, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 288, Loss: 2.1382, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 289, Loss: 2.0977, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 290, Loss: 2.1709, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 291, Loss: 2.1263, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 292, Loss: 2.1224, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 293, Loss: 2.1287, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 294, Loss: 2.1284, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 295, Loss: 2.1228, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 296, Loss: 2.1272, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 297, Loss: 2.1210, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 298, Loss: 2.1110, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 299, Loss: 2.1145, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 300, Loss: 2.1210, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 301, Loss: 2.1099, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 302, Loss: 2.1218, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 303, Loss: 2.1224, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 304, Loss: 2.1113, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 305, Loss: 2.0933, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 306, Loss: 2.1279, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 307, Loss: 2.1236, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 308, Loss: 2.1145, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 309, Loss: 2.1133, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 310, Loss: 2.1479, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 311, Loss: 2.1218, Train 0.3688, Val 0.3692\n",
      "Fold: 1, Epoch: 312, Loss: 2.1318, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 313, Loss: 2.1152, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 314, Loss: 2.1051, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 315, Loss: 2.1047, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 316, Loss: 2.0983, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 317, Loss: 2.1086, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 318, Loss: 2.0900, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 319, Loss: 2.1096, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 320, Loss: 2.0972, Train 0.3243, Val 0.3692\n",
      "Fold: 1, Epoch: 321, Loss: 2.1577, Train 0.3837, Val 0.3538\n",
      "Fold: 1, Epoch: 322, Loss: 2.0962, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 323, Loss: 2.1074, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 324, Loss: 2.1246, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 325, Loss: 2.0901, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 326, Loss: 2.0955, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 327, Loss: 2.1076, Train 0.3713, Val 0.3692\n",
      "Fold: 1, Epoch: 328, Loss: 2.1291, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 329, Loss: 2.1030, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 330, Loss: 2.1090, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 331, Loss: 2.1641, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 332, Loss: 2.1361, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 333, Loss: 2.1081, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 334, Loss: 2.1389, Train 0.3837, Val 0.3538\n",
      "Fold: 1, Epoch: 335, Loss: 2.0963, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 336, Loss: 2.0848, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 337, Loss: 2.1410, Train 0.3812, Val 0.3692\n",
      "Fold: 1, Epoch: 338, Loss: 2.0794, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 339, Loss: 2.1468, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 340, Loss: 2.0839, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 341, Loss: 2.0912, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 342, Loss: 2.1417, Train 0.3837, Val 0.3538\n",
      "Fold: 1, Epoch: 343, Loss: 2.0841, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 344, Loss: 2.1373, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 345, Loss: 2.0594, Train 0.3540, Val 0.3692\n",
      "Fold: 1, Epoch: 346, Loss: 2.0927, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 347, Loss: 2.0615, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 348, Loss: 2.0787, Train 0.3985, Val 0.3538\n",
      "Fold: 1, Epoch: 349, Loss: 2.0459, Train 0.3812, Val 0.3538\n",
      "Fold: 1, Epoch: 350, Loss: 2.1088, Train 0.3540, Val 0.3692\n",
      "Fold: 1, Epoch: 351, Loss: 2.0608, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 352, Loss: 2.0284, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 353, Loss: 2.0564, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 354, Loss: 2.0346, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 355, Loss: 2.0464, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 356, Loss: 2.0065, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 357, Loss: 2.0664, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 358, Loss: 1.9781, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 359, Loss: 1.9755, Train 0.3342, Val 0.3692\n",
      "Fold: 1, Epoch: 360, Loss: 1.9929, Train 0.3342, Val 0.3692\n",
      "Fold: 1, Epoch: 361, Loss: 1.9651, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 362, Loss: 1.9598, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 363, Loss: 1.9681, Train 0.3614, Val 0.3692\n",
      "Fold: 1, Epoch: 364, Loss: 1.9615, Train 0.3391, Val 0.3692\n",
      "Fold: 1, Epoch: 365, Loss: 1.9093, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 366, Loss: 1.8480, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 367, Loss: 1.9081, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 368, Loss: 1.8535, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 369, Loss: 1.8733, Train 0.3045, Val 0.3538\n",
      "Fold: 1, Epoch: 370, Loss: 1.9279, Train 0.3193, Val 0.3692\n",
      "Fold: 1, Epoch: 371, Loss: 1.8472, Train 0.3317, Val 0.3692\n",
      "Fold: 1, Epoch: 372, Loss: 1.8284, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 373, Loss: 1.7827, Train 0.3292, Val 0.3692\n",
      "Fold: 1, Epoch: 374, Loss: 1.7170, Train 0.3317, Val 0.3692\n",
      "Fold: 1, Epoch: 375, Loss: 1.6578, Train 0.6485, Val 0.6923\n",
      "Fold: 1, Epoch: 376, Loss: 1.6217, Train 0.3366, Val 0.3692\n",
      "Fold: 1, Epoch: 377, Loss: 1.6056, Train 0.3515, Val 0.3692\n",
      "Fold: 1, Epoch: 378, Loss: 1.5537, Train 0.3416, Val 0.3692\n",
      "Fold: 1, Epoch: 379, Loss: 1.4929, Train 0.3267, Val 0.3692\n",
      "Fold: 1, Epoch: 380, Loss: 1.5501, Train 0.5223, Val 0.5692\n",
      "Fold: 1, Epoch: 381, Loss: 1.4588, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 382, Loss: 1.5803, Train 0.6782, Val 0.7231\n",
      "Fold: 1, Epoch: 383, Loss: 1.3583, Train 0.3218, Val 0.3692\n",
      "Fold: 1, Epoch: 384, Loss: 1.7362, Train 0.6881, Val 0.7231\n",
      "Fold: 1, Epoch: 385, Loss: 1.3459, Train 0.5149, Val 0.4923\n",
      "Fold: 1, Epoch: 386, Loss: 1.6905, Train 0.5842, Val 0.6000\n",
      "Fold: 1, Epoch: 387, Loss: 1.3619, Train 0.6733, Val 0.6923\n",
      "Fold: 1, Epoch: 388, Loss: 1.5872, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 389, Loss: 1.4322, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 390, Loss: 1.4672, Train 0.5941, Val 0.6154\n",
      "Fold: 1, Epoch: 391, Loss: 1.5540, Train 0.7030, Val 0.7231\n",
      "Fold: 1, Epoch: 392, Loss: 1.3135, Train 0.7178, Val 0.7077\n",
      "Fold: 1, Epoch: 393, Loss: 1.3955, Train 0.7153, Val 0.7538\n",
      "Fold: 1, Epoch: 394, Loss: 1.4248, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 395, Loss: 1.3147, Train 0.5173, Val 0.4923\n",
      "Fold: 1, Epoch: 396, Loss: 1.4051, Train 0.5173, Val 0.4923\n",
      "Fold: 1, Epoch: 397, Loss: 1.3179, Train 0.5495, Val 0.5385\n",
      "Fold: 1, Epoch: 398, Loss: 1.2964, Train 0.7030, Val 0.7231\n",
      "Fold: 1, Epoch: 399, Loss: 1.3548, Train 0.5198, Val 0.4769\n",
      "Fold: 1, Epoch: 400, Loss: 1.3012, Train 0.4257, Val 0.4308\n",
      "Fold: 1, Epoch: 401, Loss: 1.3296, Train 0.4233, Val 0.4308\n",
      "Fold: 1, Epoch: 402, Loss: 1.2605, Train 0.2723, Val 0.2000\n",
      "Fold: 1, Epoch: 403, Loss: 1.3455, Train 0.5149, Val 0.4923\n",
      "Fold: 1, Epoch: 404, Loss: 1.3144, Train 0.5149, Val 0.4923\n",
      "Fold: 1, Epoch: 405, Loss: 1.3721, Train 0.5792, Val 0.5538\n",
      "Fold: 1, Epoch: 406, Loss: 1.4904, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 407, Loss: 1.2839, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 408, Loss: 1.3206, Train 0.7030, Val 0.7231\n",
      "Fold: 1, Epoch: 409, Loss: 1.3817, Train 0.4282, Val 0.4000\n",
      "Fold: 1, Epoch: 410, Loss: 1.3354, Train 0.3416, Val 0.2923\n",
      "Fold: 1, Epoch: 411, Loss: 1.2697, Train 0.5173, Val 0.4923\n",
      "Fold: 1, Epoch: 412, Loss: 1.2427, Train 0.6856, Val 0.7231\n",
      "Fold: 1, Epoch: 413, Loss: 1.3993, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 414, Loss: 1.2661, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 415, Loss: 1.2944, Train 0.7005, Val 0.7231\n",
      "Fold: 1, Epoch: 416, Loss: 1.2908, Train 0.5371, Val 0.5077\n",
      "Fold: 1, Epoch: 417, Loss: 1.2339, Train 0.4282, Val 0.4308\n",
      "Fold: 1, Epoch: 418, Loss: 1.2587, Train 0.2574, Val 0.2000\n",
      "Fold: 1, Epoch: 419, Loss: 1.2978, Train 0.5297, Val 0.4923\n",
      "Fold: 1, Epoch: 420, Loss: 1.2582, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 421, Loss: 1.1893, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 422, Loss: 1.2742, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 423, Loss: 1.3709, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 424, Loss: 1.3688, Train 0.7030, Val 0.7385\n",
      "Fold: 1, Epoch: 425, Loss: 1.2273, Train 0.3168, Val 0.2308\n",
      "Fold: 1, Epoch: 426, Loss: 1.4385, Train 0.5644, Val 0.5077\n",
      "Fold: 1, Epoch: 427, Loss: 1.2571, Train 0.7104, Val 0.7385\n",
      "Fold: 1, Epoch: 428, Loss: 1.2196, Train 0.7005, Val 0.7385\n",
      "Fold: 1, Epoch: 429, Loss: 1.2493, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 430, Loss: 1.2478, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 431, Loss: 1.3218, Train 0.7005, Val 0.7231\n",
      "Fold: 1, Epoch: 432, Loss: 1.2143, Train 0.7153, Val 0.7385\n",
      "Fold: 1, Epoch: 433, Loss: 1.2093, Train 0.7203, Val 0.7231\n",
      "Fold: 1, Epoch: 434, Loss: 1.2068, Train 0.7129, Val 0.7231\n",
      "Fold: 1, Epoch: 435, Loss: 1.3645, Train 0.7005, Val 0.7231\n",
      "Fold: 1, Epoch: 436, Loss: 1.2064, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 437, Loss: 1.2881, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 438, Loss: 1.2765, Train 0.6980, Val 0.7231\n",
      "Fold: 1, Epoch: 439, Loss: 1.1810, Train 0.7475, Val 0.7692\n",
      "Fold: 1, Epoch: 440, Loss: 1.1851, Train 0.4431, Val 0.4308\n",
      "Fold: 1, Epoch: 441, Loss: 1.1784, Train 0.4876, Val 0.4769\n",
      "Fold: 1, Epoch: 442, Loss: 1.1221, Train 0.4431, Val 0.4308\n",
      "Fold: 1, Epoch: 443, Loss: 1.2071, Train 0.5792, Val 0.5231\n",
      "Fold: 1, Epoch: 444, Loss: 1.2664, Train 0.7079, Val 0.7385\n",
      "Fold: 1, Epoch: 445, Loss: 1.3147, Train 0.5446, Val 0.5077\n",
      "Fold: 1, Epoch: 446, Loss: 1.2421, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 447, Loss: 1.1991, Train 0.7104, Val 0.7231\n",
      "Fold: 1, Epoch: 448, Loss: 1.1982, Train 0.7178, Val 0.7231\n",
      "Fold: 1, Epoch: 449, Loss: 1.1912, Train 0.7203, Val 0.7231\n",
      "Fold: 1, Epoch: 450, Loss: 1.1860, Train 0.7450, Val 0.7385\n",
      "Fold: 1, Epoch: 451, Loss: 1.1722, Train 0.7550, Val 0.7538\n",
      "Fold: 1, Epoch: 452, Loss: 1.1661, Train 0.5248, Val 0.4769\n",
      "Fold: 1, Epoch: 453, Loss: 1.1724, Train 0.5198, Val 0.4923\n",
      "Fold: 1, Epoch: 454, Loss: 1.2510, Train 0.7376, Val 0.7538\n",
      "Fold: 1, Epoch: 455, Loss: 1.1294, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 456, Loss: 1.1946, Train 0.6757, Val 0.7077\n",
      "Fold: 1, Epoch: 457, Loss: 1.2463, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 458, Loss: 1.1701, Train 0.7450, Val 0.7538\n",
      "Fold: 1, Epoch: 459, Loss: 1.1475, Train 0.5396, Val 0.5231\n",
      "Fold: 1, Epoch: 460, Loss: 1.3099, Train 0.5644, Val 0.5385\n",
      "Fold: 1, Epoch: 461, Loss: 1.4009, Train 0.4678, Val 0.4000\n",
      "Fold: 1, Epoch: 462, Loss: 1.1027, Train 0.7079, Val 0.6769\n",
      "Fold: 1, Epoch: 463, Loss: 1.1565, Train 0.6906, Val 0.6923\n",
      "Fold: 1, Epoch: 464, Loss: 1.2246, Train 0.7054, Val 0.7231\n",
      "Fold: 1, Epoch: 465, Loss: 1.1614, Train 0.7327, Val 0.7385\n",
      "Fold: 1, Epoch: 466, Loss: 1.0860, Train 0.7104, Val 0.7538\n",
      "Fold: 1, Epoch: 467, Loss: 1.1154, Train 0.5272, Val 0.5077\n",
      "Fold: 1, Epoch: 468, Loss: 1.1562, Train 0.5470, Val 0.5077\n",
      "Fold: 1, Epoch: 469, Loss: 1.2245, Train 0.7723, Val 0.7538\n",
      "Fold: 1, Epoch: 470, Loss: 1.2690, Train 0.7772, Val 0.7692\n",
      "Fold: 1, Epoch: 471, Loss: 1.1888, Train 0.7649, Val 0.7538\n",
      "Fold: 1, Epoch: 472, Loss: 1.1230, Train 0.7574, Val 0.7385\n",
      "Fold: 1, Epoch: 473, Loss: 1.1446, Train 0.7723, Val 0.7538\n",
      "Fold: 1, Epoch: 474, Loss: 1.1676, Train 0.7723, Val 0.7692\n",
      "Fold: 1, Epoch: 475, Loss: 1.2067, Train 0.7327, Val 0.7385\n",
      "Fold: 1, Epoch: 476, Loss: 1.0868, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 477, Loss: 1.1277, Train 0.6856, Val 0.7231\n",
      "Fold: 1, Epoch: 478, Loss: 1.0807, Train 0.7054, Val 0.7231\n",
      "Fold: 1, Epoch: 479, Loss: 1.0726, Train 0.7673, Val 0.7538\n",
      "Fold: 1, Epoch: 480, Loss: 1.0142, Train 0.7673, Val 0.7385\n",
      "Fold: 1, Epoch: 481, Loss: 1.0317, Train 0.5743, Val 0.4923\n",
      "Fold: 1, Epoch: 482, Loss: 1.0756, Train 0.4901, Val 0.4000\n",
      "Fold: 1, Epoch: 483, Loss: 1.0482, Train 0.5124, Val 0.4154\n",
      "Fold: 1, Epoch: 484, Loss: 1.0459, Train 0.7847, Val 0.7692\n",
      "Fold: 1, Epoch: 485, Loss: 1.0405, Train 0.7450, Val 0.7385\n",
      "Fold: 1, Epoch: 486, Loss: 1.0526, Train 0.7030, Val 0.7231\n",
      "Fold: 1, Epoch: 487, Loss: 1.0121, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 488, Loss: 1.0036, Train 0.7252, Val 0.7385\n",
      "Fold: 1, Epoch: 489, Loss: 1.0413, Train 0.7624, Val 0.7538\n",
      "Fold: 1, Epoch: 490, Loss: 1.0597, Train 0.4851, Val 0.4000\n",
      "Fold: 1, Epoch: 491, Loss: 1.0001, Train 0.4777, Val 0.4000\n",
      "Fold: 1, Epoch: 492, Loss: 1.0782, Train 0.4876, Val 0.4000\n",
      "Fold: 1, Epoch: 493, Loss: 1.0728, Train 0.7797, Val 0.7692\n",
      "Fold: 1, Epoch: 494, Loss: 1.0117, Train 0.7054, Val 0.7231\n",
      "Fold: 1, Epoch: 495, Loss: 1.0674, Train 0.6807, Val 0.7077\n",
      "Fold: 1, Epoch: 496, Loss: 1.2413, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 497, Loss: 1.0278, Train 0.7797, Val 0.7538\n",
      "Fold: 1, Epoch: 498, Loss: 0.9710, Train 0.4926, Val 0.4000\n",
      "Fold: 1, Epoch: 499, Loss: 1.2294, Train 0.4975, Val 0.4000\n",
      "Fold: 1, Epoch: 500, Loss: 1.0868, Train 0.5025, Val 0.4000\n",
      "Fold: 1, Epoch: 501, Loss: 0.9881, Train 0.7153, Val 0.6615\n",
      "Fold: 1, Epoch: 502, Loss: 0.9767, Train 0.7673, Val 0.7538\n",
      "Fold: 1, Epoch: 503, Loss: 0.9985, Train 0.7599, Val 0.7385\n",
      "Fold: 1, Epoch: 504, Loss: 1.0514, Train 0.7748, Val 0.7538\n",
      "Fold: 1, Epoch: 505, Loss: 1.1071, Train 0.7673, Val 0.7538\n",
      "Fold: 1, Epoch: 506, Loss: 1.1297, Train 0.7426, Val 0.7077\n",
      "Fold: 1, Epoch: 507, Loss: 1.0387, Train 0.7871, Val 0.7692\n",
      "Fold: 1, Epoch: 508, Loss: 0.9083, Train 0.7723, Val 0.7538\n",
      "Fold: 1, Epoch: 509, Loss: 1.0495, Train 0.7450, Val 0.7385\n",
      "Fold: 1, Epoch: 510, Loss: 1.0907, Train 0.7748, Val 0.7538\n",
      "Fold: 1, Epoch: 511, Loss: 1.0565, Train 0.7847, Val 0.7538\n",
      "Fold: 1, Epoch: 512, Loss: 0.9544, Train 0.7649, Val 0.7385\n",
      "Fold: 1, Epoch: 513, Loss: 1.0519, Train 0.7178, Val 0.7231\n",
      "Fold: 1, Epoch: 514, Loss: 0.9993, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 515, Loss: 1.0103, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 516, Loss: 0.9463, Train 0.7153, Val 0.7231\n",
      "Fold: 1, Epoch: 517, Loss: 0.9941, Train 0.7450, Val 0.7385\n",
      "Fold: 1, Epoch: 518, Loss: 0.9901, Train 0.7624, Val 0.7385\n",
      "Fold: 1, Epoch: 519, Loss: 0.9098, Train 0.7599, Val 0.7385\n",
      "Fold: 1, Epoch: 520, Loss: 0.9402, Train 0.7450, Val 0.7385\n",
      "Fold: 1, Epoch: 521, Loss: 0.9253, Train 0.7178, Val 0.7231\n",
      "Fold: 1, Epoch: 522, Loss: 0.8573, Train 0.7178, Val 0.7231\n",
      "Fold: 1, Epoch: 523, Loss: 1.0380, Train 0.7723, Val 0.7385\n",
      "Fold: 1, Epoch: 524, Loss: 0.9601, Train 0.7970, Val 0.7692\n",
      "Fold: 1, Epoch: 525, Loss: 0.9586, Train 0.7970, Val 0.7846\n",
      "Fold: 1, Epoch: 526, Loss: 1.0659, Train 0.7946, Val 0.7692\n",
      "Fold: 1, Epoch: 527, Loss: 0.9536, Train 0.7946, Val 0.7692\n",
      "Fold: 1, Epoch: 528, Loss: 0.8904, Train 0.7847, Val 0.7692\n",
      "Fold: 1, Epoch: 529, Loss: 0.9264, Train 0.7426, Val 0.7385\n",
      "Fold: 1, Epoch: 530, Loss: 0.9510, Train 0.7129, Val 0.7231\n",
      "Fold: 1, Epoch: 531, Loss: 0.9594, Train 0.7698, Val 0.7385\n",
      "Fold: 1, Epoch: 532, Loss: 0.9811, Train 0.7921, Val 0.7692\n",
      "Fold: 1, Epoch: 533, Loss: 1.0450, Train 0.7921, Val 0.7692\n",
      "Fold: 1, Epoch: 534, Loss: 1.0542, Train 0.7748, Val 0.7538\n",
      "Fold: 1, Epoch: 535, Loss: 1.0255, Train 0.6955, Val 0.7077\n",
      "Fold: 1, Epoch: 536, Loss: 0.9500, Train 0.6782, Val 0.7077\n",
      "Fold: 1, Epoch: 537, Loss: 0.9684, Train 0.6757, Val 0.7077\n",
      "Fold: 1, Epoch: 538, Loss: 0.9515, Train 0.6782, Val 0.7077\n",
      "Fold: 1, Epoch: 539, Loss: 1.0511, Train 0.7129, Val 0.7231\n",
      "Fold: 1, Epoch: 540, Loss: 0.9300, Train 0.7698, Val 0.7385\n",
      "Fold: 1, Epoch: 541, Loss: 0.9442, Train 0.7054, Val 0.7231\n",
      "Fold: 1, Epoch: 542, Loss: 1.1033, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 543, Loss: 0.9203, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 544, Loss: 0.9713, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 545, Loss: 0.9019, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 546, Loss: 0.9179, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 547, Loss: 0.9638, Train 0.7203, Val 0.7231\n",
      "Fold: 1, Epoch: 548, Loss: 0.9428, Train 0.7772, Val 0.7385\n",
      "Fold: 1, Epoch: 549, Loss: 0.9359, Train 0.7871, Val 0.7692\n",
      "Fold: 1, Epoch: 550, Loss: 1.0305, Train 0.7228, Val 0.7231\n",
      "Fold: 1, Epoch: 551, Loss: 0.8990, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 552, Loss: 0.9556, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 553, Loss: 0.9339, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 554, Loss: 0.9553, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 555, Loss: 0.9139, Train 0.7525, Val 0.7538\n",
      "Fold: 1, Epoch: 556, Loss: 0.9432, Train 0.7748, Val 0.7538\n",
      "Fold: 1, Epoch: 557, Loss: 0.9482, Train 0.7896, Val 0.7692\n",
      "Fold: 1, Epoch: 558, Loss: 0.9757, Train 0.7772, Val 0.7692\n",
      "Fold: 1, Epoch: 559, Loss: 0.8705, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 560, Loss: 0.9179, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 561, Loss: 0.9603, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 562, Loss: 0.9066, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 563, Loss: 0.8525, Train 0.7030, Val 0.7231\n",
      "Fold: 1, Epoch: 564, Loss: 0.8858, Train 0.7228, Val 0.7692\n",
      "Fold: 1, Epoch: 565, Loss: 0.9577, Train 0.6559, Val 0.6769\n",
      "Fold: 1, Epoch: 566, Loss: 0.9674, Train 0.7252, Val 0.7692\n",
      "Fold: 1, Epoch: 567, Loss: 0.9727, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 568, Loss: 0.8509, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 569, Loss: 0.8856, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 570, Loss: 0.8634, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 571, Loss: 0.8426, Train 0.7327, Val 0.7231\n",
      "Fold: 1, Epoch: 572, Loss: 0.9298, Train 0.7599, Val 0.7385\n",
      "Fold: 1, Epoch: 573, Loss: 0.9039, Train 0.7153, Val 0.7385\n",
      "Fold: 1, Epoch: 574, Loss: 0.8150, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 575, Loss: 0.9430, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 576, Loss: 0.7922, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 577, Loss: 0.8560, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 578, Loss: 0.9663, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 579, Loss: 0.9091, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 580, Loss: 0.9124, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 581, Loss: 0.8949, Train 0.6881, Val 0.7231\n",
      "Fold: 1, Epoch: 582, Loss: 0.9180, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 583, Loss: 0.8950, Train 0.7079, Val 0.7231\n",
      "Fold: 1, Epoch: 584, Loss: 0.8653, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 585, Loss: 0.9154, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 586, Loss: 0.8530, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 587, Loss: 0.9013, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 588, Loss: 0.9260, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 589, Loss: 0.9610, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 590, Loss: 0.9095, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 591, Loss: 0.8482, Train 0.7450, Val 0.7385\n",
      "Fold: 1, Epoch: 592, Loss: 1.0114, Train 0.7525, Val 0.7385\n",
      "Fold: 1, Epoch: 593, Loss: 0.8458, Train 0.7426, Val 0.7385\n",
      "Fold: 1, Epoch: 594, Loss: 0.9162, Train 0.7450, Val 0.7538\n",
      "Fold: 1, Epoch: 595, Loss: 0.9177, Train 0.7104, Val 0.7385\n",
      "Fold: 1, Epoch: 596, Loss: 0.8479, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 597, Loss: 0.9037, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 598, Loss: 0.8936, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 599, Loss: 0.9259, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 600, Loss: 0.9096, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 601, Loss: 1.0027, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 602, Loss: 0.9241, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 603, Loss: 0.8085, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 604, Loss: 0.8482, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 605, Loss: 0.8360, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 606, Loss: 0.9322, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 607, Loss: 0.9039, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 608, Loss: 0.8343, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 609, Loss: 1.0563, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 610, Loss: 0.8411, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 611, Loss: 0.8620, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 612, Loss: 0.8939, Train 0.6807, Val 0.7077\n",
      "Fold: 1, Epoch: 613, Loss: 0.9997, Train 0.6782, Val 0.7077\n",
      "Fold: 1, Epoch: 614, Loss: 1.0395, Train 0.6832, Val 0.7077\n",
      "Fold: 1, Epoch: 615, Loss: 0.9240, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 616, Loss: 0.8961, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 617, Loss: 0.8433, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 618, Loss: 0.9272, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 619, Loss: 0.8446, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 620, Loss: 0.9014, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 621, Loss: 0.8340, Train 0.7351, Val 0.7385\n",
      "Fold: 1, Epoch: 622, Loss: 0.9299, Train 0.7129, Val 0.7231\n",
      "Fold: 1, Epoch: 623, Loss: 0.8930, Train 0.6832, Val 0.7077\n",
      "Fold: 1, Epoch: 624, Loss: 0.9910, Train 0.6832, Val 0.7077\n",
      "Fold: 1, Epoch: 625, Loss: 0.8509, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 626, Loss: 0.8700, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 627, Loss: 0.8967, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 628, Loss: 0.9028, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 629, Loss: 0.9012, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 630, Loss: 0.7826, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 631, Loss: 0.8455, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 632, Loss: 0.8452, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 633, Loss: 0.8691, Train 0.6807, Val 0.7077\n",
      "Fold: 1, Epoch: 634, Loss: 0.9089, Train 0.6807, Val 0.7077\n",
      "Fold: 1, Epoch: 635, Loss: 0.8673, Train 0.6832, Val 0.7077\n",
      "Fold: 1, Epoch: 636, Loss: 0.8696, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 637, Loss: 0.9219, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 638, Loss: 0.8634, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 639, Loss: 0.8976, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 640, Loss: 0.8483, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 641, Loss: 0.8209, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 642, Loss: 0.7620, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 643, Loss: 0.9500, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 644, Loss: 0.8629, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 645, Loss: 0.7810, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 646, Loss: 0.8210, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 647, Loss: 0.8945, Train 0.7054, Val 0.7231\n",
      "Fold: 1, Epoch: 648, Loss: 0.9666, Train 0.7327, Val 0.7385\n",
      "Fold: 1, Epoch: 649, Loss: 0.9110, Train 0.7475, Val 0.7385\n",
      "Fold: 1, Epoch: 650, Loss: 0.8945, Train 0.5743, Val 0.5538\n",
      "Fold: 1, Epoch: 651, Loss: 0.8883, Train 0.4653, Val 0.4769\n",
      "Fold: 1, Epoch: 652, Loss: 0.8333, Train 0.4035, Val 0.3692\n",
      "Fold: 1, Epoch: 653, Loss: 0.8748, Train 0.5248, Val 0.4769\n",
      "Fold: 1, Epoch: 654, Loss: 0.8661, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 655, Loss: 0.8324, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 656, Loss: 0.8399, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 657, Loss: 0.8540, Train 0.7054, Val 0.7231\n",
      "Fold: 1, Epoch: 658, Loss: 0.8342, Train 0.7104, Val 0.7231\n",
      "Fold: 1, Epoch: 659, Loss: 0.8467, Train 0.4480, Val 0.4154\n",
      "Fold: 1, Epoch: 660, Loss: 0.8331, Train 0.6064, Val 0.6000\n",
      "Fold: 1, Epoch: 661, Loss: 0.8433, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 662, Loss: 0.7877, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 663, Loss: 0.8539, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 664, Loss: 0.8197, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 665, Loss: 0.8220, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 666, Loss: 0.8530, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 667, Loss: 0.7765, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 668, Loss: 0.8655, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 669, Loss: 0.8379, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 670, Loss: 0.8979, Train 0.6807, Val 0.7077\n",
      "Fold: 1, Epoch: 671, Loss: 0.9995, Train 0.6807, Val 0.7077\n",
      "Fold: 1, Epoch: 672, Loss: 0.9566, Train 0.6807, Val 0.7077\n",
      "Fold: 1, Epoch: 673, Loss: 0.8668, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 674, Loss: 0.8315, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 675, Loss: 0.8895, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 676, Loss: 0.8390, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 677, Loss: 0.9481, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 678, Loss: 0.8313, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 679, Loss: 0.8430, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 680, Loss: 0.9860, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 681, Loss: 0.7987, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 682, Loss: 0.8166, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 683, Loss: 0.8104, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 684, Loss: 0.8523, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 685, Loss: 0.9058, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 686, Loss: 0.9260, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 687, Loss: 0.9069, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 688, Loss: 0.8288, Train 0.6708, Val 0.6923\n",
      "Fold: 1, Epoch: 689, Loss: 0.9095, Train 0.5668, Val 0.4923\n",
      "Fold: 1, Epoch: 690, Loss: 0.8524, Train 0.6287, Val 0.5846\n",
      "Fold: 1, Epoch: 691, Loss: 0.9999, Train 0.7005, Val 0.7385\n",
      "Fold: 1, Epoch: 692, Loss: 0.8249, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 693, Loss: 0.7735, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 694, Loss: 0.8825, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 695, Loss: 0.8093, Train 0.7030, Val 0.7231\n",
      "Fold: 1, Epoch: 696, Loss: 0.8389, Train 0.7079, Val 0.7231\n",
      "Fold: 1, Epoch: 697, Loss: 0.8472, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 698, Loss: 0.8427, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 699, Loss: 0.8004, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 700, Loss: 0.7808, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 701, Loss: 0.8747, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 702, Loss: 0.8584, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 703, Loss: 0.8539, Train 0.6931, Val 0.7231\n",
      "Fold: 1, Epoch: 704, Loss: 0.7722, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 705, Loss: 0.8206, Train 0.6980, Val 0.7231\n",
      "Fold: 1, Epoch: 706, Loss: 0.8473, Train 0.6881, Val 0.7231\n",
      "Fold: 1, Epoch: 707, Loss: 0.8580, Train 0.6906, Val 0.7231\n",
      "Fold: 1, Epoch: 708, Loss: 0.7882, Train 0.6856, Val 0.7077\n",
      "Fold: 1, Epoch: 709, Loss: 0.8130, Train 0.6881, Val 0.7077\n",
      "Fold: 1, Epoch: 710, Loss: 0.8826, Train 0.6931, Val 0.7077\n",
      "Fold: 1, Epoch: 711, Loss: 0.7695, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 712, Loss: 0.8047, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 713, Loss: 0.8099, Train 0.7079, Val 0.7385\n",
      "Fold: 1, Epoch: 714, Loss: 0.7482, Train 0.4752, Val 0.4308\n",
      "Fold: 1, Epoch: 715, Loss: 0.8158, Train 0.7129, Val 0.7385\n",
      "Fold: 1, Epoch: 716, Loss: 0.8291, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 717, Loss: 0.8472, Train 0.6931, Val 0.7077\n",
      "Fold: 1, Epoch: 718, Loss: 0.7389, Train 0.6931, Val 0.7077\n",
      "Fold: 1, Epoch: 719, Loss: 0.8242, Train 0.6931, Val 0.7077\n",
      "Fold: 1, Epoch: 720, Loss: 0.7693, Train 0.6906, Val 0.7077\n",
      "Fold: 1, Epoch: 721, Loss: 0.7272, Train 0.7054, Val 0.7231\n",
      "Fold: 1, Epoch: 722, Loss: 0.8326, Train 0.7178, Val 0.7231\n",
      "Fold: 1, Epoch: 723, Loss: 0.7240, Train 0.7871, Val 0.7692\n",
      "Fold: 1, Epoch: 724, Loss: 0.7128, Train 0.7896, Val 0.7692\n",
      "Fold: 1, Epoch: 725, Loss: 0.7261, Train 0.7847, Val 0.7846\n",
      "Fold: 1, Epoch: 726, Loss: 0.7770, Train 0.6559, Val 0.5846\n",
      "Fold: 1, Epoch: 727, Loss: 0.7418, Train 0.5495, Val 0.4462\n",
      "Fold: 1, Epoch: 728, Loss: 0.8148, Train 0.4554, Val 0.3846\n",
      "Fold: 1, Epoch: 729, Loss: 0.7558, Train 0.4505, Val 0.4000\n",
      "Fold: 1, Epoch: 730, Loss: 0.7691, Train 0.4381, Val 0.4000\n",
      "Fold: 1, Epoch: 731, Loss: 0.7681, Train 0.4653, Val 0.4308\n",
      "Fold: 1, Epoch: 732, Loss: 0.7713, Train 0.5223, Val 0.4769\n",
      "Fold: 1, Epoch: 733, Loss: 0.7626, Train 0.7574, Val 0.7538\n",
      "Fold: 1, Epoch: 734, Loss: 0.7364, Train 0.7475, Val 0.7692\n",
      "Fold: 1, Epoch: 735, Loss: 0.7847, Train 0.7079, Val 0.7385\n",
      "Fold: 1, Epoch: 736, Loss: 0.7566, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 737, Loss: 0.7367, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 738, Loss: 0.7388, Train 0.7129, Val 0.7692\n",
      "Fold: 1, Epoch: 739, Loss: 0.7575, Train 0.7228, Val 0.7385\n",
      "Fold: 1, Epoch: 740, Loss: 0.6390, Train 0.6955, Val 0.6769\n",
      "Fold: 1, Epoch: 741, Loss: 0.7159, Train 0.6559, Val 0.6000\n",
      "Fold: 1, Epoch: 742, Loss: 0.6948, Train 0.7426, Val 0.8000\n",
      "Fold: 1, Epoch: 743, Loss: 0.7215, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 744, Loss: 0.6847, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 745, Loss: 0.6953, Train 0.6955, Val 0.7231\n",
      "Fold: 1, Epoch: 746, Loss: 0.6486, Train 0.7104, Val 0.7692\n",
      "Fold: 1, Epoch: 747, Loss: 0.6721, Train 0.7698, Val 0.8000\n",
      "Fold: 1, Epoch: 748, Loss: 0.6749, Train 0.7104, Val 0.7231\n",
      "Fold: 1, Epoch: 749, Loss: 0.6802, Train 0.6634, Val 0.6462\n",
      "Fold: 1, Epoch: 750, Loss: 0.6689, Train 0.6733, Val 0.6615\n",
      "Fold: 1, Epoch: 751, Loss: 0.7047, Train 0.6931, Val 0.6000\n",
      "Fold: 1, Epoch: 752, Loss: 0.5998, Train 0.8317, Val 0.7692\n",
      "Fold: 1, Epoch: 753, Loss: 0.6324, Train 0.8861, Val 0.8308\n",
      "Fold: 1, Epoch: 754, Loss: 0.6525, Train 0.8416, Val 0.8462\n",
      "Fold: 1, Epoch: 755, Loss: 0.5571, Train 0.7673, Val 0.8000\n",
      "Fold: 1, Epoch: 756, Loss: 0.6182, Train 0.7376, Val 0.8154\n",
      "Fold: 1, Epoch: 757, Loss: 0.6075, Train 0.7203, Val 0.8000\n",
      "Fold: 1, Epoch: 758, Loss: 0.6546, Train 0.6708, Val 0.6462\n",
      "Fold: 1, Epoch: 759, Loss: 0.5658, Train 0.6733, Val 0.6462\n",
      "Fold: 1, Epoch: 760, Loss: 0.5245, Train 0.6609, Val 0.6000\n",
      "Fold: 1, Epoch: 761, Loss: 0.6856, Train 0.5520, Val 0.4615\n",
      "Fold: 1, Epoch: 762, Loss: 0.6490, Train 0.6757, Val 0.6308\n",
      "Fold: 1, Epoch: 763, Loss: 0.5013, Train 0.8168, Val 0.7846\n",
      "Fold: 1, Epoch: 764, Loss: 0.6635, Train 0.8985, Val 0.8923\n",
      "Fold: 1, Epoch: 765, Loss: 0.5625, Train 0.8762, Val 0.9077\n",
      "Fold: 1, Epoch: 766, Loss: 0.4691, Train 0.8564, Val 0.9077\n",
      "Fold: 1, Epoch: 767, Loss: 0.5924, Train 0.8589, Val 0.9077\n",
      "Fold: 1, Epoch: 768, Loss: 0.5719, Train 0.8738, Val 0.8923\n",
      "Fold: 1, Epoch: 769, Loss: 0.6114, Train 0.8465, Val 0.8615\n",
      "Fold: 1, Epoch: 770, Loss: 0.5611, Train 0.8688, Val 0.8923\n",
      "Fold: 1, Epoch: 771, Loss: 0.5971, Train 0.8985, Val 0.8923\n",
      "Fold: 1, Epoch: 772, Loss: 0.4635, Train 0.8985, Val 0.8923\n",
      "Fold: 1, Epoch: 773, Loss: 0.6000, Train 0.9010, Val 0.9231\n",
      "Fold: 1, Epoch: 774, Loss: 0.5621, Train 0.8812, Val 0.9077\n",
      "Fold: 1, Epoch: 775, Loss: 0.4581, Train 0.8366, Val 0.8615\n",
      "Fold: 1, Epoch: 776, Loss: 0.5154, Train 0.8267, Val 0.8462\n",
      "Fold: 1, Epoch: 777, Loss: 0.5548, Train 0.8465, Val 0.8615\n",
      "Fold: 1, Epoch: 778, Loss: 0.5086, Train 0.8762, Val 0.8769\n",
      "Fold: 1, Epoch: 779, Loss: 0.5078, Train 0.8812, Val 0.8769\n",
      "Fold: 1, Epoch: 780, Loss: 0.6063, Train 0.9010, Val 0.8615\n",
      "Fold: 1, Epoch: 781, Loss: 0.4245, Train 0.9010, Val 0.8462\n",
      "Fold: 1, Epoch: 782, Loss: 0.4305, Train 0.9010, Val 0.8615\n",
      "Fold: 1, Epoch: 783, Loss: 0.4665, Train 0.9059, Val 0.8615\n",
      "Fold: 1, Epoch: 784, Loss: 0.5269, Train 0.9035, Val 0.8615\n",
      "Fold: 1, Epoch: 785, Loss: 0.5522, Train 0.8886, Val 0.8769\n",
      "Fold: 1, Epoch: 786, Loss: 0.4026, Train 0.8515, Val 0.8769\n",
      "Fold: 1, Epoch: 787, Loss: 0.5108, Train 0.8144, Val 0.8462\n",
      "Fold: 1, Epoch: 788, Loss: 0.6300, Train 0.8589, Val 0.9077\n",
      "Fold: 1, Epoch: 789, Loss: 0.5760, Train 0.8911, Val 0.9077\n",
      "Fold: 1, Epoch: 790, Loss: 0.5946, Train 0.8960, Val 0.8923\n",
      "Fold: 1, Epoch: 791, Loss: 0.5702, Train 0.8639, Val 0.8462\n",
      "Fold: 1, Epoch: 792, Loss: 0.5243, Train 0.7104, Val 0.6308\n",
      "Fold: 1, Epoch: 793, Loss: 0.4655, Train 0.6238, Val 0.5231\n",
      "Fold: 1, Epoch: 794, Loss: 0.4976, Train 0.6188, Val 0.5231\n",
      "Fold: 1, Epoch: 795, Loss: 0.6256, Train 0.6213, Val 0.5231\n",
      "Fold: 1, Epoch: 796, Loss: 0.6106, Train 0.7376, Val 0.6769\n",
      "Fold: 1, Epoch: 797, Loss: 0.5639, Train 0.7475, Val 0.7231\n",
      "Fold: 1, Epoch: 798, Loss: 0.4563, Train 0.7599, Val 0.7231\n",
      "Fold: 1, Epoch: 799, Loss: 0.5372, Train 0.7995, Val 0.7846\n",
      "Fold: 1, Epoch: 800, Loss: 0.5885, Train 0.8020, Val 0.8000\n",
      "Fold: 1, Epoch: 801, Loss: 0.4894, Train 0.8317, Val 0.8462\n",
      "Fold: 1, Epoch: 802, Loss: 0.5704, Train 0.8960, Val 0.8769\n",
      "Fold: 1, Epoch: 803, Loss: 0.5522, Train 0.9010, Val 0.8769\n",
      "Fold: 1, Epoch: 804, Loss: 0.5636, Train 0.9059, Val 0.8769\n",
      "Fold: 1, Epoch: 805, Loss: 0.4624, Train 0.9084, Val 0.8923\n",
      "Fold: 1, Epoch: 806, Loss: 0.6501, Train 0.8515, Val 0.8462\n",
      "Fold: 1, Epoch: 807, Loss: 0.4456, Train 0.7500, Val 0.7692\n",
      "Fold: 1, Epoch: 808, Loss: 0.4450, Train 0.7252, Val 0.7231\n",
      "Fold: 1, Epoch: 809, Loss: 0.4538, Train 0.7748, Val 0.7538\n",
      "Fold: 1, Epoch: 810, Loss: 0.5823, Train 0.8094, Val 0.7846\n",
      "Fold: 1, Epoch: 811, Loss: 0.5450, Train 0.8366, Val 0.8462\n",
      "Fold: 1, Epoch: 812, Loss: 0.4575, Train 0.8663, Val 0.9231\n",
      "Fold: 1, Epoch: 813, Loss: 0.6396, Train 0.9134, Val 0.8615\n",
      "Fold: 1, Epoch: 814, Loss: 0.4674, Train 0.7129, Val 0.6154\n",
      "Fold: 1, Epoch: 815, Loss: 0.4358, Train 0.6337, Val 0.5231\n",
      "Fold: 1, Epoch: 816, Loss: 0.4834, Train 0.6733, Val 0.5846\n",
      "Fold: 1, Epoch: 817, Loss: 0.5895, Train 0.8614, Val 0.8308\n",
      "Fold: 1, Epoch: 818, Loss: 0.4988, Train 0.8985, Val 0.8615\n",
      "Fold: 1, Epoch: 819, Loss: 0.4611, Train 0.8936, Val 0.8615\n",
      "Fold: 1, Epoch: 820, Loss: 0.4263, Train 0.8614, Val 0.8769\n",
      "Fold: 1, Epoch: 821, Loss: 0.4321, Train 0.8366, Val 0.8462\n",
      "Fold: 1, Epoch: 822, Loss: 0.4347, Train 0.8391, Val 0.8462\n",
      "Fold: 1, Epoch: 823, Loss: 0.4455, Train 0.8614, Val 0.8923\n",
      "Fold: 1, Epoch: 824, Loss: 0.4392, Train 0.9109, Val 0.8769\n",
      "Fold: 1, Epoch: 825, Loss: 0.4493, Train 0.9282, Val 0.8769\n",
      "Fold: 1, Epoch: 826, Loss: 0.4032, Train 0.9035, Val 0.8615\n",
      "Fold: 1, Epoch: 827, Loss: 0.3900, Train 0.8639, Val 0.8000\n",
      "Fold: 1, Epoch: 828, Loss: 0.4761, Train 0.7401, Val 0.6615\n",
      "Fold: 1, Epoch: 829, Loss: 0.5318, Train 0.8762, Val 0.7846\n",
      "Fold: 1, Epoch: 830, Loss: 0.4700, Train 0.9134, Val 0.8615\n",
      "Fold: 1, Epoch: 831, Loss: 0.4149, Train 0.9134, Val 0.8769\n",
      "Fold: 1, Epoch: 832, Loss: 0.4180, Train 0.8837, Val 0.8769\n",
      "Fold: 1, Epoch: 833, Loss: 0.5539, Train 0.8490, Val 0.8769\n",
      "Fold: 1, Epoch: 834, Loss: 0.5233, Train 0.8589, Val 0.8923\n",
      "Fold: 1, Epoch: 835, Loss: 0.4224, Train 0.9059, Val 0.8769\n",
      "Fold: 1, Epoch: 836, Loss: 0.3774, Train 0.9257, Val 0.8923\n",
      "Fold: 1, Epoch: 837, Loss: 0.4514, Train 0.9059, Val 0.8923\n",
      "Fold: 1, Epoch: 838, Loss: 0.4312, Train 0.8515, Val 0.8154\n",
      "Fold: 1, Epoch: 839, Loss: 0.4204, Train 0.7946, Val 0.7538\n",
      "Fold: 1, Epoch: 840, Loss: 0.4086, Train 0.7871, Val 0.7538\n",
      "Fold: 1, Epoch: 841, Loss: 0.4799, Train 0.8119, Val 0.7538\n",
      "Fold: 1, Epoch: 842, Loss: 0.4281, Train 0.7946, Val 0.7231\n",
      "Fold: 1, Epoch: 843, Loss: 0.4491, Train 0.8094, Val 0.7231\n",
      "Fold: 1, Epoch: 844, Loss: 0.3387, Train 0.7871, Val 0.7077\n",
      "Fold: 1, Epoch: 845, Loss: 0.4561, Train 0.7500, Val 0.7077\n",
      "Fold: 1, Epoch: 846, Loss: 0.4472, Train 0.8069, Val 0.7538\n",
      "Fold: 1, Epoch: 847, Loss: 0.5522, Train 0.9059, Val 0.8462\n",
      "Fold: 1, Epoch: 848, Loss: 0.4093, Train 0.9183, Val 0.9077\n",
      "Fold: 1, Epoch: 849, Loss: 0.4310, Train 0.9134, Val 0.9077\n",
      "Fold: 1, Epoch: 850, Loss: 0.4287, Train 0.9109, Val 0.9077\n",
      "Fold: 1, Epoch: 851, Loss: 0.4288, Train 0.9134, Val 0.9077\n",
      "Fold: 1, Epoch: 852, Loss: 0.4790, Train 0.9183, Val 0.8769\n",
      "Fold: 1, Epoch: 853, Loss: 0.4450, Train 0.9158, Val 0.8769\n",
      "Fold: 1, Epoch: 854, Loss: 0.5093, Train 0.9208, Val 0.8923\n",
      "Fold: 1, Epoch: 855, Loss: 0.3776, Train 0.9158, Val 0.8923\n",
      "Fold: 1, Epoch: 856, Loss: 0.4189, Train 0.9059, Val 0.8769\n",
      "Fold: 1, Epoch: 857, Loss: 0.4476, Train 0.8936, Val 0.8462\n",
      "Fold: 1, Epoch: 858, Loss: 0.4581, Train 0.8762, Val 0.8308\n",
      "Fold: 1, Epoch: 859, Loss: 0.4613, Train 0.8490, Val 0.8154\n",
      "Fold: 1, Epoch: 860, Loss: 0.3766, Train 0.8639, Val 0.8462\n",
      "Fold: 1, Epoch: 861, Loss: 0.3967, Train 0.8960, Val 0.8462\n",
      "Fold: 1, Epoch: 862, Loss: 0.3750, Train 0.9035, Val 0.8462\n",
      "Fold: 1, Epoch: 863, Loss: 0.3673, Train 0.9010, Val 0.8615\n",
      "Fold: 1, Epoch: 864, Loss: 0.4154, Train 0.8688, Val 0.8154\n",
      "Fold: 1, Epoch: 865, Loss: 0.4078, Train 0.8243, Val 0.7846\n",
      "Fold: 1, Epoch: 866, Loss: 0.3281, Train 0.7376, Val 0.6615\n",
      "Fold: 1, Epoch: 867, Loss: 0.4360, Train 0.7054, Val 0.5846\n",
      "Fold: 1, Epoch: 868, Loss: 0.3836, Train 0.6733, Val 0.5538\n",
      "Fold: 1, Epoch: 869, Loss: 0.3847, Train 0.6634, Val 0.5385\n",
      "Fold: 1, Epoch: 870, Loss: 0.4317, Train 0.6485, Val 0.5385\n",
      "Fold: 1, Epoch: 871, Loss: 0.4608, Train 0.6584, Val 0.5385\n",
      "Fold: 1, Epoch: 872, Loss: 0.4169, Train 0.7030, Val 0.5692\n",
      "Fold: 1, Epoch: 873, Loss: 0.4055, Train 0.7624, Val 0.6462\n",
      "Fold: 1, Epoch: 874, Loss: 0.3655, Train 0.8564, Val 0.8000\n",
      "Fold: 1, Epoch: 875, Loss: 0.4859, Train 0.9233, Val 0.8769\n",
      "Fold: 1, Epoch: 876, Loss: 0.3577, Train 0.9208, Val 0.8923\n",
      "Fold: 1, Epoch: 877, Loss: 0.3971, Train 0.8837, Val 0.8769\n",
      "Fold: 1, Epoch: 878, Loss: 0.4323, Train 0.8317, Val 0.8308\n",
      "Fold: 1, Epoch: 879, Loss: 0.3701, Train 0.8094, Val 0.8154\n",
      "Fold: 1, Epoch: 880, Loss: 0.4512, Train 0.8094, Val 0.8154\n",
      "Fold: 1, Epoch: 881, Loss: 0.4231, Train 0.8243, Val 0.8462\n",
      "Fold: 1, Epoch: 882, Loss: 0.4886, Train 0.8861, Val 0.9077\n",
      "Fold: 1, Epoch: 883, Loss: 0.4519, Train 0.9183, Val 0.9231\n",
      "Fold: 1, Epoch: 884, Loss: 0.3642, Train 0.8985, Val 0.9077\n",
      "Fold: 1, Epoch: 885, Loss: 0.3508, Train 0.8812, Val 0.8769\n",
      "Fold: 1, Epoch: 886, Loss: 0.3321, Train 0.8243, Val 0.8000\n",
      "Fold: 1, Epoch: 887, Loss: 0.4056, Train 0.7921, Val 0.7538\n",
      "Fold: 1, Epoch: 888, Loss: 0.4493, Train 0.8168, Val 0.7846\n",
      "Fold: 1, Epoch: 889, Loss: 0.3851, Train 0.8762, Val 0.8308\n",
      "Fold: 1, Epoch: 890, Loss: 0.4130, Train 0.9134, Val 0.8462\n",
      "Fold: 1, Epoch: 891, Loss: 0.4870, Train 0.9307, Val 0.8615\n",
      "Fold: 1, Epoch: 892, Loss: 0.3634, Train 0.9381, Val 0.8923\n",
      "Fold: 1, Epoch: 893, Loss: 0.3134, Train 0.9059, Val 0.8923\n",
      "Fold: 1, Epoch: 894, Loss: 0.3518, Train 0.8564, Val 0.8769\n",
      "Fold: 1, Epoch: 895, Loss: 0.4580, Train 0.8218, Val 0.8769\n",
      "Fold: 1, Epoch: 896, Loss: 0.4355, Train 0.8391, Val 0.8769\n",
      "Fold: 1, Epoch: 897, Loss: 0.3976, Train 0.8317, Val 0.8769\n",
      "Fold: 1, Epoch: 898, Loss: 0.4734, Train 0.8861, Val 0.9077\n",
      "Fold: 1, Epoch: 899, Loss: 0.3710, Train 0.9233, Val 0.9385\n",
      "Fold: 1, Epoch: 900, Loss: 0.4824, Train 0.9134, Val 0.8769\n",
      "Fold: 1, Epoch: 901, Loss: 0.3784, Train 0.9109, Val 0.8769\n",
      "Fold: 1, Epoch: 902, Loss: 0.4551, Train 0.9109, Val 0.8769\n",
      "Fold: 1, Epoch: 903, Loss: 0.3444, Train 0.9084, Val 0.8462\n",
      "Fold: 1, Epoch: 904, Loss: 0.3605, Train 0.8936, Val 0.8462\n",
      "Fold: 1, Epoch: 905, Loss: 0.3501, Train 0.8490, Val 0.8000\n",
      "Fold: 1, Epoch: 906, Loss: 0.3574, Train 0.8342, Val 0.7692\n",
      "Fold: 1, Epoch: 907, Loss: 0.3444, Train 0.8391, Val 0.8000\n",
      "Fold: 1, Epoch: 908, Loss: 0.3685, Train 0.8540, Val 0.8154\n",
      "Fold: 1, Epoch: 909, Loss: 0.4220, Train 0.8911, Val 0.8462\n",
      "Fold: 1, Epoch: 910, Loss: 0.3426, Train 0.9035, Val 0.8615\n",
      "Fold: 1, Epoch: 911, Loss: 0.4116, Train 0.9233, Val 0.8923\n",
      "Fold: 1, Epoch: 912, Loss: 0.4670, Train 0.9233, Val 0.8769\n",
      "Fold: 1, Epoch: 913, Loss: 0.4251, Train 0.9307, Val 0.8769\n",
      "Fold: 1, Epoch: 914, Loss: 0.3147, Train 0.9183, Val 0.8769\n",
      "Fold: 1, Epoch: 915, Loss: 0.3546, Train 0.9183, Val 0.8769\n",
      "Fold: 1, Epoch: 916, Loss: 0.4913, Train 0.9183, Val 0.8769\n",
      "Fold: 1, Epoch: 917, Loss: 0.4852, Train 0.9233, Val 0.8769\n",
      "Fold: 1, Epoch: 918, Loss: 0.4327, Train 0.8911, Val 0.8154\n",
      "Fold: 1, Epoch: 919, Loss: 0.4301, Train 0.7079, Val 0.5846\n",
      "Fold: 1, Epoch: 920, Loss: 0.3048, Train 0.5965, Val 0.4923\n",
      "Fold: 1, Epoch: 921, Loss: 0.3883, Train 0.5322, Val 0.4308\n",
      "Fold: 1, Epoch: 922, Loss: 0.4793, Train 0.5272, Val 0.4154\n",
      "Fold: 1, Epoch: 923, Loss: 0.3508, Train 0.5421, Val 0.4308\n",
      "Fold: 1, Epoch: 924, Loss: 0.4316, Train 0.6337, Val 0.5231\n",
      "Fold: 1, Epoch: 925, Loss: 0.4236, Train 0.8812, Val 0.8923\n",
      "Fold: 1, Epoch: 926, Loss: 0.4726, Train 0.9406, Val 0.9077\n",
      "Fold: 1, Epoch: 927, Loss: 0.3044, Train 0.9332, Val 0.8769\n",
      "Fold: 1, Epoch: 928, Loss: 0.3727, Train 0.9233, Val 0.8769\n",
      "Fold: 1, Epoch: 929, Loss: 0.3376, Train 0.8663, Val 0.8615\n",
      "Fold: 1, Epoch: 930, Loss: 0.3531, Train 0.8243, Val 0.8154\n",
      "Fold: 1, Epoch: 931, Loss: 0.3497, Train 0.8168, Val 0.8154\n",
      "Fold: 1, Epoch: 932, Loss: 0.3077, Train 0.8168, Val 0.8154\n",
      "Fold: 1, Epoch: 933, Loss: 0.3911, Train 0.8515, Val 0.8462\n",
      "Fold: 1, Epoch: 934, Loss: 0.4176, Train 0.9134, Val 0.8769\n",
      "Fold: 1, Epoch: 935, Loss: 0.3543, Train 0.9233, Val 0.9077\n",
      "Fold: 1, Epoch: 936, Loss: 0.4173, Train 0.9183, Val 0.9077\n",
      "Fold: 1, Epoch: 937, Loss: 0.3370, Train 0.9134, Val 0.8923\n",
      "Fold: 1, Epoch: 938, Loss: 0.4672, Train 0.9233, Val 0.9077\n",
      "Fold: 1, Epoch: 939, Loss: 0.4197, Train 0.9183, Val 0.8769\n",
      "Fold: 1, Epoch: 940, Loss: 0.3780, Train 0.8985, Val 0.8769\n",
      "Fold: 1, Epoch: 941, Loss: 0.4621, Train 0.8837, Val 0.8769\n",
      "Fold: 1, Epoch: 942, Loss: 0.3806, Train 0.9059, Val 0.9385\n",
      "Fold: 1, Epoch: 943, Loss: 0.4366, Train 0.8812, Val 0.9231\n",
      "Fold: 1, Epoch: 944, Loss: 0.4348, Train 0.8416, Val 0.8308\n",
      "Fold: 1, Epoch: 945, Loss: 0.3784, Train 0.8391, Val 0.8308\n",
      "Fold: 1, Epoch: 946, Loss: 0.3365, Train 0.7921, Val 0.8000\n",
      "Fold: 1, Epoch: 947, Loss: 0.3942, Train 0.7550, Val 0.7385\n",
      "Fold: 1, Epoch: 948, Loss: 0.4850, Train 0.7797, Val 0.7692\n",
      "Fold: 1, Epoch: 949, Loss: 0.4896, Train 0.8639, Val 0.8615\n",
      "Fold: 1, Epoch: 950, Loss: 0.3681, Train 0.9530, Val 0.9385\n",
      "Fold: 1, Epoch: 951, Loss: 0.3895, Train 0.9035, Val 0.8769\n",
      "Fold: 1, Epoch: 952, Loss: 0.3892, Train 0.8243, Val 0.8308\n",
      "Fold: 1, Epoch: 953, Loss: 0.3724, Train 0.8168, Val 0.8154\n",
      "Fold: 1, Epoch: 954, Loss: 0.3736, Train 0.8144, Val 0.8154\n",
      "Fold: 1, Epoch: 955, Loss: 0.4113, Train 0.8144, Val 0.8154\n",
      "Fold: 1, Epoch: 956, Loss: 0.3804, Train 0.8564, Val 0.8615\n",
      "Fold: 1, Epoch: 957, Loss: 0.3278, Train 0.8985, Val 0.8769\n",
      "Fold: 1, Epoch: 958, Loss: 0.4067, Train 0.9134, Val 0.8769\n",
      "Fold: 1, Epoch: 959, Loss: 0.2929, Train 0.9158, Val 0.8769\n",
      "Fold: 1, Epoch: 960, Loss: 0.3521, Train 0.9183, Val 0.8923\n",
      "Fold: 1, Epoch: 961, Loss: 0.3227, Train 0.9084, Val 0.8769\n",
      "Fold: 1, Epoch: 962, Loss: 0.3987, Train 0.9084, Val 0.8769\n",
      "Fold: 1, Epoch: 963, Loss: 0.3475, Train 0.8960, Val 0.8923\n",
      "Fold: 1, Epoch: 964, Loss: 0.3589, Train 0.8688, Val 0.8769\n",
      "Fold: 1, Epoch: 965, Loss: 0.3807, Train 0.8564, Val 0.8615\n",
      "Fold: 1, Epoch: 966, Loss: 0.3357, Train 0.8515, Val 0.8615\n",
      "Fold: 1, Epoch: 967, Loss: 0.4090, Train 0.8589, Val 0.8615\n",
      "Fold: 1, Epoch: 968, Loss: 0.3959, Train 0.8713, Val 0.8615\n",
      "Fold: 1, Epoch: 969, Loss: 0.4097, Train 0.9084, Val 0.8769\n",
      "Fold: 1, Epoch: 970, Loss: 0.3650, Train 0.9257, Val 0.8769\n",
      "Fold: 1, Epoch: 971, Loss: 0.3666, Train 0.9158, Val 0.8769\n",
      "Fold: 1, Epoch: 972, Loss: 0.3421, Train 0.8515, Val 0.7846\n",
      "Fold: 1, Epoch: 973, Loss: 0.2844, Train 0.7599, Val 0.7231\n",
      "Fold: 1, Epoch: 974, Loss: 0.3591, Train 0.8045, Val 0.7846\n",
      "Fold: 1, Epoch: 975, Loss: 0.2578, Train 0.8639, Val 0.8462\n",
      "Fold: 1, Epoch: 976, Loss: 0.2856, Train 0.9010, Val 0.8769\n",
      "Fold: 1, Epoch: 977, Loss: 0.2976, Train 0.9257, Val 0.9385\n",
      "Fold: 1, Epoch: 978, Loss: 0.4377, Train 0.9307, Val 0.9385\n",
      "Fold: 1, Epoch: 979, Loss: 0.4164, Train 0.9208, Val 0.9077\n",
      "Fold: 1, Epoch: 980, Loss: 0.3474, Train 0.9109, Val 0.8769\n",
      "Fold: 1, Epoch: 981, Loss: 0.4013, Train 0.9059, Val 0.8769\n",
      "Fold: 1, Epoch: 982, Loss: 0.3607, Train 0.8911, Val 0.8462\n",
      "Fold: 1, Epoch: 983, Loss: 0.3266, Train 0.8614, Val 0.8462\n",
      "Fold: 1, Epoch: 984, Loss: 0.4018, Train 0.8639, Val 0.8154\n",
      "Fold: 1, Epoch: 985, Loss: 0.3705, Train 0.8762, Val 0.8308\n",
      "Fold: 1, Epoch: 986, Loss: 0.3361, Train 0.9109, Val 0.9077\n",
      "Fold: 1, Epoch: 987, Loss: 0.3498, Train 0.9431, Val 0.9385\n",
      "Fold: 1, Epoch: 988, Loss: 0.4314, Train 0.9579, Val 0.9385\n",
      "Fold: 1, Epoch: 989, Loss: 0.3170, Train 0.9158, Val 0.9385\n",
      "Fold: 1, Epoch: 990, Loss: 0.3425, Train 0.8713, Val 0.8923\n",
      "Fold: 1, Epoch: 991, Loss: 0.3589, Train 0.8540, Val 0.8769\n",
      "Fold: 1, Epoch: 992, Loss: 0.3220, Train 0.8589, Val 0.8769\n",
      "Fold: 1, Epoch: 993, Loss: 0.2775, Train 0.8589, Val 0.8769\n",
      "Fold: 1, Epoch: 994, Loss: 0.4093, Train 0.8540, Val 0.8769\n",
      "Fold: 1, Epoch: 995, Loss: 0.4326, Train 0.8589, Val 0.8308\n",
      "Fold: 1, Epoch: 996, Loss: 0.3544, Train 0.8762, Val 0.8769\n",
      "Fold: 1, Epoch: 997, Loss: 0.2943, Train 0.9109, Val 0.8769\n",
      "Fold: 1, Epoch: 998, Loss: 0.3689, Train 0.9233, Val 0.8923\n",
      "Fold: 1, Epoch: 999, Loss: 0.3156, Train 0.9208, Val 0.9077\n",
      "Fold: 2, Epoch: 001, Loss: 2.7025, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 002, Loss: 5.0333, Train 0.3713, Val 0.3846\n",
      "Fold: 2, Epoch: 003, Loss: 3.0025, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 004, Loss: 4.0523, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 005, Loss: 3.8551, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 006, Loss: 3.9654, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 007, Loss: 2.7595, Train 0.3119, Val 0.3692\n",
      "Fold: 2, Epoch: 008, Loss: 2.3593, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 009, Loss: 2.5280, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 010, Loss: 2.8683, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 011, Loss: 2.7766, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 012, Loss: 3.1365, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 013, Loss: 2.9956, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 014, Loss: 3.0888, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 015, Loss: 2.5373, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 016, Loss: 2.3925, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 017, Loss: 2.4832, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 018, Loss: 2.5630, Train 0.3119, Val 0.3692\n",
      "Fold: 2, Epoch: 019, Loss: 2.5266, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 020, Loss: 2.2176, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 021, Loss: 2.3180, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 022, Loss: 2.4216, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 023, Loss: 2.4055, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 024, Loss: 2.4469, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 025, Loss: 2.3699, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 026, Loss: 2.4498, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 027, Loss: 2.4039, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 028, Loss: 2.6175, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 029, Loss: 2.2921, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 030, Loss: 2.2089, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 031, Loss: 2.2237, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 032, Loss: 2.3629, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 033, Loss: 2.2808, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 034, Loss: 2.2661, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 035, Loss: 2.2295, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 036, Loss: 2.2419, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 037, Loss: 2.3699, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 038, Loss: 2.2275, Train 0.3762, Val 0.4000\n",
      "Fold: 2, Epoch: 039, Loss: 2.2105, Train 0.1361, Val 0.1385\n",
      "Fold: 2, Epoch: 040, Loss: 2.2174, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 041, Loss: 2.1658, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 042, Loss: 2.2685, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 043, Loss: 2.1632, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 044, Loss: 2.1924, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 045, Loss: 2.2019, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 046, Loss: 2.2718, Train 0.3515, Val 0.3846\n",
      "Fold: 2, Epoch: 047, Loss: 2.1666, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 048, Loss: 2.2554, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 049, Loss: 2.1881, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 050, Loss: 2.1485, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 051, Loss: 2.1579, Train 0.1361, Val 0.1385\n",
      "Fold: 2, Epoch: 052, Loss: 2.2033, Train 0.1361, Val 0.1385\n",
      "Fold: 2, Epoch: 053, Loss: 2.2037, Train 0.1361, Val 0.1385\n",
      "Fold: 2, Epoch: 054, Loss: 2.1993, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 055, Loss: 2.1826, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 056, Loss: 2.1707, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 057, Loss: 2.1733, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 058, Loss: 2.2259, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 059, Loss: 2.2726, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 060, Loss: 2.1928, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 061, Loss: 2.1687, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 062, Loss: 2.1633, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 063, Loss: 2.2429, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 064, Loss: 2.2252, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 065, Loss: 2.2029, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 066, Loss: 2.1630, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 067, Loss: 2.1321, Train 0.1312, Val 0.1385\n",
      "Fold: 2, Epoch: 068, Loss: 2.1904, Train 0.1361, Val 0.1385\n",
      "Fold: 2, Epoch: 069, Loss: 2.2163, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 070, Loss: 2.1536, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 071, Loss: 2.1873, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 072, Loss: 2.2598, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 073, Loss: 2.1797, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 074, Loss: 2.1565, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 075, Loss: 2.1940, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 076, Loss: 2.1587, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 077, Loss: 2.1363, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 078, Loss: 2.1494, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 079, Loss: 2.1800, Train 0.3540, Val 0.3692\n",
      "Fold: 2, Epoch: 080, Loss: 2.1717, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 081, Loss: 2.1536, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 082, Loss: 2.1829, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 083, Loss: 2.1455, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 084, Loss: 2.1628, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 085, Loss: 2.1948, Train 0.3762, Val 0.4000\n",
      "Fold: 2, Epoch: 086, Loss: 2.1284, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 087, Loss: 2.1273, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 088, Loss: 2.1958, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 089, Loss: 2.1666, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 090, Loss: 2.1672, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 091, Loss: 2.1517, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 092, Loss: 2.1437, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 093, Loss: 2.1516, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 094, Loss: 2.1346, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 095, Loss: 2.1510, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 096, Loss: 2.1767, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 097, Loss: 2.1590, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 098, Loss: 2.1237, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 099, Loss: 2.1848, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 100, Loss: 2.1421, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 101, Loss: 2.1495, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 102, Loss: 2.2174, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 103, Loss: 2.1326, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 104, Loss: 2.1860, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 105, Loss: 2.1995, Train 0.3762, Val 0.4000\n",
      "Fold: 2, Epoch: 106, Loss: 2.1691, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 107, Loss: 2.1649, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 108, Loss: 2.1535, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 109, Loss: 2.1420, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 110, Loss: 2.1398, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 111, Loss: 2.1734, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 112, Loss: 2.2057, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 113, Loss: 2.1598, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 114, Loss: 2.1339, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 115, Loss: 2.1844, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 116, Loss: 2.2161, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 117, Loss: 2.1766, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 118, Loss: 2.2308, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 119, Loss: 2.1883, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 120, Loss: 2.1546, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 121, Loss: 2.1598, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 122, Loss: 2.1384, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 123, Loss: 2.1501, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 124, Loss: 2.1574, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 125, Loss: 2.1860, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 126, Loss: 2.1487, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 127, Loss: 2.2146, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 128, Loss: 2.1946, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 129, Loss: 2.1524, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 130, Loss: 2.1209, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 131, Loss: 2.1104, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 132, Loss: 2.1443, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 133, Loss: 2.1229, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 134, Loss: 2.1390, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 135, Loss: 2.1339, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 136, Loss: 2.1434, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 137, Loss: 2.1196, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 138, Loss: 2.1458, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 139, Loss: 2.1274, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 140, Loss: 2.1407, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 141, Loss: 2.1563, Train 0.3762, Val 0.4000\n",
      "Fold: 2, Epoch: 142, Loss: 2.1381, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 143, Loss: 2.1606, Train 0.3762, Val 0.4000\n",
      "Fold: 2, Epoch: 144, Loss: 2.1531, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 145, Loss: 2.1549, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 146, Loss: 2.1693, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 147, Loss: 2.1512, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 148, Loss: 2.1222, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 149, Loss: 2.1287, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 150, Loss: 2.1148, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 151, Loss: 2.1724, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 152, Loss: 2.1408, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 153, Loss: 2.1361, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 154, Loss: 2.1332, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 155, Loss: 2.1334, Train 0.3762, Val 0.4000\n",
      "Fold: 2, Epoch: 156, Loss: 2.1632, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 157, Loss: 2.1433, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 158, Loss: 2.1357, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 159, Loss: 2.1988, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 160, Loss: 2.1526, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 161, Loss: 2.1840, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 162, Loss: 2.2337, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 163, Loss: 2.1433, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 164, Loss: 2.1691, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 165, Loss: 2.1933, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 166, Loss: 2.1624, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 167, Loss: 2.1637, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 168, Loss: 2.1460, Train 0.3762, Val 0.4000\n",
      "Fold: 2, Epoch: 169, Loss: 2.1194, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 170, Loss: 2.1455, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 171, Loss: 2.1397, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 172, Loss: 2.1687, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 173, Loss: 2.1329, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 174, Loss: 2.1585, Train 0.3144, Val 0.3692\n",
      "Fold: 2, Epoch: 175, Loss: 2.2736, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 176, Loss: 2.1334, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 177, Loss: 2.1689, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 178, Loss: 2.2232, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 179, Loss: 2.1480, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 180, Loss: 2.1331, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 181, Loss: 2.1515, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 182, Loss: 2.2464, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 183, Loss: 2.1313, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 184, Loss: 2.1796, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 185, Loss: 2.2444, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 186, Loss: 2.1273, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 187, Loss: 2.1457, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 188, Loss: 2.1697, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 189, Loss: 2.1629, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 190, Loss: 2.1597, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 191, Loss: 2.1427, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 192, Loss: 2.2136, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 193, Loss: 2.1636, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 194, Loss: 2.1528, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 195, Loss: 2.1524, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 196, Loss: 2.1590, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 197, Loss: 2.1455, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 198, Loss: 2.1323, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 199, Loss: 2.1334, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 200, Loss: 2.1265, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 201, Loss: 2.1465, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 202, Loss: 2.1631, Train 0.3168, Val 0.3692\n",
      "Fold: 2, Epoch: 203, Loss: 2.1602, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 204, Loss: 2.1233, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 205, Loss: 2.1527, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 206, Loss: 2.1594, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 207, Loss: 2.1251, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 208, Loss: 2.1514, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 209, Loss: 2.1792, Train 0.3787, Val 0.4000\n",
      "Fold: 2, Epoch: 210, Loss: 2.1433, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 211, Loss: 2.1384, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 212, Loss: 2.1860, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 213, Loss: 2.1372, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 214, Loss: 2.1617, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 215, Loss: 2.1271, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 216, Loss: 2.1438, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 217, Loss: 2.1479, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 218, Loss: 2.1319, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 219, Loss: 2.1282, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 220, Loss: 2.1578, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 221, Loss: 2.1572, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 222, Loss: 2.1107, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 223, Loss: 2.1678, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 224, Loss: 2.1385, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 225, Loss: 2.1269, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 226, Loss: 2.1262, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 227, Loss: 2.1422, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 228, Loss: 2.1526, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 229, Loss: 2.1368, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 230, Loss: 2.1454, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 231, Loss: 2.1093, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 232, Loss: 2.1401, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 233, Loss: 2.1282, Train 0.4183, Val 0.3692\n",
      "Fold: 2, Epoch: 234, Loss: 2.1207, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 235, Loss: 2.1517, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 236, Loss: 2.1144, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 237, Loss: 2.1086, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 238, Loss: 2.1231, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 239, Loss: 2.1187, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 240, Loss: 2.1226, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 241, Loss: 2.1295, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 242, Loss: 2.1286, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 243, Loss: 2.1083, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 244, Loss: 2.1667, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 245, Loss: 2.1221, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 246, Loss: 2.1539, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 247, Loss: 2.1246, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 248, Loss: 2.1439, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 249, Loss: 2.1428, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 250, Loss: 2.1305, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 251, Loss: 2.1174, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 252, Loss: 2.1285, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 253, Loss: 2.1281, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 254, Loss: 2.1234, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 255, Loss: 2.1389, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 256, Loss: 2.1270, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 257, Loss: 2.1508, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 258, Loss: 2.0953, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 259, Loss: 2.1266, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 260, Loss: 2.1481, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 261, Loss: 2.1373, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 262, Loss: 2.1354, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 263, Loss: 2.1329, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 264, Loss: 2.1073, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 265, Loss: 2.1370, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 266, Loss: 2.1187, Train 0.3342, Val 0.3692\n",
      "Fold: 2, Epoch: 267, Loss: 2.1158, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 268, Loss: 2.1217, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 269, Loss: 2.1260, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 270, Loss: 2.1260, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 271, Loss: 2.1165, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 272, Loss: 2.1536, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 273, Loss: 2.1424, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 274, Loss: 2.1292, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 275, Loss: 2.1120, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 276, Loss: 2.1388, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 277, Loss: 2.1160, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 278, Loss: 2.1081, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 279, Loss: 2.1473, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 280, Loss: 2.1218, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 281, Loss: 2.1277, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 282, Loss: 2.1039, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 283, Loss: 2.1074, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 284, Loss: 2.1106, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 285, Loss: 2.1287, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 286, Loss: 2.1219, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 287, Loss: 2.1058, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 288, Loss: 2.1382, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 289, Loss: 2.0977, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 290, Loss: 2.1709, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 291, Loss: 2.1263, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 292, Loss: 2.1224, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 293, Loss: 2.1287, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 294, Loss: 2.1284, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 295, Loss: 2.1228, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 296, Loss: 2.1272, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 297, Loss: 2.1210, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 298, Loss: 2.1110, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 299, Loss: 2.1145, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 300, Loss: 2.1210, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 301, Loss: 2.1099, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 302, Loss: 2.1218, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 303, Loss: 2.1224, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 304, Loss: 2.1113, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 305, Loss: 2.0933, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 306, Loss: 2.1279, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 307, Loss: 2.1236, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 308, Loss: 2.1145, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 309, Loss: 2.1133, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 310, Loss: 2.1479, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 311, Loss: 2.1218, Train 0.3688, Val 0.3692\n",
      "Fold: 2, Epoch: 312, Loss: 2.1318, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 313, Loss: 2.1152, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 314, Loss: 2.1051, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 315, Loss: 2.1047, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 316, Loss: 2.0983, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 317, Loss: 2.1086, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 318, Loss: 2.0900, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 319, Loss: 2.1096, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 320, Loss: 2.0972, Train 0.3243, Val 0.3692\n",
      "Fold: 2, Epoch: 321, Loss: 2.1577, Train 0.3837, Val 0.4000\n",
      "Fold: 2, Epoch: 322, Loss: 2.0962, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 323, Loss: 2.1074, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 324, Loss: 2.1246, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 325, Loss: 2.0901, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 326, Loss: 2.0955, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 327, Loss: 2.1076, Train 0.3713, Val 0.3692\n",
      "Fold: 2, Epoch: 328, Loss: 2.1291, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 329, Loss: 2.1030, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 330, Loss: 2.1090, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 331, Loss: 2.1641, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 332, Loss: 2.1361, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 333, Loss: 2.1081, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 334, Loss: 2.1389, Train 0.3837, Val 0.4000\n",
      "Fold: 2, Epoch: 335, Loss: 2.0963, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 336, Loss: 2.0848, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 337, Loss: 2.1410, Train 0.3812, Val 0.3846\n",
      "Fold: 2, Epoch: 338, Loss: 2.0794, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 339, Loss: 2.1468, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 340, Loss: 2.0839, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 341, Loss: 2.0912, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 342, Loss: 2.1417, Train 0.3837, Val 0.4000\n",
      "Fold: 2, Epoch: 343, Loss: 2.0841, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 344, Loss: 2.1373, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 345, Loss: 2.0594, Train 0.3540, Val 0.3692\n",
      "Fold: 2, Epoch: 346, Loss: 2.0927, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 347, Loss: 2.0615, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 348, Loss: 2.0787, Train 0.3985, Val 0.4000\n",
      "Fold: 2, Epoch: 349, Loss: 2.0459, Train 0.3812, Val 0.4000\n",
      "Fold: 2, Epoch: 350, Loss: 2.1088, Train 0.3540, Val 0.3692\n",
      "Fold: 2, Epoch: 351, Loss: 2.0608, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 352, Loss: 2.0284, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 353, Loss: 2.0564, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 354, Loss: 2.0346, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 355, Loss: 2.0464, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 356, Loss: 2.0065, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 357, Loss: 2.0664, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 358, Loss: 1.9781, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 359, Loss: 1.9755, Train 0.3342, Val 0.3692\n",
      "Fold: 2, Epoch: 360, Loss: 1.9929, Train 0.3342, Val 0.3692\n",
      "Fold: 2, Epoch: 361, Loss: 1.9651, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 362, Loss: 1.9598, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 363, Loss: 1.9681, Train 0.3614, Val 0.3846\n",
      "Fold: 2, Epoch: 364, Loss: 1.9615, Train 0.3391, Val 0.3692\n",
      "Fold: 2, Epoch: 365, Loss: 1.9093, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 366, Loss: 1.8480, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 367, Loss: 1.9081, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 368, Loss: 1.8535, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 369, Loss: 1.8733, Train 0.3045, Val 0.3077\n",
      "Fold: 2, Epoch: 370, Loss: 1.9279, Train 0.3193, Val 0.3692\n",
      "Fold: 2, Epoch: 371, Loss: 1.8472, Train 0.3317, Val 0.3692\n",
      "Fold: 2, Epoch: 372, Loss: 1.8284, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 373, Loss: 1.7827, Train 0.3292, Val 0.3692\n",
      "Fold: 2, Epoch: 374, Loss: 1.7170, Train 0.3317, Val 0.3692\n",
      "Fold: 2, Epoch: 375, Loss: 1.6578, Train 0.6485, Val 0.6923\n",
      "Fold: 2, Epoch: 376, Loss: 1.6217, Train 0.3366, Val 0.3692\n",
      "Fold: 2, Epoch: 377, Loss: 1.6056, Train 0.3515, Val 0.3692\n",
      "Fold: 2, Epoch: 378, Loss: 1.5537, Train 0.3416, Val 0.3692\n",
      "Fold: 2, Epoch: 379, Loss: 1.4929, Train 0.3267, Val 0.3692\n",
      "Fold: 2, Epoch: 380, Loss: 1.5501, Train 0.5223, Val 0.5385\n",
      "Fold: 2, Epoch: 381, Loss: 1.4588, Train 0.6906, Val 0.7385\n",
      "Fold: 2, Epoch: 382, Loss: 1.5803, Train 0.6782, Val 0.7538\n",
      "Fold: 2, Epoch: 383, Loss: 1.3583, Train 0.3218, Val 0.3692\n",
      "Fold: 2, Epoch: 384, Loss: 1.7362, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 385, Loss: 1.3459, Train 0.5149, Val 0.5385\n",
      "Fold: 2, Epoch: 386, Loss: 1.6905, Train 0.5842, Val 0.6462\n",
      "Fold: 2, Epoch: 387, Loss: 1.3619, Train 0.6733, Val 0.7385\n",
      "Fold: 2, Epoch: 388, Loss: 1.5872, Train 0.6955, Val 0.7538\n",
      "Fold: 2, Epoch: 389, Loss: 1.4322, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 390, Loss: 1.4672, Train 0.5941, Val 0.6615\n",
      "Fold: 2, Epoch: 391, Loss: 1.5540, Train 0.7030, Val 0.7692\n",
      "Fold: 2, Epoch: 392, Loss: 1.3135, Train 0.7178, Val 0.7692\n",
      "Fold: 2, Epoch: 393, Loss: 1.3955, Train 0.7153, Val 0.7692\n",
      "Fold: 2, Epoch: 394, Loss: 1.4248, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 395, Loss: 1.3147, Train 0.5173, Val 0.5385\n",
      "Fold: 2, Epoch: 396, Loss: 1.4051, Train 0.5173, Val 0.5385\n",
      "Fold: 2, Epoch: 397, Loss: 1.3179, Train 0.5495, Val 0.5692\n",
      "Fold: 2, Epoch: 398, Loss: 1.2964, Train 0.7030, Val 0.7385\n",
      "Fold: 2, Epoch: 399, Loss: 1.3548, Train 0.5198, Val 0.5077\n",
      "Fold: 2, Epoch: 400, Loss: 1.3012, Train 0.4257, Val 0.4615\n",
      "Fold: 2, Epoch: 401, Loss: 1.3296, Train 0.4233, Val 0.4308\n",
      "Fold: 2, Epoch: 402, Loss: 1.2605, Train 0.2723, Val 0.2308\n",
      "Fold: 2, Epoch: 403, Loss: 1.3455, Train 0.5149, Val 0.5231\n",
      "Fold: 2, Epoch: 404, Loss: 1.3144, Train 0.5149, Val 0.5385\n",
      "Fold: 2, Epoch: 405, Loss: 1.3721, Train 0.5792, Val 0.6462\n",
      "Fold: 2, Epoch: 406, Loss: 1.4904, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 407, Loss: 1.2839, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 408, Loss: 1.3206, Train 0.7030, Val 0.7538\n",
      "Fold: 2, Epoch: 409, Loss: 1.3817, Train 0.4282, Val 0.4462\n",
      "Fold: 2, Epoch: 410, Loss: 1.3354, Train 0.3416, Val 0.2615\n",
      "Fold: 2, Epoch: 411, Loss: 1.2697, Train 0.5173, Val 0.5385\n",
      "Fold: 2, Epoch: 412, Loss: 1.2427, Train 0.6856, Val 0.7385\n",
      "Fold: 2, Epoch: 413, Loss: 1.3993, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 414, Loss: 1.2661, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 415, Loss: 1.2944, Train 0.7005, Val 0.7538\n",
      "Fold: 2, Epoch: 416, Loss: 1.2908, Train 0.5371, Val 0.5231\n",
      "Fold: 2, Epoch: 417, Loss: 1.2339, Train 0.4282, Val 0.4615\n",
      "Fold: 2, Epoch: 418, Loss: 1.2587, Train 0.2574, Val 0.2308\n",
      "Fold: 2, Epoch: 419, Loss: 1.2978, Train 0.5297, Val 0.5231\n",
      "Fold: 2, Epoch: 420, Loss: 1.2582, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 421, Loss: 1.1893, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 422, Loss: 1.2742, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 423, Loss: 1.3709, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 424, Loss: 1.3688, Train 0.7030, Val 0.7538\n",
      "Fold: 2, Epoch: 425, Loss: 1.2273, Train 0.3168, Val 0.2615\n",
      "Fold: 2, Epoch: 426, Loss: 1.4385, Train 0.5644, Val 0.5231\n",
      "Fold: 2, Epoch: 427, Loss: 1.2571, Train 0.7104, Val 0.7538\n",
      "Fold: 2, Epoch: 428, Loss: 1.2196, Train 0.7005, Val 0.7538\n",
      "Fold: 2, Epoch: 429, Loss: 1.2493, Train 0.6955, Val 0.7538\n",
      "Fold: 2, Epoch: 430, Loss: 1.2478, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 431, Loss: 1.3218, Train 0.7005, Val 0.7692\n",
      "Fold: 2, Epoch: 432, Loss: 1.2143, Train 0.7153, Val 0.7538\n",
      "Fold: 2, Epoch: 433, Loss: 1.2093, Train 0.7203, Val 0.8000\n",
      "Fold: 2, Epoch: 434, Loss: 1.2068, Train 0.7129, Val 0.7692\n",
      "Fold: 2, Epoch: 435, Loss: 1.3645, Train 0.7005, Val 0.7692\n",
      "Fold: 2, Epoch: 436, Loss: 1.2064, Train 0.6955, Val 0.7538\n",
      "Fold: 2, Epoch: 437, Loss: 1.2881, Train 0.6955, Val 0.7538\n",
      "Fold: 2, Epoch: 438, Loss: 1.2765, Train 0.6980, Val 0.7692\n",
      "Fold: 2, Epoch: 439, Loss: 1.1810, Train 0.7475, Val 0.8000\n",
      "Fold: 2, Epoch: 440, Loss: 1.1851, Train 0.4431, Val 0.4615\n",
      "Fold: 2, Epoch: 441, Loss: 1.1784, Train 0.4876, Val 0.4769\n",
      "Fold: 2, Epoch: 442, Loss: 1.1221, Train 0.4431, Val 0.4308\n",
      "Fold: 2, Epoch: 443, Loss: 1.2071, Train 0.5792, Val 0.5538\n",
      "Fold: 2, Epoch: 444, Loss: 1.2664, Train 0.7079, Val 0.7538\n",
      "Fold: 2, Epoch: 445, Loss: 1.3147, Train 0.5446, Val 0.5538\n",
      "Fold: 2, Epoch: 446, Loss: 1.2421, Train 0.6931, Val 0.7538\n",
      "Fold: 2, Epoch: 447, Loss: 1.1991, Train 0.7104, Val 0.7692\n",
      "Fold: 2, Epoch: 448, Loss: 1.1982, Train 0.7178, Val 0.8000\n",
      "Fold: 2, Epoch: 449, Loss: 1.1912, Train 0.7203, Val 0.8000\n",
      "Fold: 2, Epoch: 450, Loss: 1.1860, Train 0.7450, Val 0.8000\n",
      "Fold: 2, Epoch: 451, Loss: 1.1722, Train 0.7550, Val 0.8154\n",
      "Fold: 2, Epoch: 452, Loss: 1.1661, Train 0.5248, Val 0.4615\n",
      "Fold: 2, Epoch: 453, Loss: 1.1724, Train 0.5198, Val 0.4923\n",
      "Fold: 2, Epoch: 454, Loss: 1.2510, Train 0.7376, Val 0.8000\n",
      "Fold: 2, Epoch: 455, Loss: 1.1294, Train 0.6906, Val 0.7385\n",
      "Fold: 2, Epoch: 456, Loss: 1.1946, Train 0.6757, Val 0.7385\n",
      "Fold: 2, Epoch: 457, Loss: 1.2463, Train 0.6955, Val 0.7385\n",
      "Fold: 2, Epoch: 458, Loss: 1.1701, Train 0.7450, Val 0.8308\n",
      "Fold: 2, Epoch: 459, Loss: 1.1475, Train 0.5396, Val 0.5385\n",
      "Fold: 2, Epoch: 460, Loss: 1.3099, Train 0.5644, Val 0.6154\n",
      "Fold: 2, Epoch: 461, Loss: 1.4009, Train 0.4678, Val 0.4462\n",
      "Fold: 2, Epoch: 462, Loss: 1.1027, Train 0.7079, Val 0.7231\n",
      "Fold: 2, Epoch: 463, Loss: 1.1565, Train 0.6906, Val 0.7077\n",
      "Fold: 2, Epoch: 464, Loss: 1.2246, Train 0.7054, Val 0.7385\n",
      "Fold: 2, Epoch: 465, Loss: 1.1614, Train 0.7327, Val 0.8462\n",
      "Fold: 2, Epoch: 466, Loss: 1.0860, Train 0.7104, Val 0.7692\n",
      "Fold: 2, Epoch: 467, Loss: 1.1154, Train 0.5272, Val 0.5385\n",
      "Fold: 2, Epoch: 468, Loss: 1.1562, Train 0.5470, Val 0.5538\n",
      "Fold: 2, Epoch: 469, Loss: 1.2245, Train 0.7723, Val 0.8308\n",
      "Fold: 2, Epoch: 470, Loss: 1.2690, Train 0.7772, Val 0.8308\n",
      "Fold: 2, Epoch: 471, Loss: 1.1888, Train 0.7649, Val 0.8000\n",
      "Fold: 2, Epoch: 472, Loss: 1.1230, Train 0.7574, Val 0.7538\n",
      "Fold: 2, Epoch: 473, Loss: 1.1446, Train 0.7723, Val 0.8154\n",
      "Fold: 2, Epoch: 474, Loss: 1.1676, Train 0.7723, Val 0.8308\n",
      "Fold: 2, Epoch: 475, Loss: 1.2067, Train 0.7327, Val 0.7692\n",
      "Fold: 2, Epoch: 476, Loss: 1.0868, Train 0.6955, Val 0.7385\n",
      "Fold: 2, Epoch: 477, Loss: 1.1277, Train 0.6856, Val 0.7385\n",
      "Fold: 2, Epoch: 478, Loss: 1.0807, Train 0.7054, Val 0.7385\n",
      "Fold: 2, Epoch: 479, Loss: 1.0726, Train 0.7673, Val 0.8308\n",
      "Fold: 2, Epoch: 480, Loss: 1.0142, Train 0.7673, Val 0.7846\n",
      "Fold: 2, Epoch: 481, Loss: 1.0317, Train 0.5743, Val 0.5692\n",
      "Fold: 2, Epoch: 482, Loss: 1.0756, Train 0.4901, Val 0.4769\n",
      "Fold: 2, Epoch: 483, Loss: 1.0482, Train 0.5124, Val 0.4923\n",
      "Fold: 2, Epoch: 484, Loss: 1.0459, Train 0.7847, Val 0.8308\n",
      "Fold: 2, Epoch: 485, Loss: 1.0405, Train 0.7450, Val 0.7692\n",
      "Fold: 2, Epoch: 486, Loss: 1.0526, Train 0.7030, Val 0.7385\n",
      "Fold: 2, Epoch: 487, Loss: 1.0121, Train 0.6931, Val 0.7385\n",
      "Fold: 2, Epoch: 488, Loss: 1.0036, Train 0.7252, Val 0.7692\n",
      "Fold: 2, Epoch: 489, Loss: 1.0413, Train 0.7624, Val 0.7846\n",
      "Fold: 2, Epoch: 490, Loss: 1.0597, Train 0.4851, Val 0.4615\n",
      "Fold: 2, Epoch: 491, Loss: 1.0001, Train 0.4777, Val 0.4615\n",
      "Fold: 2, Epoch: 492, Loss: 1.0782, Train 0.4876, Val 0.4615\n",
      "Fold: 2, Epoch: 493, Loss: 1.0728, Train 0.7797, Val 0.8000\n",
      "Fold: 2, Epoch: 494, Loss: 1.0117, Train 0.7054, Val 0.7385\n",
      "Fold: 2, Epoch: 495, Loss: 1.0674, Train 0.6807, Val 0.7385\n",
      "Fold: 2, Epoch: 496, Loss: 1.2413, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 497, Loss: 1.0278, Train 0.7797, Val 0.8462\n",
      "Fold: 2, Epoch: 498, Loss: 0.9710, Train 0.4926, Val 0.4923\n",
      "Fold: 2, Epoch: 499, Loss: 1.2294, Train 0.4975, Val 0.4769\n",
      "Fold: 2, Epoch: 500, Loss: 1.0868, Train 0.5025, Val 0.4769\n",
      "Fold: 2, Epoch: 501, Loss: 0.9881, Train 0.7153, Val 0.7385\n",
      "Fold: 2, Epoch: 502, Loss: 0.9767, Train 0.7673, Val 0.8308\n",
      "Fold: 2, Epoch: 503, Loss: 0.9985, Train 0.7599, Val 0.8308\n",
      "Fold: 2, Epoch: 504, Loss: 1.0514, Train 0.7748, Val 0.8308\n",
      "Fold: 2, Epoch: 505, Loss: 1.1071, Train 0.7673, Val 0.7846\n",
      "Fold: 2, Epoch: 506, Loss: 1.1297, Train 0.7426, Val 0.7692\n",
      "Fold: 2, Epoch: 507, Loss: 1.0387, Train 0.7871, Val 0.8308\n",
      "Fold: 2, Epoch: 508, Loss: 0.9083, Train 0.7723, Val 0.8462\n",
      "Fold: 2, Epoch: 509, Loss: 1.0495, Train 0.7450, Val 0.7846\n",
      "Fold: 2, Epoch: 510, Loss: 1.0907, Train 0.7748, Val 0.8462\n",
      "Fold: 2, Epoch: 511, Loss: 1.0565, Train 0.7847, Val 0.8462\n",
      "Fold: 2, Epoch: 512, Loss: 0.9544, Train 0.7649, Val 0.8308\n",
      "Fold: 2, Epoch: 513, Loss: 1.0519, Train 0.7178, Val 0.7538\n",
      "Fold: 2, Epoch: 514, Loss: 0.9993, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 515, Loss: 1.0103, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 516, Loss: 0.9463, Train 0.7153, Val 0.7538\n",
      "Fold: 2, Epoch: 517, Loss: 0.9941, Train 0.7450, Val 0.7846\n",
      "Fold: 2, Epoch: 518, Loss: 0.9901, Train 0.7624, Val 0.8154\n",
      "Fold: 2, Epoch: 519, Loss: 0.9098, Train 0.7599, Val 0.8154\n",
      "Fold: 2, Epoch: 520, Loss: 0.9402, Train 0.7450, Val 0.7846\n",
      "Fold: 2, Epoch: 521, Loss: 0.9253, Train 0.7178, Val 0.7538\n",
      "Fold: 2, Epoch: 522, Loss: 0.8573, Train 0.7178, Val 0.7538\n",
      "Fold: 2, Epoch: 523, Loss: 1.0380, Train 0.7723, Val 0.8462\n",
      "Fold: 2, Epoch: 524, Loss: 0.9601, Train 0.7970, Val 0.8462\n",
      "Fold: 2, Epoch: 525, Loss: 0.9586, Train 0.7970, Val 0.8308\n",
      "Fold: 2, Epoch: 526, Loss: 1.0659, Train 0.7946, Val 0.8308\n",
      "Fold: 2, Epoch: 527, Loss: 0.9536, Train 0.7946, Val 0.8308\n",
      "Fold: 2, Epoch: 528, Loss: 0.8904, Train 0.7847, Val 0.8308\n",
      "Fold: 2, Epoch: 529, Loss: 0.9264, Train 0.7426, Val 0.7692\n",
      "Fold: 2, Epoch: 530, Loss: 0.9510, Train 0.7129, Val 0.7538\n",
      "Fold: 2, Epoch: 531, Loss: 0.9594, Train 0.7698, Val 0.8308\n",
      "Fold: 2, Epoch: 532, Loss: 0.9811, Train 0.7921, Val 0.8154\n",
      "Fold: 2, Epoch: 533, Loss: 1.0450, Train 0.7921, Val 0.8154\n",
      "Fold: 2, Epoch: 534, Loss: 1.0542, Train 0.7748, Val 0.8462\n",
      "Fold: 2, Epoch: 535, Loss: 1.0255, Train 0.6955, Val 0.7538\n",
      "Fold: 2, Epoch: 536, Loss: 0.9500, Train 0.6782, Val 0.7385\n",
      "Fold: 2, Epoch: 537, Loss: 0.9684, Train 0.6757, Val 0.7385\n",
      "Fold: 2, Epoch: 538, Loss: 0.9515, Train 0.6782, Val 0.7385\n",
      "Fold: 2, Epoch: 539, Loss: 1.0511, Train 0.7129, Val 0.7692\n",
      "Fold: 2, Epoch: 540, Loss: 0.9300, Train 0.7698, Val 0.8154\n",
      "Fold: 2, Epoch: 541, Loss: 0.9442, Train 0.7054, Val 0.7692\n",
      "Fold: 2, Epoch: 542, Loss: 1.1033, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 543, Loss: 0.9203, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 544, Loss: 0.9713, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 545, Loss: 0.9019, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 546, Loss: 0.9179, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 547, Loss: 0.9638, Train 0.7203, Val 0.7692\n",
      "Fold: 2, Epoch: 548, Loss: 0.9428, Train 0.7772, Val 0.8000\n",
      "Fold: 2, Epoch: 549, Loss: 0.9359, Train 0.7871, Val 0.8154\n",
      "Fold: 2, Epoch: 550, Loss: 1.0305, Train 0.7228, Val 0.7692\n",
      "Fold: 2, Epoch: 551, Loss: 0.8990, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 552, Loss: 0.9556, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 553, Loss: 0.9339, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 554, Loss: 0.9553, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 555, Loss: 0.9139, Train 0.7525, Val 0.7846\n",
      "Fold: 2, Epoch: 556, Loss: 0.9432, Train 0.7748, Val 0.8000\n",
      "Fold: 2, Epoch: 557, Loss: 0.9482, Train 0.7896, Val 0.8154\n",
      "Fold: 2, Epoch: 558, Loss: 0.9757, Train 0.7772, Val 0.8462\n",
      "Fold: 2, Epoch: 559, Loss: 0.8705, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 560, Loss: 0.9179, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 561, Loss: 0.9603, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 562, Loss: 0.9066, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 563, Loss: 0.8525, Train 0.7030, Val 0.7692\n",
      "Fold: 2, Epoch: 564, Loss: 0.8858, Train 0.7228, Val 0.7231\n",
      "Fold: 2, Epoch: 565, Loss: 0.9577, Train 0.6559, Val 0.6462\n",
      "Fold: 2, Epoch: 566, Loss: 0.9674, Train 0.7252, Val 0.7385\n",
      "Fold: 2, Epoch: 567, Loss: 0.9727, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 568, Loss: 0.8509, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 569, Loss: 0.8856, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 570, Loss: 0.8634, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 571, Loss: 0.8426, Train 0.7327, Val 0.7846\n",
      "Fold: 2, Epoch: 572, Loss: 0.9298, Train 0.7599, Val 0.8154\n",
      "Fold: 2, Epoch: 573, Loss: 0.9039, Train 0.7153, Val 0.7846\n",
      "Fold: 2, Epoch: 574, Loss: 0.8150, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 575, Loss: 0.9430, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 576, Loss: 0.7922, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 577, Loss: 0.8560, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 578, Loss: 0.9663, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 579, Loss: 0.9091, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 580, Loss: 0.9124, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 581, Loss: 0.8949, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 582, Loss: 0.9180, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 583, Loss: 0.8950, Train 0.7079, Val 0.7538\n",
      "Fold: 2, Epoch: 584, Loss: 0.8653, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 585, Loss: 0.9154, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 586, Loss: 0.8530, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 587, Loss: 0.9013, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 588, Loss: 0.9260, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 589, Loss: 0.9610, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 590, Loss: 0.9095, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 591, Loss: 0.8482, Train 0.7450, Val 0.7846\n",
      "Fold: 2, Epoch: 592, Loss: 1.0114, Train 0.7525, Val 0.7846\n",
      "Fold: 2, Epoch: 593, Loss: 0.8458, Train 0.7426, Val 0.8000\n",
      "Fold: 2, Epoch: 594, Loss: 0.9162, Train 0.7450, Val 0.8154\n",
      "Fold: 2, Epoch: 595, Loss: 0.9177, Train 0.7104, Val 0.7846\n",
      "Fold: 2, Epoch: 596, Loss: 0.8479, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 597, Loss: 0.9037, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 598, Loss: 0.8936, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 599, Loss: 0.9259, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 600, Loss: 0.9096, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 601, Loss: 1.0027, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 602, Loss: 0.9241, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 603, Loss: 0.8085, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 604, Loss: 0.8482, Train 0.6955, Val 0.7538\n",
      "Fold: 2, Epoch: 605, Loss: 0.8360, Train 0.6931, Val 0.7538\n",
      "Fold: 2, Epoch: 606, Loss: 0.9322, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 607, Loss: 0.9039, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 608, Loss: 0.8343, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 609, Loss: 1.0563, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 610, Loss: 0.8411, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 611, Loss: 0.8620, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 612, Loss: 0.8939, Train 0.6807, Val 0.7538\n",
      "Fold: 2, Epoch: 613, Loss: 0.9997, Train 0.6782, Val 0.7385\n",
      "Fold: 2, Epoch: 614, Loss: 1.0395, Train 0.6832, Val 0.7538\n",
      "Fold: 2, Epoch: 615, Loss: 0.9240, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 616, Loss: 0.8961, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 617, Loss: 0.8433, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 618, Loss: 0.9272, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 619, Loss: 0.8446, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 620, Loss: 0.9014, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 621, Loss: 0.8340, Train 0.7351, Val 0.7846\n",
      "Fold: 2, Epoch: 622, Loss: 0.9299, Train 0.7129, Val 0.7692\n",
      "Fold: 2, Epoch: 623, Loss: 0.8930, Train 0.6832, Val 0.7538\n",
      "Fold: 2, Epoch: 624, Loss: 0.9910, Train 0.6832, Val 0.7538\n",
      "Fold: 2, Epoch: 625, Loss: 0.8509, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 626, Loss: 0.8700, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 627, Loss: 0.8967, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 628, Loss: 0.9028, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 629, Loss: 0.9012, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 630, Loss: 0.7826, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 631, Loss: 0.8455, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 632, Loss: 0.8452, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 633, Loss: 0.8691, Train 0.6807, Val 0.7538\n",
      "Fold: 2, Epoch: 634, Loss: 0.9089, Train 0.6807, Val 0.7538\n",
      "Fold: 2, Epoch: 635, Loss: 0.8673, Train 0.6832, Val 0.7538\n",
      "Fold: 2, Epoch: 636, Loss: 0.8696, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 637, Loss: 0.9219, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 638, Loss: 0.8634, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 639, Loss: 0.8976, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 640, Loss: 0.8483, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 641, Loss: 0.8209, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 642, Loss: 0.7620, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 643, Loss: 0.9500, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 644, Loss: 0.8629, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 645, Loss: 0.7810, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 646, Loss: 0.8210, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 647, Loss: 0.8945, Train 0.7054, Val 0.7692\n",
      "Fold: 2, Epoch: 648, Loss: 0.9666, Train 0.7327, Val 0.7692\n",
      "Fold: 2, Epoch: 649, Loss: 0.9110, Train 0.7475, Val 0.7846\n",
      "Fold: 2, Epoch: 650, Loss: 0.8945, Train 0.5743, Val 0.5538\n",
      "Fold: 2, Epoch: 651, Loss: 0.8883, Train 0.4653, Val 0.4154\n",
      "Fold: 2, Epoch: 652, Loss: 0.8333, Train 0.4035, Val 0.4000\n",
      "Fold: 2, Epoch: 653, Loss: 0.8748, Train 0.5248, Val 0.4769\n",
      "Fold: 2, Epoch: 654, Loss: 0.8661, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 655, Loss: 0.8324, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 656, Loss: 0.8399, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 657, Loss: 0.8540, Train 0.7054, Val 0.7692\n",
      "Fold: 2, Epoch: 658, Loss: 0.8342, Train 0.7104, Val 0.7692\n",
      "Fold: 2, Epoch: 659, Loss: 0.8467, Train 0.4480, Val 0.4000\n",
      "Fold: 2, Epoch: 660, Loss: 0.8331, Train 0.6064, Val 0.6154\n",
      "Fold: 2, Epoch: 661, Loss: 0.8433, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 662, Loss: 0.7877, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 663, Loss: 0.8539, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 664, Loss: 0.8197, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 665, Loss: 0.8220, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 666, Loss: 0.8530, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 667, Loss: 0.7765, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 668, Loss: 0.8655, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 669, Loss: 0.8379, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 670, Loss: 0.8979, Train 0.6807, Val 0.7385\n",
      "Fold: 2, Epoch: 671, Loss: 0.9995, Train 0.6807, Val 0.7385\n",
      "Fold: 2, Epoch: 672, Loss: 0.9566, Train 0.6807, Val 0.7385\n",
      "Fold: 2, Epoch: 673, Loss: 0.8668, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 674, Loss: 0.8315, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 675, Loss: 0.8895, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 676, Loss: 0.8390, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 677, Loss: 0.9481, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 678, Loss: 0.8313, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 679, Loss: 0.8430, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 680, Loss: 0.9860, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 681, Loss: 0.7987, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 682, Loss: 0.8166, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 683, Loss: 0.8104, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 684, Loss: 0.8523, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 685, Loss: 0.9058, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 686, Loss: 0.9260, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 687, Loss: 0.9069, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 688, Loss: 0.8288, Train 0.6708, Val 0.7077\n",
      "Fold: 2, Epoch: 689, Loss: 0.9095, Train 0.5668, Val 0.5538\n",
      "Fold: 2, Epoch: 690, Loss: 0.8524, Train 0.6287, Val 0.5846\n",
      "Fold: 2, Epoch: 691, Loss: 0.9999, Train 0.7005, Val 0.7692\n",
      "Fold: 2, Epoch: 692, Loss: 0.8249, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 693, Loss: 0.7735, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 694, Loss: 0.8825, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 695, Loss: 0.8093, Train 0.7030, Val 0.7692\n",
      "Fold: 2, Epoch: 696, Loss: 0.8389, Train 0.7079, Val 0.7692\n",
      "Fold: 2, Epoch: 697, Loss: 0.8472, Train 0.6955, Val 0.7538\n",
      "Fold: 2, Epoch: 698, Loss: 0.8427, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 699, Loss: 0.8004, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 700, Loss: 0.7808, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 701, Loss: 0.8747, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 702, Loss: 0.8584, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 703, Loss: 0.8539, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 704, Loss: 0.7722, Train 0.6906, Val 0.7692\n",
      "Fold: 2, Epoch: 705, Loss: 0.8206, Train 0.6980, Val 0.7692\n",
      "Fold: 2, Epoch: 706, Loss: 0.8473, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 707, Loss: 0.8580, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 708, Loss: 0.7882, Train 0.6856, Val 0.7538\n",
      "Fold: 2, Epoch: 709, Loss: 0.8130, Train 0.6881, Val 0.7538\n",
      "Fold: 2, Epoch: 710, Loss: 0.8826, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 711, Loss: 0.7695, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 712, Loss: 0.8047, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 713, Loss: 0.8099, Train 0.7079, Val 0.7846\n",
      "Fold: 2, Epoch: 714, Loss: 0.7482, Train 0.4752, Val 0.4462\n",
      "Fold: 2, Epoch: 715, Loss: 0.8158, Train 0.7129, Val 0.7846\n",
      "Fold: 2, Epoch: 716, Loss: 0.8291, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 717, Loss: 0.8472, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 718, Loss: 0.7389, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 719, Loss: 0.8242, Train 0.6931, Val 0.7692\n",
      "Fold: 2, Epoch: 720, Loss: 0.7693, Train 0.6906, Val 0.7538\n",
      "Fold: 2, Epoch: 721, Loss: 0.7272, Train 0.7054, Val 0.7692\n",
      "Fold: 2, Epoch: 722, Loss: 0.8326, Train 0.7178, Val 0.7692\n",
      "Fold: 2, Epoch: 723, Loss: 0.7240, Train 0.7871, Val 0.8462\n",
      "Fold: 2, Epoch: 724, Loss: 0.7128, Train 0.7896, Val 0.8462\n",
      "Fold: 2, Epoch: 725, Loss: 0.7261, Train 0.7847, Val 0.8000\n",
      "Fold: 2, Epoch: 726, Loss: 0.7770, Train 0.6559, Val 0.6308\n",
      "Fold: 2, Epoch: 727, Loss: 0.7418, Train 0.5495, Val 0.5538\n",
      "Fold: 2, Epoch: 728, Loss: 0.8148, Train 0.4554, Val 0.4462\n",
      "Fold: 2, Epoch: 729, Loss: 0.7558, Train 0.4505, Val 0.4154\n",
      "Fold: 2, Epoch: 730, Loss: 0.7691, Train 0.4381, Val 0.4154\n",
      "Fold: 2, Epoch: 731, Loss: 0.7681, Train 0.4653, Val 0.4308\n",
      "Fold: 2, Epoch: 732, Loss: 0.7713, Train 0.5223, Val 0.4615\n",
      "Fold: 2, Epoch: 733, Loss: 0.7626, Train 0.7574, Val 0.8308\n",
      "Fold: 2, Epoch: 734, Loss: 0.7364, Train 0.7475, Val 0.8154\n",
      "Fold: 2, Epoch: 735, Loss: 0.7847, Train 0.7079, Val 0.7846\n",
      "Fold: 2, Epoch: 736, Loss: 0.7566, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 737, Loss: 0.7367, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 738, Loss: 0.7388, Train 0.7129, Val 0.8000\n",
      "Fold: 2, Epoch: 739, Loss: 0.7575, Train 0.7228, Val 0.8308\n",
      "Fold: 2, Epoch: 740, Loss: 0.6390, Train 0.6955, Val 0.8308\n",
      "Fold: 2, Epoch: 741, Loss: 0.7159, Train 0.6559, Val 0.7692\n",
      "Fold: 2, Epoch: 742, Loss: 0.6948, Train 0.7426, Val 0.8308\n",
      "Fold: 2, Epoch: 743, Loss: 0.7215, Train 0.6955, Val 0.7538\n",
      "Fold: 2, Epoch: 744, Loss: 0.6847, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 745, Loss: 0.6953, Train 0.6955, Val 0.7692\n",
      "Fold: 2, Epoch: 746, Loss: 0.6486, Train 0.7104, Val 0.7846\n",
      "Fold: 2, Epoch: 747, Loss: 0.6721, Train 0.7698, Val 0.8769\n",
      "Fold: 2, Epoch: 748, Loss: 0.6749, Train 0.7104, Val 0.8308\n",
      "Fold: 2, Epoch: 749, Loss: 0.6802, Train 0.6634, Val 0.7077\n",
      "Fold: 2, Epoch: 750, Loss: 0.6689, Train 0.6733, Val 0.7077\n",
      "Fold: 2, Epoch: 751, Loss: 0.7047, Train 0.6931, Val 0.6923\n",
      "Fold: 2, Epoch: 752, Loss: 0.5998, Train 0.8317, Val 0.8923\n",
      "Fold: 2, Epoch: 753, Loss: 0.6324, Train 0.8861, Val 0.9385\n",
      "Fold: 2, Epoch: 754, Loss: 0.6525, Train 0.8416, Val 0.8769\n",
      "Fold: 2, Epoch: 755, Loss: 0.5571, Train 0.7673, Val 0.8154\n",
      "Fold: 2, Epoch: 756, Loss: 0.6182, Train 0.7376, Val 0.7692\n",
      "Fold: 2, Epoch: 757, Loss: 0.6075, Train 0.7203, Val 0.7538\n",
      "Fold: 2, Epoch: 758, Loss: 0.6546, Train 0.6708, Val 0.6462\n",
      "Fold: 2, Epoch: 759, Loss: 0.5658, Train 0.6733, Val 0.6462\n",
      "Fold: 2, Epoch: 760, Loss: 0.5245, Train 0.6609, Val 0.6308\n",
      "Fold: 2, Epoch: 761, Loss: 0.6856, Train 0.5520, Val 0.5077\n",
      "Fold: 2, Epoch: 762, Loss: 0.6490, Train 0.6757, Val 0.6923\n",
      "Fold: 2, Epoch: 763, Loss: 0.5013, Train 0.8168, Val 0.8000\n",
      "Fold: 2, Epoch: 764, Loss: 0.6635, Train 0.8985, Val 0.9385\n",
      "Fold: 2, Epoch: 765, Loss: 0.5625, Train 0.8762, Val 0.9385\n",
      "Fold: 2, Epoch: 766, Loss: 0.4691, Train 0.8564, Val 0.9385\n",
      "Fold: 2, Epoch: 767, Loss: 0.5924, Train 0.8589, Val 0.9385\n",
      "Fold: 2, Epoch: 768, Loss: 0.5719, Train 0.8738, Val 0.9385\n",
      "Fold: 2, Epoch: 769, Loss: 0.6114, Train 0.8465, Val 0.8923\n",
      "Fold: 2, Epoch: 770, Loss: 0.5611, Train 0.8688, Val 0.9077\n",
      "Fold: 2, Epoch: 771, Loss: 0.5971, Train 0.8985, Val 0.9385\n",
      "Fold: 2, Epoch: 772, Loss: 0.4635, Train 0.8985, Val 0.9385\n",
      "Fold: 2, Epoch: 773, Loss: 0.6000, Train 0.9010, Val 0.9385\n",
      "Fold: 2, Epoch: 774, Loss: 0.5621, Train 0.8812, Val 0.9385\n",
      "Fold: 2, Epoch: 775, Loss: 0.4581, Train 0.8366, Val 0.8769\n",
      "Fold: 2, Epoch: 776, Loss: 0.5154, Train 0.8267, Val 0.8923\n",
      "Fold: 2, Epoch: 777, Loss: 0.5548, Train 0.8465, Val 0.9077\n",
      "Fold: 2, Epoch: 778, Loss: 0.5086, Train 0.8762, Val 0.9385\n",
      "Fold: 2, Epoch: 779, Loss: 0.5078, Train 0.8812, Val 0.9231\n",
      "Fold: 2, Epoch: 780, Loss: 0.6063, Train 0.9010, Val 0.9231\n",
      "Fold: 2, Epoch: 781, Loss: 0.4245, Train 0.9010, Val 0.9077\n",
      "Fold: 2, Epoch: 782, Loss: 0.4305, Train 0.9010, Val 0.8923\n",
      "Fold: 2, Epoch: 783, Loss: 0.4665, Train 0.9059, Val 0.9077\n",
      "Fold: 2, Epoch: 784, Loss: 0.5269, Train 0.9035, Val 0.8923\n",
      "Fold: 2, Epoch: 785, Loss: 0.5522, Train 0.8886, Val 0.8769\n",
      "Fold: 2, Epoch: 786, Loss: 0.4026, Train 0.8515, Val 0.8615\n",
      "Fold: 2, Epoch: 787, Loss: 0.5108, Train 0.8144, Val 0.8615\n",
      "Fold: 2, Epoch: 788, Loss: 0.6300, Train 0.8589, Val 0.9077\n",
      "Fold: 2, Epoch: 789, Loss: 0.5760, Train 0.8911, Val 0.9538\n",
      "Fold: 2, Epoch: 790, Loss: 0.5946, Train 0.8960, Val 0.9385\n",
      "Fold: 2, Epoch: 791, Loss: 0.5702, Train 0.8639, Val 0.8769\n",
      "Fold: 2, Epoch: 792, Loss: 0.5243, Train 0.7104, Val 0.7231\n",
      "Fold: 2, Epoch: 793, Loss: 0.4655, Train 0.6238, Val 0.6308\n",
      "Fold: 2, Epoch: 794, Loss: 0.4976, Train 0.6188, Val 0.6308\n",
      "Fold: 2, Epoch: 795, Loss: 0.6256, Train 0.6213, Val 0.6308\n",
      "Fold: 2, Epoch: 796, Loss: 0.6106, Train 0.7376, Val 0.7538\n",
      "Fold: 2, Epoch: 797, Loss: 0.5639, Train 0.7475, Val 0.7846\n",
      "Fold: 2, Epoch: 798, Loss: 0.4563, Train 0.7599, Val 0.7692\n",
      "Fold: 2, Epoch: 799, Loss: 0.5372, Train 0.7995, Val 0.8615\n",
      "Fold: 2, Epoch: 800, Loss: 0.5885, Train 0.8020, Val 0.8462\n",
      "Fold: 2, Epoch: 801, Loss: 0.4894, Train 0.8317, Val 0.8923\n",
      "Fold: 2, Epoch: 802, Loss: 0.5704, Train 0.8960, Val 0.9692\n",
      "Fold: 2, Epoch: 803, Loss: 0.5522, Train 0.9010, Val 0.9231\n",
      "Fold: 2, Epoch: 804, Loss: 0.5636, Train 0.9059, Val 0.9077\n",
      "Fold: 2, Epoch: 805, Loss: 0.4624, Train 0.9084, Val 0.8923\n",
      "Fold: 2, Epoch: 806, Loss: 0.6501, Train 0.8515, Val 0.8923\n",
      "Fold: 2, Epoch: 807, Loss: 0.4456, Train 0.7500, Val 0.7846\n",
      "Fold: 2, Epoch: 808, Loss: 0.4450, Train 0.7252, Val 0.6923\n",
      "Fold: 2, Epoch: 809, Loss: 0.4538, Train 0.7748, Val 0.7385\n",
      "Fold: 2, Epoch: 810, Loss: 0.5823, Train 0.8094, Val 0.8462\n",
      "Fold: 2, Epoch: 811, Loss: 0.5450, Train 0.8366, Val 0.8769\n",
      "Fold: 2, Epoch: 812, Loss: 0.4575, Train 0.8663, Val 0.8769\n",
      "Fold: 2, Epoch: 813, Loss: 0.6396, Train 0.9134, Val 0.8923\n",
      "Fold: 2, Epoch: 814, Loss: 0.4674, Train 0.7129, Val 0.6923\n",
      "Fold: 2, Epoch: 815, Loss: 0.4358, Train 0.6337, Val 0.6462\n",
      "Fold: 2, Epoch: 816, Loss: 0.4834, Train 0.6733, Val 0.6769\n",
      "Fold: 2, Epoch: 817, Loss: 0.5895, Train 0.8614, Val 0.8923\n",
      "Fold: 2, Epoch: 818, Loss: 0.4988, Train 0.8985, Val 0.9385\n",
      "Fold: 2, Epoch: 819, Loss: 0.4611, Train 0.8936, Val 0.9538\n",
      "Fold: 2, Epoch: 820, Loss: 0.4263, Train 0.8614, Val 0.9231\n",
      "Fold: 2, Epoch: 821, Loss: 0.4321, Train 0.8366, Val 0.8923\n",
      "Fold: 2, Epoch: 822, Loss: 0.4347, Train 0.8391, Val 0.8769\n",
      "Fold: 2, Epoch: 823, Loss: 0.4455, Train 0.8614, Val 0.8923\n",
      "Fold: 2, Epoch: 824, Loss: 0.4392, Train 0.9109, Val 0.9385\n",
      "Fold: 2, Epoch: 825, Loss: 0.4493, Train 0.9282, Val 0.9385\n",
      "Fold: 2, Epoch: 826, Loss: 0.4032, Train 0.9035, Val 0.9231\n",
      "Fold: 2, Epoch: 827, Loss: 0.3900, Train 0.8639, Val 0.8923\n",
      "Fold: 2, Epoch: 828, Loss: 0.4761, Train 0.7401, Val 0.7538\n",
      "Fold: 2, Epoch: 829, Loss: 0.5318, Train 0.8762, Val 0.8923\n",
      "Fold: 2, Epoch: 830, Loss: 0.4700, Train 0.9134, Val 0.9385\n",
      "Fold: 2, Epoch: 831, Loss: 0.4149, Train 0.9134, Val 0.9385\n",
      "Fold: 2, Epoch: 832, Loss: 0.4180, Train 0.8837, Val 0.9385\n",
      "Fold: 2, Epoch: 833, Loss: 0.5539, Train 0.8490, Val 0.9231\n",
      "Fold: 2, Epoch: 834, Loss: 0.5233, Train 0.8589, Val 0.9385\n",
      "Fold: 2, Epoch: 835, Loss: 0.4224, Train 0.9059, Val 0.9692\n",
      "Fold: 2, Epoch: 836, Loss: 0.3774, Train 0.9257, Val 0.9385\n",
      "Fold: 2, Epoch: 837, Loss: 0.4514, Train 0.9059, Val 0.9231\n",
      "Fold: 2, Epoch: 838, Loss: 0.4312, Train 0.8515, Val 0.8615\n",
      "Fold: 2, Epoch: 839, Loss: 0.4204, Train 0.7946, Val 0.8154\n",
      "Fold: 2, Epoch: 840, Loss: 0.4086, Train 0.7871, Val 0.8000\n",
      "Fold: 2, Epoch: 841, Loss: 0.4799, Train 0.8119, Val 0.7385\n",
      "Fold: 2, Epoch: 842, Loss: 0.4281, Train 0.7946, Val 0.7385\n",
      "Fold: 2, Epoch: 843, Loss: 0.4491, Train 0.8094, Val 0.8000\n",
      "Fold: 2, Epoch: 844, Loss: 0.3387, Train 0.7871, Val 0.7692\n",
      "Fold: 2, Epoch: 845, Loss: 0.4561, Train 0.7500, Val 0.7385\n",
      "Fold: 2, Epoch: 846, Loss: 0.4472, Train 0.8069, Val 0.8154\n",
      "Fold: 2, Epoch: 847, Loss: 0.5522, Train 0.9059, Val 0.9385\n",
      "Fold: 2, Epoch: 848, Loss: 0.4093, Train 0.9183, Val 0.9692\n",
      "Fold: 2, Epoch: 849, Loss: 0.4310, Train 0.9134, Val 0.9538\n",
      "Fold: 2, Epoch: 850, Loss: 0.4287, Train 0.9109, Val 0.9538\n",
      "Fold: 2, Epoch: 851, Loss: 0.4288, Train 0.9134, Val 0.9538\n",
      "Fold: 2, Epoch: 852, Loss: 0.4790, Train 0.9183, Val 0.9385\n",
      "Fold: 2, Epoch: 853, Loss: 0.4450, Train 0.9158, Val 0.9385\n",
      "Fold: 2, Epoch: 854, Loss: 0.5093, Train 0.9208, Val 0.9385\n",
      "Fold: 2, Epoch: 855, Loss: 0.3776, Train 0.9158, Val 0.9231\n",
      "Fold: 2, Epoch: 856, Loss: 0.4189, Train 0.9059, Val 0.9231\n",
      "Fold: 2, Epoch: 857, Loss: 0.4476, Train 0.8936, Val 0.9231\n",
      "Fold: 2, Epoch: 858, Loss: 0.4581, Train 0.8762, Val 0.9077\n",
      "Fold: 2, Epoch: 859, Loss: 0.4613, Train 0.8490, Val 0.9077\n",
      "Fold: 2, Epoch: 860, Loss: 0.3766, Train 0.8639, Val 0.9077\n",
      "Fold: 2, Epoch: 861, Loss: 0.3967, Train 0.8960, Val 0.9385\n",
      "Fold: 2, Epoch: 862, Loss: 0.3750, Train 0.9035, Val 0.9231\n",
      "Fold: 2, Epoch: 863, Loss: 0.3673, Train 0.9010, Val 0.9231\n",
      "Fold: 2, Epoch: 864, Loss: 0.4154, Train 0.8688, Val 0.8769\n",
      "Fold: 2, Epoch: 865, Loss: 0.4078, Train 0.8243, Val 0.8462\n",
      "Fold: 2, Epoch: 866, Loss: 0.3281, Train 0.7376, Val 0.7077\n",
      "Fold: 2, Epoch: 867, Loss: 0.4360, Train 0.7054, Val 0.6462\n",
      "Fold: 2, Epoch: 868, Loss: 0.3836, Train 0.6733, Val 0.6154\n",
      "Fold: 2, Epoch: 869, Loss: 0.3847, Train 0.6634, Val 0.6154\n",
      "Fold: 2, Epoch: 870, Loss: 0.4317, Train 0.6485, Val 0.6308\n",
      "Fold: 2, Epoch: 871, Loss: 0.4608, Train 0.6584, Val 0.6308\n",
      "Fold: 2, Epoch: 872, Loss: 0.4169, Train 0.7030, Val 0.6615\n",
      "Fold: 2, Epoch: 873, Loss: 0.4055, Train 0.7624, Val 0.7692\n",
      "Fold: 2, Epoch: 874, Loss: 0.3655, Train 0.8564, Val 0.8462\n",
      "Fold: 2, Epoch: 875, Loss: 0.4859, Train 0.9233, Val 0.9385\n",
      "Fold: 2, Epoch: 876, Loss: 0.3577, Train 0.9208, Val 0.9538\n",
      "Fold: 2, Epoch: 877, Loss: 0.3971, Train 0.8837, Val 0.9538\n",
      "Fold: 2, Epoch: 878, Loss: 0.4323, Train 0.8317, Val 0.9077\n",
      "Fold: 2, Epoch: 879, Loss: 0.3701, Train 0.8094, Val 0.8769\n",
      "Fold: 2, Epoch: 880, Loss: 0.4512, Train 0.8094, Val 0.8769\n",
      "Fold: 2, Epoch: 881, Loss: 0.4231, Train 0.8243, Val 0.8923\n",
      "Fold: 2, Epoch: 882, Loss: 0.4886, Train 0.8861, Val 0.9846\n",
      "Fold: 2, Epoch: 883, Loss: 0.4519, Train 0.9183, Val 0.9846\n",
      "Fold: 2, Epoch: 884, Loss: 0.3642, Train 0.8985, Val 0.9231\n",
      "Fold: 2, Epoch: 885, Loss: 0.3508, Train 0.8812, Val 0.9077\n",
      "Fold: 2, Epoch: 886, Loss: 0.3321, Train 0.8243, Val 0.8462\n",
      "Fold: 2, Epoch: 887, Loss: 0.4056, Train 0.7921, Val 0.8154\n",
      "Fold: 2, Epoch: 888, Loss: 0.4493, Train 0.8168, Val 0.8154\n",
      "Fold: 2, Epoch: 889, Loss: 0.3851, Train 0.8762, Val 0.9077\n",
      "Fold: 2, Epoch: 890, Loss: 0.4130, Train 0.9134, Val 0.9385\n",
      "Fold: 2, Epoch: 891, Loss: 0.4870, Train 0.9307, Val 0.9231\n",
      "Fold: 2, Epoch: 892, Loss: 0.3634, Train 0.9381, Val 0.9385\n",
      "Fold: 2, Epoch: 893, Loss: 0.3134, Train 0.9059, Val 0.9231\n",
      "Fold: 2, Epoch: 894, Loss: 0.3518, Train 0.8564, Val 0.8769\n",
      "Fold: 2, Epoch: 895, Loss: 0.4580, Train 0.8218, Val 0.8462\n",
      "Fold: 2, Epoch: 896, Loss: 0.4355, Train 0.8391, Val 0.8615\n",
      "Fold: 2, Epoch: 897, Loss: 0.3976, Train 0.8317, Val 0.8615\n",
      "Fold: 2, Epoch: 898, Loss: 0.4734, Train 0.8861, Val 0.8615\n",
      "Fold: 2, Epoch: 899, Loss: 0.3710, Train 0.9233, Val 0.9077\n",
      "Fold: 2, Epoch: 900, Loss: 0.4824, Train 0.9134, Val 0.9231\n",
      "Fold: 2, Epoch: 901, Loss: 0.3784, Train 0.9109, Val 0.9385\n",
      "Fold: 2, Epoch: 902, Loss: 0.4551, Train 0.9109, Val 0.9385\n",
      "Fold: 2, Epoch: 903, Loss: 0.3444, Train 0.9084, Val 0.9385\n",
      "Fold: 2, Epoch: 904, Loss: 0.3605, Train 0.8936, Val 0.9538\n",
      "Fold: 2, Epoch: 905, Loss: 0.3501, Train 0.8490, Val 0.8769\n",
      "Fold: 2, Epoch: 906, Loss: 0.3574, Train 0.8342, Val 0.8462\n",
      "Fold: 2, Epoch: 907, Loss: 0.3444, Train 0.8391, Val 0.8615\n",
      "Fold: 2, Epoch: 908, Loss: 0.3685, Train 0.8540, Val 0.8769\n",
      "Fold: 2, Epoch: 909, Loss: 0.4220, Train 0.8911, Val 0.9385\n",
      "Fold: 2, Epoch: 910, Loss: 0.3426, Train 0.9035, Val 0.9538\n",
      "Fold: 2, Epoch: 911, Loss: 0.4116, Train 0.9233, Val 0.9538\n",
      "Fold: 2, Epoch: 912, Loss: 0.4670, Train 0.9233, Val 0.9692\n",
      "Fold: 2, Epoch: 913, Loss: 0.4251, Train 0.9307, Val 0.9846\n",
      "Fold: 2, Epoch: 914, Loss: 0.3147, Train 0.9183, Val 0.9692\n",
      "Fold: 2, Epoch: 915, Loss: 0.3546, Train 0.9183, Val 0.9692\n",
      "Fold: 2, Epoch: 916, Loss: 0.4913, Train 0.9183, Val 0.9692\n",
      "Fold: 2, Epoch: 917, Loss: 0.4852, Train 0.9233, Val 0.9385\n",
      "Fold: 2, Epoch: 918, Loss: 0.4327, Train 0.8911, Val 0.9077\n",
      "Fold: 2, Epoch: 919, Loss: 0.4301, Train 0.7079, Val 0.6462\n",
      "Fold: 2, Epoch: 920, Loss: 0.3048, Train 0.5965, Val 0.5538\n",
      "Fold: 2, Epoch: 921, Loss: 0.3883, Train 0.5322, Val 0.5077\n",
      "Fold: 2, Epoch: 922, Loss: 0.4793, Train 0.5272, Val 0.5077\n",
      "Fold: 2, Epoch: 923, Loss: 0.3508, Train 0.5421, Val 0.5231\n",
      "Fold: 2, Epoch: 924, Loss: 0.4316, Train 0.6337, Val 0.5846\n",
      "Fold: 2, Epoch: 925, Loss: 0.4236, Train 0.8812, Val 0.8308\n",
      "Fold: 2, Epoch: 926, Loss: 0.4726, Train 0.9406, Val 0.9385\n",
      "Fold: 2, Epoch: 927, Loss: 0.3044, Train 0.9332, Val 0.9538\n",
      "Fold: 2, Epoch: 928, Loss: 0.3727, Train 0.9233, Val 0.9538\n",
      "Fold: 2, Epoch: 929, Loss: 0.3376, Train 0.8663, Val 0.9385\n",
      "Fold: 2, Epoch: 930, Loss: 0.3531, Train 0.8243, Val 0.9077\n",
      "Fold: 2, Epoch: 931, Loss: 0.3497, Train 0.8168, Val 0.8923\n",
      "Fold: 2, Epoch: 932, Loss: 0.3077, Train 0.8168, Val 0.8923\n",
      "Fold: 2, Epoch: 933, Loss: 0.3911, Train 0.8515, Val 0.9385\n",
      "Fold: 2, Epoch: 934, Loss: 0.4176, Train 0.9134, Val 0.9846\n",
      "Fold: 2, Epoch: 935, Loss: 0.3543, Train 0.9233, Val 0.9846\n",
      "Fold: 2, Epoch: 936, Loss: 0.4173, Train 0.9183, Val 0.9692\n",
      "Fold: 2, Epoch: 937, Loss: 0.3370, Train 0.9134, Val 0.9385\n",
      "Fold: 2, Epoch: 938, Loss: 0.4672, Train 0.9233, Val 0.9846\n",
      "Fold: 2, Epoch: 939, Loss: 0.4197, Train 0.9183, Val 0.9846\n",
      "Fold: 2, Epoch: 940, Loss: 0.3780, Train 0.8985, Val 0.9538\n",
      "Fold: 2, Epoch: 941, Loss: 0.4621, Train 0.8837, Val 0.9538\n",
      "Fold: 2, Epoch: 942, Loss: 0.3806, Train 0.9059, Val 0.9538\n",
      "Fold: 2, Epoch: 943, Loss: 0.4366, Train 0.8812, Val 0.8923\n",
      "Fold: 2, Epoch: 944, Loss: 0.4348, Train 0.8416, Val 0.8462\n",
      "Fold: 2, Epoch: 945, Loss: 0.3784, Train 0.8391, Val 0.8462\n",
      "Fold: 2, Epoch: 946, Loss: 0.3365, Train 0.7921, Val 0.7538\n",
      "Fold: 2, Epoch: 947, Loss: 0.3942, Train 0.7550, Val 0.7385\n",
      "Fold: 2, Epoch: 948, Loss: 0.4850, Train 0.7797, Val 0.7538\n",
      "Fold: 2, Epoch: 949, Loss: 0.4896, Train 0.8639, Val 0.8154\n",
      "Fold: 2, Epoch: 950, Loss: 0.3681, Train 0.9530, Val 0.9538\n",
      "Fold: 2, Epoch: 951, Loss: 0.3895, Train 0.9035, Val 0.9538\n",
      "Fold: 2, Epoch: 952, Loss: 0.3892, Train 0.8243, Val 0.8769\n",
      "Fold: 2, Epoch: 953, Loss: 0.3724, Train 0.8168, Val 0.8923\n",
      "Fold: 2, Epoch: 954, Loss: 0.3736, Train 0.8144, Val 0.8769\n",
      "Fold: 2, Epoch: 955, Loss: 0.4113, Train 0.8144, Val 0.8769\n",
      "Fold: 2, Epoch: 956, Loss: 0.3804, Train 0.8564, Val 0.9231\n",
      "Fold: 2, Epoch: 957, Loss: 0.3278, Train 0.8985, Val 0.9692\n",
      "Fold: 2, Epoch: 958, Loss: 0.4067, Train 0.9134, Val 0.9846\n",
      "Fold: 2, Epoch: 959, Loss: 0.2929, Train 0.9158, Val 0.9538\n",
      "Fold: 2, Epoch: 960, Loss: 0.3521, Train 0.9183, Val 0.9538\n",
      "Fold: 2, Epoch: 961, Loss: 0.3227, Train 0.9084, Val 0.9538\n",
      "Fold: 2, Epoch: 962, Loss: 0.3987, Train 0.9084, Val 0.9846\n",
      "Fold: 2, Epoch: 963, Loss: 0.3475, Train 0.8960, Val 0.9692\n",
      "Fold: 2, Epoch: 964, Loss: 0.3589, Train 0.8688, Val 0.9385\n",
      "Fold: 2, Epoch: 965, Loss: 0.3807, Train 0.8564, Val 0.9231\n",
      "Fold: 2, Epoch: 966, Loss: 0.3357, Train 0.8515, Val 0.9231\n",
      "Fold: 2, Epoch: 967, Loss: 0.4090, Train 0.8589, Val 0.9077\n",
      "Fold: 2, Epoch: 968, Loss: 0.3959, Train 0.8713, Val 0.9538\n",
      "Fold: 2, Epoch: 969, Loss: 0.4097, Train 0.9084, Val 0.9692\n",
      "Fold: 2, Epoch: 970, Loss: 0.3650, Train 0.9257, Val 0.9692\n",
      "Fold: 2, Epoch: 971, Loss: 0.3666, Train 0.9158, Val 0.9385\n",
      "Fold: 2, Epoch: 972, Loss: 0.3421, Train 0.8515, Val 0.8615\n",
      "Fold: 2, Epoch: 973, Loss: 0.2844, Train 0.7599, Val 0.7385\n",
      "Fold: 2, Epoch: 974, Loss: 0.3591, Train 0.8045, Val 0.7846\n",
      "Fold: 2, Epoch: 975, Loss: 0.2578, Train 0.8639, Val 0.8615\n",
      "Fold: 2, Epoch: 976, Loss: 0.2856, Train 0.9010, Val 0.8769\n",
      "Fold: 2, Epoch: 977, Loss: 0.2976, Train 0.9257, Val 0.9077\n",
      "Fold: 2, Epoch: 978, Loss: 0.4377, Train 0.9307, Val 0.9231\n",
      "Fold: 2, Epoch: 979, Loss: 0.4164, Train 0.9208, Val 0.9077\n",
      "Fold: 2, Epoch: 980, Loss: 0.3474, Train 0.9109, Val 0.9231\n",
      "Fold: 2, Epoch: 981, Loss: 0.4013, Train 0.9059, Val 0.9077\n",
      "Fold: 2, Epoch: 982, Loss: 0.3607, Train 0.8911, Val 0.8923\n",
      "Fold: 2, Epoch: 983, Loss: 0.3266, Train 0.8614, Val 0.8615\n",
      "Fold: 2, Epoch: 984, Loss: 0.4018, Train 0.8639, Val 0.8769\n",
      "Fold: 2, Epoch: 985, Loss: 0.3705, Train 0.8762, Val 0.8769\n",
      "Fold: 2, Epoch: 986, Loss: 0.3361, Train 0.9109, Val 0.8923\n",
      "Fold: 2, Epoch: 987, Loss: 0.3498, Train 0.9431, Val 0.9231\n",
      "Fold: 2, Epoch: 988, Loss: 0.4314, Train 0.9579, Val 0.9692\n",
      "Fold: 2, Epoch: 989, Loss: 0.3170, Train 0.9158, Val 0.9385\n",
      "Fold: 2, Epoch: 990, Loss: 0.3425, Train 0.8713, Val 0.8923\n",
      "Fold: 2, Epoch: 991, Loss: 0.3589, Train 0.8540, Val 0.8769\n",
      "Fold: 2, Epoch: 992, Loss: 0.3220, Train 0.8589, Val 0.8769\n",
      "Fold: 2, Epoch: 993, Loss: 0.2775, Train 0.8589, Val 0.8923\n",
      "Fold: 2, Epoch: 994, Loss: 0.4093, Train 0.8540, Val 0.8769\n",
      "Fold: 2, Epoch: 995, Loss: 0.4326, Train 0.8589, Val 0.8923\n",
      "Fold: 2, Epoch: 996, Loss: 0.3544, Train 0.8762, Val 0.9231\n",
      "Fold: 2, Epoch: 997, Loss: 0.2943, Train 0.9109, Val 0.9692\n",
      "Fold: 2, Epoch: 998, Loss: 0.3689, Train 0.9233, Val 0.9692\n",
      "Fold: 2, Epoch: 999, Loss: 0.3156, Train 0.9208, Val 0.9692\n",
      "Fold: 3, Epoch: 001, Loss: 2.7025, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 002, Loss: 5.0333, Train 0.3713, Val 0.4000\n",
      "Fold: 3, Epoch: 003, Loss: 3.0025, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 004, Loss: 4.0523, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 005, Loss: 3.8551, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 006, Loss: 3.9654, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 007, Loss: 2.7595, Train 0.3119, Val 0.2462\n",
      "Fold: 3, Epoch: 008, Loss: 2.3593, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 009, Loss: 2.5280, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 010, Loss: 2.8683, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 011, Loss: 2.7766, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 012, Loss: 3.1365, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 013, Loss: 2.9956, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 014, Loss: 3.0888, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 015, Loss: 2.5373, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 016, Loss: 2.3925, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 017, Loss: 2.4832, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 018, Loss: 2.5630, Train 0.3119, Val 0.2462\n",
      "Fold: 3, Epoch: 019, Loss: 2.5266, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 020, Loss: 2.2176, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 021, Loss: 2.3180, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 022, Loss: 2.4216, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 023, Loss: 2.4055, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 024, Loss: 2.4469, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 025, Loss: 2.3699, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 026, Loss: 2.4498, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 027, Loss: 2.4039, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 028, Loss: 2.6175, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 029, Loss: 2.2921, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 030, Loss: 2.2089, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 031, Loss: 2.2237, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 032, Loss: 2.3629, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 033, Loss: 2.2808, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 034, Loss: 2.2661, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 035, Loss: 2.2295, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 036, Loss: 2.2419, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 037, Loss: 2.3699, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 038, Loss: 2.2275, Train 0.3762, Val 0.3692\n",
      "Fold: 3, Epoch: 039, Loss: 2.2105, Train 0.1361, Val 0.1846\n",
      "Fold: 3, Epoch: 040, Loss: 2.2174, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 041, Loss: 2.1658, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 042, Loss: 2.2685, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 043, Loss: 2.1632, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 044, Loss: 2.1924, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 045, Loss: 2.2019, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 046, Loss: 2.2718, Train 0.3515, Val 0.2615\n",
      "Fold: 3, Epoch: 047, Loss: 2.1666, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 048, Loss: 2.2554, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 049, Loss: 2.1881, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 050, Loss: 2.1485, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 051, Loss: 2.1579, Train 0.1361, Val 0.1846\n",
      "Fold: 3, Epoch: 052, Loss: 2.2033, Train 0.1361, Val 0.1846\n",
      "Fold: 3, Epoch: 053, Loss: 2.2037, Train 0.1361, Val 0.1846\n",
      "Fold: 3, Epoch: 054, Loss: 2.1993, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 055, Loss: 2.1826, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 056, Loss: 2.1707, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 057, Loss: 2.1733, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 058, Loss: 2.2259, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 059, Loss: 2.2726, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 060, Loss: 2.1928, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 061, Loss: 2.1687, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 062, Loss: 2.1633, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 063, Loss: 2.2429, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 064, Loss: 2.2252, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 065, Loss: 2.2029, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 066, Loss: 2.1630, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 067, Loss: 2.1321, Train 0.1312, Val 0.1846\n",
      "Fold: 3, Epoch: 068, Loss: 2.1904, Train 0.1361, Val 0.1846\n",
      "Fold: 3, Epoch: 069, Loss: 2.2163, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 070, Loss: 2.1536, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 071, Loss: 2.1873, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 072, Loss: 2.2598, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 073, Loss: 2.1797, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 074, Loss: 2.1565, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 075, Loss: 2.1940, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 076, Loss: 2.1587, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 077, Loss: 2.1363, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 078, Loss: 2.1494, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 079, Loss: 2.1800, Train 0.3540, Val 0.2615\n",
      "Fold: 3, Epoch: 080, Loss: 2.1717, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 081, Loss: 2.1536, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 082, Loss: 2.1829, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 083, Loss: 2.1455, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 084, Loss: 2.1628, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 085, Loss: 2.1948, Train 0.3762, Val 0.3692\n",
      "Fold: 3, Epoch: 086, Loss: 2.1284, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 087, Loss: 2.1273, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 088, Loss: 2.1958, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 089, Loss: 2.1666, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 090, Loss: 2.1672, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 091, Loss: 2.1517, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 092, Loss: 2.1437, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 093, Loss: 2.1516, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 094, Loss: 2.1346, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 095, Loss: 2.1510, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 096, Loss: 2.1767, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 097, Loss: 2.1590, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 098, Loss: 2.1237, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 099, Loss: 2.1848, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 100, Loss: 2.1421, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 101, Loss: 2.1495, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 102, Loss: 2.2174, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 103, Loss: 2.1326, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 104, Loss: 2.1860, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 105, Loss: 2.1995, Train 0.3762, Val 0.3692\n",
      "Fold: 3, Epoch: 106, Loss: 2.1691, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 107, Loss: 2.1649, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 108, Loss: 2.1535, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 109, Loss: 2.1420, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 110, Loss: 2.1398, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 111, Loss: 2.1734, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 112, Loss: 2.2057, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 113, Loss: 2.1598, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 114, Loss: 2.1339, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 115, Loss: 2.1844, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 116, Loss: 2.2161, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 117, Loss: 2.1766, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 118, Loss: 2.2308, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 119, Loss: 2.1883, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 120, Loss: 2.1546, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 121, Loss: 2.1598, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 122, Loss: 2.1384, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 123, Loss: 2.1501, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 124, Loss: 2.1574, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 125, Loss: 2.1860, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 126, Loss: 2.1487, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 127, Loss: 2.2146, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 128, Loss: 2.1946, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 129, Loss: 2.1524, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 130, Loss: 2.1209, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 131, Loss: 2.1104, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 132, Loss: 2.1443, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 133, Loss: 2.1229, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 134, Loss: 2.1390, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 135, Loss: 2.1339, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 136, Loss: 2.1434, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 137, Loss: 2.1196, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 138, Loss: 2.1458, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 139, Loss: 2.1274, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 140, Loss: 2.1407, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 141, Loss: 2.1563, Train 0.3762, Val 0.3692\n",
      "Fold: 3, Epoch: 142, Loss: 2.1381, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 143, Loss: 2.1606, Train 0.3762, Val 0.3692\n",
      "Fold: 3, Epoch: 144, Loss: 2.1531, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 145, Loss: 2.1549, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 146, Loss: 2.1693, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 147, Loss: 2.1512, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 148, Loss: 2.1222, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 149, Loss: 2.1287, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 150, Loss: 2.1148, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 151, Loss: 2.1724, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 152, Loss: 2.1408, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 153, Loss: 2.1361, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 154, Loss: 2.1332, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 155, Loss: 2.1334, Train 0.3762, Val 0.3692\n",
      "Fold: 3, Epoch: 156, Loss: 2.1632, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 157, Loss: 2.1433, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 158, Loss: 2.1357, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 159, Loss: 2.1988, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 160, Loss: 2.1526, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 161, Loss: 2.1840, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 162, Loss: 2.2337, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 163, Loss: 2.1433, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 164, Loss: 2.1691, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 165, Loss: 2.1933, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 166, Loss: 2.1624, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 167, Loss: 2.1637, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 168, Loss: 2.1460, Train 0.3762, Val 0.3692\n",
      "Fold: 3, Epoch: 169, Loss: 2.1194, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 170, Loss: 2.1455, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 171, Loss: 2.1397, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 172, Loss: 2.1687, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 173, Loss: 2.1329, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 174, Loss: 2.1585, Train 0.3144, Val 0.2462\n",
      "Fold: 3, Epoch: 175, Loss: 2.2736, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 176, Loss: 2.1334, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 177, Loss: 2.1689, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 178, Loss: 2.2232, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 179, Loss: 2.1480, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 180, Loss: 2.1331, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 181, Loss: 2.1515, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 182, Loss: 2.2464, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 183, Loss: 2.1313, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 184, Loss: 2.1796, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 185, Loss: 2.2444, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 186, Loss: 2.1273, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 187, Loss: 2.1457, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 188, Loss: 2.1697, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 189, Loss: 2.1629, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 190, Loss: 2.1597, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 191, Loss: 2.1427, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 192, Loss: 2.2136, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 193, Loss: 2.1636, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 194, Loss: 2.1528, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 195, Loss: 2.1524, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 196, Loss: 2.1590, Train 0.3218, Val 0.2308\n",
      "Fold: 3, Epoch: 197, Loss: 2.1455, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 198, Loss: 2.1323, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 199, Loss: 2.1334, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 200, Loss: 2.1265, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 201, Loss: 2.1465, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 202, Loss: 2.1631, Train 0.3168, Val 0.2462\n",
      "Fold: 3, Epoch: 203, Loss: 2.1602, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 204, Loss: 2.1233, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 205, Loss: 2.1527, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 206, Loss: 2.1594, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 207, Loss: 2.1251, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 208, Loss: 2.1514, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 209, Loss: 2.1792, Train 0.3787, Val 0.3692\n",
      "Fold: 3, Epoch: 210, Loss: 2.1433, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 211, Loss: 2.1384, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 212, Loss: 2.1860, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 213, Loss: 2.1372, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 214, Loss: 2.1617, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 215, Loss: 2.1271, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 216, Loss: 2.1438, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 217, Loss: 2.1479, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 218, Loss: 2.1319, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 219, Loss: 2.1282, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 220, Loss: 2.1578, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 221, Loss: 2.1572, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 222, Loss: 2.1107, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 223, Loss: 2.1678, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 224, Loss: 2.1385, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 225, Loss: 2.1269, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 226, Loss: 2.1262, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 227, Loss: 2.1422, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 228, Loss: 2.1526, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 229, Loss: 2.1368, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 230, Loss: 2.1454, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 231, Loss: 2.1093, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 232, Loss: 2.1401, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 233, Loss: 2.1282, Train 0.4183, Val 0.3692\n",
      "Fold: 3, Epoch: 234, Loss: 2.1207, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 235, Loss: 2.1517, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 236, Loss: 2.1144, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 237, Loss: 2.1086, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 238, Loss: 2.1231, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 239, Loss: 2.1187, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 240, Loss: 2.1226, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 241, Loss: 2.1295, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 242, Loss: 2.1286, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 243, Loss: 2.1083, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 244, Loss: 2.1667, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 245, Loss: 2.1221, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 246, Loss: 2.1539, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 247, Loss: 2.1246, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 248, Loss: 2.1439, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 249, Loss: 2.1428, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 250, Loss: 2.1305, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 251, Loss: 2.1174, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 252, Loss: 2.1285, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 253, Loss: 2.1281, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 254, Loss: 2.1234, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 255, Loss: 2.1389, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 256, Loss: 2.1270, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 257, Loss: 2.1508, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 258, Loss: 2.0953, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 259, Loss: 2.1266, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 260, Loss: 2.1481, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 261, Loss: 2.1373, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 262, Loss: 2.1354, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 263, Loss: 2.1329, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 264, Loss: 2.1073, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 265, Loss: 2.1370, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 266, Loss: 2.1187, Train 0.3342, Val 0.2308\n",
      "Fold: 3, Epoch: 267, Loss: 2.1158, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 268, Loss: 2.1217, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 269, Loss: 2.1260, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 270, Loss: 2.1260, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 271, Loss: 2.1165, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 272, Loss: 2.1536, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 273, Loss: 2.1424, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 274, Loss: 2.1292, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 275, Loss: 2.1120, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 276, Loss: 2.1388, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 277, Loss: 2.1160, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 278, Loss: 2.1081, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 279, Loss: 2.1473, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 280, Loss: 2.1218, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 281, Loss: 2.1277, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 282, Loss: 2.1039, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 283, Loss: 2.1074, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 284, Loss: 2.1106, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 285, Loss: 2.1287, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 286, Loss: 2.1219, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 287, Loss: 2.1058, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 288, Loss: 2.1382, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 289, Loss: 2.0977, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 290, Loss: 2.1709, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 291, Loss: 2.1263, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 292, Loss: 2.1224, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 293, Loss: 2.1287, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 294, Loss: 2.1284, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 295, Loss: 2.1228, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 296, Loss: 2.1272, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 297, Loss: 2.1210, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 298, Loss: 2.1110, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 299, Loss: 2.1145, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 300, Loss: 2.1210, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 301, Loss: 2.1099, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 302, Loss: 2.1218, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 303, Loss: 2.1224, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 304, Loss: 2.1113, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 305, Loss: 2.0933, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 306, Loss: 2.1279, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 307, Loss: 2.1236, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 308, Loss: 2.1145, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 309, Loss: 2.1133, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 310, Loss: 2.1479, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 311, Loss: 2.1218, Train 0.3688, Val 0.2615\n",
      "Fold: 3, Epoch: 312, Loss: 2.1318, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 313, Loss: 2.1152, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 314, Loss: 2.1051, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 315, Loss: 2.1047, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 316, Loss: 2.0983, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 317, Loss: 2.1086, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 318, Loss: 2.0900, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 319, Loss: 2.1096, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 320, Loss: 2.0972, Train 0.3243, Val 0.2462\n",
      "Fold: 3, Epoch: 321, Loss: 2.1577, Train 0.3837, Val 0.3692\n",
      "Fold: 3, Epoch: 322, Loss: 2.0962, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 323, Loss: 2.1074, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 324, Loss: 2.1246, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 325, Loss: 2.0901, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 326, Loss: 2.0955, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 327, Loss: 2.1076, Train 0.3713, Val 0.2615\n",
      "Fold: 3, Epoch: 328, Loss: 2.1291, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 329, Loss: 2.1030, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 330, Loss: 2.1090, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 331, Loss: 2.1641, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 332, Loss: 2.1361, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 333, Loss: 2.1081, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 334, Loss: 2.1389, Train 0.3837, Val 0.3692\n",
      "Fold: 3, Epoch: 335, Loss: 2.0963, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 336, Loss: 2.0848, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 337, Loss: 2.1410, Train 0.3812, Val 0.2462\n",
      "Fold: 3, Epoch: 338, Loss: 2.0794, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 339, Loss: 2.1468, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 340, Loss: 2.0839, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 341, Loss: 2.0912, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 342, Loss: 2.1417, Train 0.3837, Val 0.3692\n",
      "Fold: 3, Epoch: 343, Loss: 2.0841, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 344, Loss: 2.1373, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 345, Loss: 2.0594, Train 0.3540, Val 0.2615\n",
      "Fold: 3, Epoch: 346, Loss: 2.0927, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 347, Loss: 2.0615, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 348, Loss: 2.0787, Train 0.3985, Val 0.3692\n",
      "Fold: 3, Epoch: 349, Loss: 2.0459, Train 0.3812, Val 0.3692\n",
      "Fold: 3, Epoch: 350, Loss: 2.1088, Train 0.3540, Val 0.2615\n",
      "Fold: 3, Epoch: 351, Loss: 2.0608, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 352, Loss: 2.0284, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 353, Loss: 2.0564, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 354, Loss: 2.0346, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 355, Loss: 2.0464, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 356, Loss: 2.0065, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 357, Loss: 2.0664, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 358, Loss: 1.9781, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 359, Loss: 1.9755, Train 0.3342, Val 0.2462\n",
      "Fold: 3, Epoch: 360, Loss: 1.9929, Train 0.3342, Val 0.2462\n",
      "Fold: 3, Epoch: 361, Loss: 1.9651, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 362, Loss: 1.9598, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 363, Loss: 1.9681, Train 0.3614, Val 0.2615\n",
      "Fold: 3, Epoch: 364, Loss: 1.9615, Train 0.3391, Val 0.2462\n",
      "Fold: 3, Epoch: 365, Loss: 1.9093, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 366, Loss: 1.8480, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 367, Loss: 1.9081, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 368, Loss: 1.8535, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 369, Loss: 1.8733, Train 0.3045, Val 0.1846\n",
      "Fold: 3, Epoch: 370, Loss: 1.9279, Train 0.3193, Val 0.2462\n",
      "Fold: 3, Epoch: 371, Loss: 1.8472, Train 0.3317, Val 0.2462\n",
      "Fold: 3, Epoch: 372, Loss: 1.8284, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 373, Loss: 1.7827, Train 0.3292, Val 0.2462\n",
      "Fold: 3, Epoch: 374, Loss: 1.7170, Train 0.3317, Val 0.2462\n",
      "Fold: 3, Epoch: 375, Loss: 1.6578, Train 0.6485, Val 0.5692\n",
      "Fold: 3, Epoch: 376, Loss: 1.6217, Train 0.3366, Val 0.2462\n",
      "Fold: 3, Epoch: 377, Loss: 1.6056, Train 0.3515, Val 0.2615\n",
      "Fold: 3, Epoch: 378, Loss: 1.5537, Train 0.3416, Val 0.2462\n",
      "Fold: 3, Epoch: 379, Loss: 1.4929, Train 0.3267, Val 0.2462\n",
      "Fold: 3, Epoch: 380, Loss: 1.5501, Train 0.5223, Val 0.4308\n",
      "Fold: 3, Epoch: 381, Loss: 1.4588, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 382, Loss: 1.5803, Train 0.6782, Val 0.6154\n",
      "Fold: 3, Epoch: 383, Loss: 1.3583, Train 0.3218, Val 0.2462\n",
      "Fold: 3, Epoch: 384, Loss: 1.7362, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 385, Loss: 1.3459, Train 0.5149, Val 0.5538\n",
      "Fold: 3, Epoch: 386, Loss: 1.6905, Train 0.5842, Val 0.5385\n",
      "Fold: 3, Epoch: 387, Loss: 1.3619, Train 0.6733, Val 0.6154\n",
      "Fold: 3, Epoch: 388, Loss: 1.5872, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 389, Loss: 1.4322, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 390, Loss: 1.4672, Train 0.5941, Val 0.5385\n",
      "Fold: 3, Epoch: 391, Loss: 1.5540, Train 0.7030, Val 0.6462\n",
      "Fold: 3, Epoch: 392, Loss: 1.3135, Train 0.7178, Val 0.6308\n",
      "Fold: 3, Epoch: 393, Loss: 1.3955, Train 0.7153, Val 0.6308\n",
      "Fold: 3, Epoch: 394, Loss: 1.4248, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 395, Loss: 1.3147, Train 0.5173, Val 0.5692\n",
      "Fold: 3, Epoch: 396, Loss: 1.4051, Train 0.5173, Val 0.5692\n",
      "Fold: 3, Epoch: 397, Loss: 1.3179, Train 0.5495, Val 0.5538\n",
      "Fold: 3, Epoch: 398, Loss: 1.2964, Train 0.7030, Val 0.6462\n",
      "Fold: 3, Epoch: 399, Loss: 1.3548, Train 0.5198, Val 0.4154\n",
      "Fold: 3, Epoch: 400, Loss: 1.3012, Train 0.4257, Val 0.3077\n",
      "Fold: 3, Epoch: 401, Loss: 1.3296, Train 0.4233, Val 0.3077\n",
      "Fold: 3, Epoch: 402, Loss: 1.2605, Train 0.2723, Val 0.2615\n",
      "Fold: 3, Epoch: 403, Loss: 1.3455, Train 0.5149, Val 0.5692\n",
      "Fold: 3, Epoch: 404, Loss: 1.3144, Train 0.5149, Val 0.5692\n",
      "Fold: 3, Epoch: 405, Loss: 1.3721, Train 0.5792, Val 0.5385\n",
      "Fold: 3, Epoch: 406, Loss: 1.4904, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 407, Loss: 1.2839, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 408, Loss: 1.3206, Train 0.7030, Val 0.6308\n",
      "Fold: 3, Epoch: 409, Loss: 1.3817, Train 0.4282, Val 0.2923\n",
      "Fold: 3, Epoch: 410, Loss: 1.3354, Train 0.3416, Val 0.3231\n",
      "Fold: 3, Epoch: 411, Loss: 1.2697, Train 0.5173, Val 0.5692\n",
      "Fold: 3, Epoch: 412, Loss: 1.2427, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 413, Loss: 1.3993, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 414, Loss: 1.2661, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 415, Loss: 1.2944, Train 0.7005, Val 0.6154\n",
      "Fold: 3, Epoch: 416, Loss: 1.2908, Train 0.5371, Val 0.4308\n",
      "Fold: 3, Epoch: 417, Loss: 1.2339, Train 0.4282, Val 0.3077\n",
      "Fold: 3, Epoch: 418, Loss: 1.2587, Train 0.2574, Val 0.2615\n",
      "Fold: 3, Epoch: 419, Loss: 1.2978, Train 0.5297, Val 0.5846\n",
      "Fold: 3, Epoch: 420, Loss: 1.2582, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 421, Loss: 1.1893, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 422, Loss: 1.2742, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 423, Loss: 1.3709, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 424, Loss: 1.3688, Train 0.7030, Val 0.6154\n",
      "Fold: 3, Epoch: 425, Loss: 1.2273, Train 0.3168, Val 0.3231\n",
      "Fold: 3, Epoch: 426, Loss: 1.4385, Train 0.5644, Val 0.6000\n",
      "Fold: 3, Epoch: 427, Loss: 1.2571, Train 0.7104, Val 0.6462\n",
      "Fold: 3, Epoch: 428, Loss: 1.2196, Train 0.7005, Val 0.6154\n",
      "Fold: 3, Epoch: 429, Loss: 1.2493, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 430, Loss: 1.2478, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 431, Loss: 1.3218, Train 0.7005, Val 0.6308\n",
      "Fold: 3, Epoch: 432, Loss: 1.2143, Train 0.7153, Val 0.6308\n",
      "Fold: 3, Epoch: 433, Loss: 1.2093, Train 0.7203, Val 0.6462\n",
      "Fold: 3, Epoch: 434, Loss: 1.2068, Train 0.7129, Val 0.6308\n",
      "Fold: 3, Epoch: 435, Loss: 1.3645, Train 0.7005, Val 0.6308\n",
      "Fold: 3, Epoch: 436, Loss: 1.2064, Train 0.6955, Val 0.6308\n",
      "Fold: 3, Epoch: 437, Loss: 1.2881, Train 0.6955, Val 0.6308\n",
      "Fold: 3, Epoch: 438, Loss: 1.2765, Train 0.6980, Val 0.6308\n",
      "Fold: 3, Epoch: 439, Loss: 1.1810, Train 0.7475, Val 0.6769\n",
      "Fold: 3, Epoch: 440, Loss: 1.1851, Train 0.4431, Val 0.3231\n",
      "Fold: 3, Epoch: 441, Loss: 1.1784, Train 0.4876, Val 0.3692\n",
      "Fold: 3, Epoch: 442, Loss: 1.1221, Train 0.4431, Val 0.3231\n",
      "Fold: 3, Epoch: 443, Loss: 1.2071, Train 0.5792, Val 0.6000\n",
      "Fold: 3, Epoch: 444, Loss: 1.2664, Train 0.7079, Val 0.6462\n",
      "Fold: 3, Epoch: 445, Loss: 1.3147, Train 0.5446, Val 0.5846\n",
      "Fold: 3, Epoch: 446, Loss: 1.2421, Train 0.6931, Val 0.6308\n",
      "Fold: 3, Epoch: 447, Loss: 1.1991, Train 0.7104, Val 0.6462\n",
      "Fold: 3, Epoch: 448, Loss: 1.1982, Train 0.7178, Val 0.6308\n",
      "Fold: 3, Epoch: 449, Loss: 1.1912, Train 0.7203, Val 0.6154\n",
      "Fold: 3, Epoch: 450, Loss: 1.1860, Train 0.7450, Val 0.6462\n",
      "Fold: 3, Epoch: 451, Loss: 1.1722, Train 0.7550, Val 0.6615\n",
      "Fold: 3, Epoch: 452, Loss: 1.1661, Train 0.5248, Val 0.5846\n",
      "Fold: 3, Epoch: 453, Loss: 1.1724, Train 0.5198, Val 0.5692\n",
      "Fold: 3, Epoch: 454, Loss: 1.2510, Train 0.7376, Val 0.6462\n",
      "Fold: 3, Epoch: 455, Loss: 1.1294, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 456, Loss: 1.1946, Train 0.6757, Val 0.6000\n",
      "Fold: 3, Epoch: 457, Loss: 1.2463, Train 0.6955, Val 0.6308\n",
      "Fold: 3, Epoch: 458, Loss: 1.1701, Train 0.7450, Val 0.6462\n",
      "Fold: 3, Epoch: 459, Loss: 1.1475, Train 0.5396, Val 0.6000\n",
      "Fold: 3, Epoch: 460, Loss: 1.3099, Train 0.5644, Val 0.6154\n",
      "Fold: 3, Epoch: 461, Loss: 1.4009, Train 0.4678, Val 0.4462\n",
      "Fold: 3, Epoch: 462, Loss: 1.1027, Train 0.7079, Val 0.6000\n",
      "Fold: 3, Epoch: 463, Loss: 1.1565, Train 0.6906, Val 0.6000\n",
      "Fold: 3, Epoch: 464, Loss: 1.2246, Train 0.7054, Val 0.6154\n",
      "Fold: 3, Epoch: 465, Loss: 1.1614, Train 0.7327, Val 0.6308\n",
      "Fold: 3, Epoch: 466, Loss: 1.0860, Train 0.7104, Val 0.6308\n",
      "Fold: 3, Epoch: 467, Loss: 1.1154, Train 0.5272, Val 0.5692\n",
      "Fold: 3, Epoch: 468, Loss: 1.1562, Train 0.5470, Val 0.5846\n",
      "Fold: 3, Epoch: 469, Loss: 1.2245, Train 0.7723, Val 0.6615\n",
      "Fold: 3, Epoch: 470, Loss: 1.2690, Train 0.7772, Val 0.6769\n",
      "Fold: 3, Epoch: 471, Loss: 1.1888, Train 0.7649, Val 0.6615\n",
      "Fold: 3, Epoch: 472, Loss: 1.1230, Train 0.7574, Val 0.6769\n",
      "Fold: 3, Epoch: 473, Loss: 1.1446, Train 0.7723, Val 0.7077\n",
      "Fold: 3, Epoch: 474, Loss: 1.1676, Train 0.7723, Val 0.6615\n",
      "Fold: 3, Epoch: 475, Loss: 1.2067, Train 0.7327, Val 0.6308\n",
      "Fold: 3, Epoch: 476, Loss: 1.0868, Train 0.6955, Val 0.6000\n",
      "Fold: 3, Epoch: 477, Loss: 1.1277, Train 0.6856, Val 0.6000\n",
      "Fold: 3, Epoch: 478, Loss: 1.0807, Train 0.7054, Val 0.6000\n",
      "Fold: 3, Epoch: 479, Loss: 1.0726, Train 0.7673, Val 0.6615\n",
      "Fold: 3, Epoch: 480, Loss: 1.0142, Train 0.7673, Val 0.6769\n",
      "Fold: 3, Epoch: 481, Loss: 1.0317, Train 0.5743, Val 0.5077\n",
      "Fold: 3, Epoch: 482, Loss: 1.0756, Train 0.4901, Val 0.4615\n",
      "Fold: 3, Epoch: 483, Loss: 1.0482, Train 0.5124, Val 0.4615\n",
      "Fold: 3, Epoch: 484, Loss: 1.0459, Train 0.7847, Val 0.6769\n",
      "Fold: 3, Epoch: 485, Loss: 1.0405, Train 0.7450, Val 0.6462\n",
      "Fold: 3, Epoch: 486, Loss: 1.0526, Train 0.7030, Val 0.6154\n",
      "Fold: 3, Epoch: 487, Loss: 1.0121, Train 0.6931, Val 0.6000\n",
      "Fold: 3, Epoch: 488, Loss: 1.0036, Train 0.7252, Val 0.6000\n",
      "Fold: 3, Epoch: 489, Loss: 1.0413, Train 0.7624, Val 0.6615\n",
      "Fold: 3, Epoch: 490, Loss: 1.0597, Train 0.4851, Val 0.4615\n",
      "Fold: 3, Epoch: 491, Loss: 1.0001, Train 0.4777, Val 0.4615\n",
      "Fold: 3, Epoch: 492, Loss: 1.0782, Train 0.4876, Val 0.4615\n",
      "Fold: 3, Epoch: 493, Loss: 1.0728, Train 0.7797, Val 0.7077\n",
      "Fold: 3, Epoch: 494, Loss: 1.0117, Train 0.7054, Val 0.6154\n",
      "Fold: 3, Epoch: 495, Loss: 1.0674, Train 0.6807, Val 0.6154\n",
      "Fold: 3, Epoch: 496, Loss: 1.2413, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 497, Loss: 1.0278, Train 0.7797, Val 0.6769\n",
      "Fold: 3, Epoch: 498, Loss: 0.9710, Train 0.4926, Val 0.4615\n",
      "Fold: 3, Epoch: 499, Loss: 1.2294, Train 0.4975, Val 0.4615\n",
      "Fold: 3, Epoch: 500, Loss: 1.0868, Train 0.5025, Val 0.4615\n",
      "Fold: 3, Epoch: 501, Loss: 0.9881, Train 0.7153, Val 0.6462\n",
      "Fold: 3, Epoch: 502, Loss: 0.9767, Train 0.7673, Val 0.6615\n",
      "Fold: 3, Epoch: 503, Loss: 0.9985, Train 0.7599, Val 0.6462\n",
      "Fold: 3, Epoch: 504, Loss: 1.0514, Train 0.7748, Val 0.6615\n",
      "Fold: 3, Epoch: 505, Loss: 1.1071, Train 0.7673, Val 0.6923\n",
      "Fold: 3, Epoch: 506, Loss: 1.1297, Train 0.7426, Val 0.6769\n",
      "Fold: 3, Epoch: 507, Loss: 1.0387, Train 0.7871, Val 0.7077\n",
      "Fold: 3, Epoch: 508, Loss: 0.9083, Train 0.7723, Val 0.6615\n",
      "Fold: 3, Epoch: 509, Loss: 1.0495, Train 0.7450, Val 0.6308\n",
      "Fold: 3, Epoch: 510, Loss: 1.0907, Train 0.7748, Val 0.6615\n",
      "Fold: 3, Epoch: 511, Loss: 1.0565, Train 0.7847, Val 0.6769\n",
      "Fold: 3, Epoch: 512, Loss: 0.9544, Train 0.7649, Val 0.6462\n",
      "Fold: 3, Epoch: 513, Loss: 1.0519, Train 0.7178, Val 0.6154\n",
      "Fold: 3, Epoch: 514, Loss: 0.9993, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 515, Loss: 1.0103, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 516, Loss: 0.9463, Train 0.7153, Val 0.6154\n",
      "Fold: 3, Epoch: 517, Loss: 0.9941, Train 0.7450, Val 0.6154\n",
      "Fold: 3, Epoch: 518, Loss: 0.9901, Train 0.7624, Val 0.6462\n",
      "Fold: 3, Epoch: 519, Loss: 0.9098, Train 0.7599, Val 0.6462\n",
      "Fold: 3, Epoch: 520, Loss: 0.9402, Train 0.7450, Val 0.6154\n",
      "Fold: 3, Epoch: 521, Loss: 0.9253, Train 0.7178, Val 0.6154\n",
      "Fold: 3, Epoch: 522, Loss: 0.8573, Train 0.7178, Val 0.6154\n",
      "Fold: 3, Epoch: 523, Loss: 1.0380, Train 0.7723, Val 0.6769\n",
      "Fold: 3, Epoch: 524, Loss: 0.9601, Train 0.7970, Val 0.7077\n",
      "Fold: 3, Epoch: 525, Loss: 0.9586, Train 0.7970, Val 0.7077\n",
      "Fold: 3, Epoch: 526, Loss: 1.0659, Train 0.7946, Val 0.7077\n",
      "Fold: 3, Epoch: 527, Loss: 0.9536, Train 0.7946, Val 0.7077\n",
      "Fold: 3, Epoch: 528, Loss: 0.8904, Train 0.7847, Val 0.6923\n",
      "Fold: 3, Epoch: 529, Loss: 0.9264, Train 0.7426, Val 0.6308\n",
      "Fold: 3, Epoch: 530, Loss: 0.9510, Train 0.7129, Val 0.6154\n",
      "Fold: 3, Epoch: 531, Loss: 0.9594, Train 0.7698, Val 0.6615\n",
      "Fold: 3, Epoch: 532, Loss: 0.9811, Train 0.7921, Val 0.7077\n",
      "Fold: 3, Epoch: 533, Loss: 1.0450, Train 0.7921, Val 0.7077\n",
      "Fold: 3, Epoch: 534, Loss: 1.0542, Train 0.7748, Val 0.6923\n",
      "Fold: 3, Epoch: 535, Loss: 1.0255, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 536, Loss: 0.9500, Train 0.6782, Val 0.6000\n",
      "Fold: 3, Epoch: 537, Loss: 0.9684, Train 0.6757, Val 0.6000\n",
      "Fold: 3, Epoch: 538, Loss: 0.9515, Train 0.6782, Val 0.6000\n",
      "Fold: 3, Epoch: 539, Loss: 1.0511, Train 0.7129, Val 0.6154\n",
      "Fold: 3, Epoch: 540, Loss: 0.9300, Train 0.7698, Val 0.6615\n",
      "Fold: 3, Epoch: 541, Loss: 0.9442, Train 0.7054, Val 0.6154\n",
      "Fold: 3, Epoch: 542, Loss: 1.1033, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 543, Loss: 0.9203, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 544, Loss: 0.9713, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 545, Loss: 0.9019, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 546, Loss: 0.9179, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 547, Loss: 0.9638, Train 0.7203, Val 0.6154\n",
      "Fold: 3, Epoch: 548, Loss: 0.9428, Train 0.7772, Val 0.7231\n",
      "Fold: 3, Epoch: 549, Loss: 0.9359, Train 0.7871, Val 0.7077\n",
      "Fold: 3, Epoch: 550, Loss: 1.0305, Train 0.7228, Val 0.6154\n",
      "Fold: 3, Epoch: 551, Loss: 0.8990, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 552, Loss: 0.9556, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 553, Loss: 0.9339, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 554, Loss: 0.9553, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 555, Loss: 0.9139, Train 0.7525, Val 0.6615\n",
      "Fold: 3, Epoch: 556, Loss: 0.9432, Train 0.7748, Val 0.6923\n",
      "Fold: 3, Epoch: 557, Loss: 0.9482, Train 0.7896, Val 0.7077\n",
      "Fold: 3, Epoch: 558, Loss: 0.9757, Train 0.7772, Val 0.6769\n",
      "Fold: 3, Epoch: 559, Loss: 0.8705, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 560, Loss: 0.9179, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 561, Loss: 0.9603, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 562, Loss: 0.9066, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 563, Loss: 0.8525, Train 0.7030, Val 0.6154\n",
      "Fold: 3, Epoch: 564, Loss: 0.8858, Train 0.7228, Val 0.6769\n",
      "Fold: 3, Epoch: 565, Loss: 0.9577, Train 0.6559, Val 0.6308\n",
      "Fold: 3, Epoch: 566, Loss: 0.9674, Train 0.7252, Val 0.6769\n",
      "Fold: 3, Epoch: 567, Loss: 0.9727, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 568, Loss: 0.8509, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 569, Loss: 0.8856, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 570, Loss: 0.8634, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 571, Loss: 0.8426, Train 0.7327, Val 0.6308\n",
      "Fold: 3, Epoch: 572, Loss: 0.9298, Train 0.7599, Val 0.6615\n",
      "Fold: 3, Epoch: 573, Loss: 0.9039, Train 0.7153, Val 0.6154\n",
      "Fold: 3, Epoch: 574, Loss: 0.8150, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 575, Loss: 0.9430, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 576, Loss: 0.7922, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 577, Loss: 0.8560, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 578, Loss: 0.9663, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 579, Loss: 0.9091, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 580, Loss: 0.9124, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 581, Loss: 0.8949, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 582, Loss: 0.9180, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 583, Loss: 0.8950, Train 0.7079, Val 0.6462\n",
      "Fold: 3, Epoch: 584, Loss: 0.8653, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 585, Loss: 0.9154, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 586, Loss: 0.8530, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 587, Loss: 0.9013, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 588, Loss: 0.9260, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 589, Loss: 0.9610, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 590, Loss: 0.9095, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 591, Loss: 0.8482, Train 0.7450, Val 0.6462\n",
      "Fold: 3, Epoch: 592, Loss: 1.0114, Train 0.7525, Val 0.6462\n",
      "Fold: 3, Epoch: 593, Loss: 0.8458, Train 0.7426, Val 0.6308\n",
      "Fold: 3, Epoch: 594, Loss: 0.9162, Train 0.7450, Val 0.6308\n",
      "Fold: 3, Epoch: 595, Loss: 0.9177, Train 0.7104, Val 0.6154\n",
      "Fold: 3, Epoch: 596, Loss: 0.8479, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 597, Loss: 0.9037, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 598, Loss: 0.8936, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 599, Loss: 0.9259, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 600, Loss: 0.9096, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 601, Loss: 1.0027, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 602, Loss: 0.9241, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 603, Loss: 0.8085, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 604, Loss: 0.8482, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 605, Loss: 0.8360, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 606, Loss: 0.9322, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 607, Loss: 0.9039, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 608, Loss: 0.8343, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 609, Loss: 1.0563, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 610, Loss: 0.8411, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 611, Loss: 0.8620, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 612, Loss: 0.8939, Train 0.6807, Val 0.6000\n",
      "Fold: 3, Epoch: 613, Loss: 0.9997, Train 0.6782, Val 0.6000\n",
      "Fold: 3, Epoch: 614, Loss: 1.0395, Train 0.6832, Val 0.6154\n",
      "Fold: 3, Epoch: 615, Loss: 0.9240, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 616, Loss: 0.8961, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 617, Loss: 0.8433, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 618, Loss: 0.9272, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 619, Loss: 0.8446, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 620, Loss: 0.9014, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 621, Loss: 0.8340, Train 0.7351, Val 0.6308\n",
      "Fold: 3, Epoch: 622, Loss: 0.9299, Train 0.7129, Val 0.6154\n",
      "Fold: 3, Epoch: 623, Loss: 0.8930, Train 0.6832, Val 0.6154\n",
      "Fold: 3, Epoch: 624, Loss: 0.9910, Train 0.6832, Val 0.6154\n",
      "Fold: 3, Epoch: 625, Loss: 0.8509, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 626, Loss: 0.8700, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 627, Loss: 0.8967, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 628, Loss: 0.9028, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 629, Loss: 0.9012, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 630, Loss: 0.7826, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 631, Loss: 0.8455, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 632, Loss: 0.8452, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 633, Loss: 0.8691, Train 0.6807, Val 0.6000\n",
      "Fold: 3, Epoch: 634, Loss: 0.9089, Train 0.6807, Val 0.6000\n",
      "Fold: 3, Epoch: 635, Loss: 0.8673, Train 0.6832, Val 0.6154\n",
      "Fold: 3, Epoch: 636, Loss: 0.8696, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 637, Loss: 0.9219, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 638, Loss: 0.8634, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 639, Loss: 0.8976, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 640, Loss: 0.8483, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 641, Loss: 0.8209, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 642, Loss: 0.7620, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 643, Loss: 0.9500, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 644, Loss: 0.8629, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 645, Loss: 0.7810, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 646, Loss: 0.8210, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 647, Loss: 0.8945, Train 0.7054, Val 0.6154\n",
      "Fold: 3, Epoch: 648, Loss: 0.9666, Train 0.7327, Val 0.6308\n",
      "Fold: 3, Epoch: 649, Loss: 0.9110, Train 0.7475, Val 0.6462\n",
      "Fold: 3, Epoch: 650, Loss: 0.8945, Train 0.5743, Val 0.6154\n",
      "Fold: 3, Epoch: 651, Loss: 0.8883, Train 0.4653, Val 0.4769\n",
      "Fold: 3, Epoch: 652, Loss: 0.8333, Train 0.4035, Val 0.3846\n",
      "Fold: 3, Epoch: 653, Loss: 0.8748, Train 0.5248, Val 0.4615\n",
      "Fold: 3, Epoch: 654, Loss: 0.8661, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 655, Loss: 0.8324, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 656, Loss: 0.8399, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 657, Loss: 0.8540, Train 0.7054, Val 0.6154\n",
      "Fold: 3, Epoch: 658, Loss: 0.8342, Train 0.7104, Val 0.6308\n",
      "Fold: 3, Epoch: 659, Loss: 0.8467, Train 0.4480, Val 0.4615\n",
      "Fold: 3, Epoch: 660, Loss: 0.8331, Train 0.6064, Val 0.5538\n",
      "Fold: 3, Epoch: 661, Loss: 0.8433, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 662, Loss: 0.7877, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 663, Loss: 0.8539, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 664, Loss: 0.8197, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 665, Loss: 0.8220, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 666, Loss: 0.8530, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 667, Loss: 0.7765, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 668, Loss: 0.8655, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 669, Loss: 0.8379, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 670, Loss: 0.8979, Train 0.6807, Val 0.6154\n",
      "Fold: 3, Epoch: 671, Loss: 0.9995, Train 0.6807, Val 0.6154\n",
      "Fold: 3, Epoch: 672, Loss: 0.9566, Train 0.6807, Val 0.6154\n",
      "Fold: 3, Epoch: 673, Loss: 0.8668, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 674, Loss: 0.8315, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 675, Loss: 0.8895, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 676, Loss: 0.8390, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 677, Loss: 0.9481, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 678, Loss: 0.8313, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 679, Loss: 0.8430, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 680, Loss: 0.9860, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 681, Loss: 0.7987, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 682, Loss: 0.8166, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 683, Loss: 0.8104, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 684, Loss: 0.8523, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 685, Loss: 0.9058, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 686, Loss: 0.9260, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 687, Loss: 0.9069, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 688, Loss: 0.8288, Train 0.6708, Val 0.6000\n",
      "Fold: 3, Epoch: 689, Loss: 0.9095, Train 0.5668, Val 0.5385\n",
      "Fold: 3, Epoch: 690, Loss: 0.8524, Train 0.6287, Val 0.5538\n",
      "Fold: 3, Epoch: 691, Loss: 0.9999, Train 0.7005, Val 0.6154\n",
      "Fold: 3, Epoch: 692, Loss: 0.8249, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 693, Loss: 0.7735, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 694, Loss: 0.8825, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 695, Loss: 0.8093, Train 0.7030, Val 0.6154\n",
      "Fold: 3, Epoch: 696, Loss: 0.8389, Train 0.7079, Val 0.6308\n",
      "Fold: 3, Epoch: 697, Loss: 0.8472, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 698, Loss: 0.8427, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 699, Loss: 0.8004, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 700, Loss: 0.7808, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 701, Loss: 0.8747, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 702, Loss: 0.8584, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 703, Loss: 0.8539, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 704, Loss: 0.7722, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 705, Loss: 0.8206, Train 0.6980, Val 0.6154\n",
      "Fold: 3, Epoch: 706, Loss: 0.8473, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 707, Loss: 0.8580, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 708, Loss: 0.7882, Train 0.6856, Val 0.6154\n",
      "Fold: 3, Epoch: 709, Loss: 0.8130, Train 0.6881, Val 0.6154\n",
      "Fold: 3, Epoch: 710, Loss: 0.8826, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 711, Loss: 0.7695, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 712, Loss: 0.8047, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 713, Loss: 0.8099, Train 0.7079, Val 0.6154\n",
      "Fold: 3, Epoch: 714, Loss: 0.7482, Train 0.4752, Val 0.4769\n",
      "Fold: 3, Epoch: 715, Loss: 0.8158, Train 0.7129, Val 0.6154\n",
      "Fold: 3, Epoch: 716, Loss: 0.8291, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 717, Loss: 0.8472, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 718, Loss: 0.7389, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 719, Loss: 0.8242, Train 0.6931, Val 0.6154\n",
      "Fold: 3, Epoch: 720, Loss: 0.7693, Train 0.6906, Val 0.6154\n",
      "Fold: 3, Epoch: 721, Loss: 0.7272, Train 0.7054, Val 0.6308\n",
      "Fold: 3, Epoch: 722, Loss: 0.8326, Train 0.7178, Val 0.6462\n",
      "Fold: 3, Epoch: 723, Loss: 0.7240, Train 0.7871, Val 0.6769\n",
      "Fold: 3, Epoch: 724, Loss: 0.7128, Train 0.7896, Val 0.6923\n",
      "Fold: 3, Epoch: 725, Loss: 0.7261, Train 0.7847, Val 0.6923\n",
      "Fold: 3, Epoch: 726, Loss: 0.7770, Train 0.6559, Val 0.5846\n",
      "Fold: 3, Epoch: 727, Loss: 0.7418, Train 0.5495, Val 0.4462\n",
      "Fold: 3, Epoch: 728, Loss: 0.8148, Train 0.4554, Val 0.3846\n",
      "Fold: 3, Epoch: 729, Loss: 0.7558, Train 0.4505, Val 0.4154\n",
      "Fold: 3, Epoch: 730, Loss: 0.7691, Train 0.4381, Val 0.4154\n",
      "Fold: 3, Epoch: 731, Loss: 0.7681, Train 0.4653, Val 0.4462\n",
      "Fold: 3, Epoch: 732, Loss: 0.7713, Train 0.5223, Val 0.5077\n",
      "Fold: 3, Epoch: 733, Loss: 0.7626, Train 0.7574, Val 0.6462\n",
      "Fold: 3, Epoch: 734, Loss: 0.7364, Train 0.7475, Val 0.6615\n",
      "Fold: 3, Epoch: 735, Loss: 0.7847, Train 0.7079, Val 0.6154\n",
      "Fold: 3, Epoch: 736, Loss: 0.7566, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 737, Loss: 0.7367, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 738, Loss: 0.7388, Train 0.7129, Val 0.6308\n",
      "Fold: 3, Epoch: 739, Loss: 0.7575, Train 0.7228, Val 0.6923\n",
      "Fold: 3, Epoch: 740, Loss: 0.6390, Train 0.6955, Val 0.6462\n",
      "Fold: 3, Epoch: 741, Loss: 0.7159, Train 0.6559, Val 0.6462\n",
      "Fold: 3, Epoch: 742, Loss: 0.6948, Train 0.7426, Val 0.6923\n",
      "Fold: 3, Epoch: 743, Loss: 0.7215, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 744, Loss: 0.6847, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 745, Loss: 0.6953, Train 0.6955, Val 0.6154\n",
      "Fold: 3, Epoch: 746, Loss: 0.6486, Train 0.7104, Val 0.6308\n",
      "Fold: 3, Epoch: 747, Loss: 0.6721, Train 0.7698, Val 0.7538\n",
      "Fold: 3, Epoch: 748, Loss: 0.6749, Train 0.7104, Val 0.7077\n",
      "Fold: 3, Epoch: 749, Loss: 0.6802, Train 0.6634, Val 0.7077\n",
      "Fold: 3, Epoch: 750, Loss: 0.6689, Train 0.6733, Val 0.7385\n",
      "Fold: 3, Epoch: 751, Loss: 0.7047, Train 0.6931, Val 0.7077\n",
      "Fold: 3, Epoch: 752, Loss: 0.5998, Train 0.8317, Val 0.8154\n",
      "Fold: 3, Epoch: 753, Loss: 0.6324, Train 0.8861, Val 0.8462\n",
      "Fold: 3, Epoch: 754, Loss: 0.6525, Train 0.8416, Val 0.8154\n",
      "Fold: 3, Epoch: 755, Loss: 0.5571, Train 0.7673, Val 0.7077\n",
      "Fold: 3, Epoch: 756, Loss: 0.6182, Train 0.7376, Val 0.6923\n",
      "Fold: 3, Epoch: 757, Loss: 0.6075, Train 0.7203, Val 0.7077\n",
      "Fold: 3, Epoch: 758, Loss: 0.6546, Train 0.6708, Val 0.6462\n",
      "Fold: 3, Epoch: 759, Loss: 0.5658, Train 0.6733, Val 0.6462\n",
      "Fold: 3, Epoch: 760, Loss: 0.5245, Train 0.6609, Val 0.6462\n",
      "Fold: 3, Epoch: 761, Loss: 0.6856, Train 0.5520, Val 0.5077\n",
      "Fold: 3, Epoch: 762, Loss: 0.6490, Train 0.6757, Val 0.6000\n",
      "Fold: 3, Epoch: 763, Loss: 0.5013, Train 0.8168, Val 0.7846\n",
      "Fold: 3, Epoch: 764, Loss: 0.6635, Train 0.8985, Val 0.8615\n",
      "Fold: 3, Epoch: 765, Loss: 0.5625, Train 0.8762, Val 0.8615\n",
      "Fold: 3, Epoch: 766, Loss: 0.4691, Train 0.8564, Val 0.8615\n",
      "Fold: 3, Epoch: 767, Loss: 0.5924, Train 0.8589, Val 0.8462\n",
      "Fold: 3, Epoch: 768, Loss: 0.5719, Train 0.8738, Val 0.8769\n",
      "Fold: 3, Epoch: 769, Loss: 0.6114, Train 0.8465, Val 0.8462\n",
      "Fold: 3, Epoch: 770, Loss: 0.5611, Train 0.8688, Val 0.8615\n",
      "Fold: 3, Epoch: 771, Loss: 0.5971, Train 0.8985, Val 0.8769\n",
      "Fold: 3, Epoch: 772, Loss: 0.4635, Train 0.8985, Val 0.8615\n",
      "Fold: 3, Epoch: 773, Loss: 0.6000, Train 0.9010, Val 0.8615\n",
      "Fold: 3, Epoch: 774, Loss: 0.5621, Train 0.8812, Val 0.8769\n",
      "Fold: 3, Epoch: 775, Loss: 0.4581, Train 0.8366, Val 0.8462\n",
      "Fold: 3, Epoch: 776, Loss: 0.5154, Train 0.8267, Val 0.8000\n",
      "Fold: 3, Epoch: 777, Loss: 0.5548, Train 0.8465, Val 0.8308\n",
      "Fold: 3, Epoch: 778, Loss: 0.5086, Train 0.8762, Val 0.8769\n",
      "Fold: 3, Epoch: 779, Loss: 0.5078, Train 0.8812, Val 0.8769\n",
      "Fold: 3, Epoch: 780, Loss: 0.6063, Train 0.9010, Val 0.8615\n",
      "Fold: 3, Epoch: 781, Loss: 0.4245, Train 0.9010, Val 0.8615\n",
      "Fold: 3, Epoch: 782, Loss: 0.4305, Train 0.9010, Val 0.8615\n",
      "Fold: 3, Epoch: 783, Loss: 0.4665, Train 0.9059, Val 0.8615\n",
      "Fold: 3, Epoch: 784, Loss: 0.5269, Train 0.9035, Val 0.8462\n",
      "Fold: 3, Epoch: 785, Loss: 0.5522, Train 0.8886, Val 0.8308\n",
      "Fold: 3, Epoch: 786, Loss: 0.4026, Train 0.8515, Val 0.8308\n",
      "Fold: 3, Epoch: 787, Loss: 0.5108, Train 0.8144, Val 0.7538\n",
      "Fold: 3, Epoch: 788, Loss: 0.6300, Train 0.8589, Val 0.8308\n",
      "Fold: 3, Epoch: 789, Loss: 0.5760, Train 0.8911, Val 0.8615\n",
      "Fold: 3, Epoch: 790, Loss: 0.5946, Train 0.8960, Val 0.8615\n",
      "Fold: 3, Epoch: 791, Loss: 0.5702, Train 0.8639, Val 0.8308\n",
      "Fold: 3, Epoch: 792, Loss: 0.5243, Train 0.7104, Val 0.7538\n",
      "Fold: 3, Epoch: 793, Loss: 0.4655, Train 0.6238, Val 0.6462\n",
      "Fold: 3, Epoch: 794, Loss: 0.4976, Train 0.6188, Val 0.6462\n",
      "Fold: 3, Epoch: 795, Loss: 0.6256, Train 0.6213, Val 0.6462\n",
      "Fold: 3, Epoch: 796, Loss: 0.6106, Train 0.7376, Val 0.6923\n",
      "Fold: 3, Epoch: 797, Loss: 0.5639, Train 0.7475, Val 0.7231\n",
      "Fold: 3, Epoch: 798, Loss: 0.4563, Train 0.7599, Val 0.7692\n",
      "Fold: 3, Epoch: 799, Loss: 0.5372, Train 0.7995, Val 0.8000\n",
      "Fold: 3, Epoch: 800, Loss: 0.5885, Train 0.8020, Val 0.8000\n",
      "Fold: 3, Epoch: 801, Loss: 0.4894, Train 0.8317, Val 0.8154\n",
      "Fold: 3, Epoch: 802, Loss: 0.5704, Train 0.8960, Val 0.8769\n",
      "Fold: 3, Epoch: 803, Loss: 0.5522, Train 0.9010, Val 0.8615\n",
      "Fold: 3, Epoch: 804, Loss: 0.5636, Train 0.9059, Val 0.8769\n",
      "Fold: 3, Epoch: 805, Loss: 0.4624, Train 0.9084, Val 0.8462\n",
      "Fold: 3, Epoch: 806, Loss: 0.6501, Train 0.8515, Val 0.8462\n",
      "Fold: 3, Epoch: 807, Loss: 0.4456, Train 0.7500, Val 0.7077\n",
      "Fold: 3, Epoch: 808, Loss: 0.4450, Train 0.7252, Val 0.6923\n",
      "Fold: 3, Epoch: 809, Loss: 0.4538, Train 0.7748, Val 0.7077\n",
      "Fold: 3, Epoch: 810, Loss: 0.5823, Train 0.8094, Val 0.7846\n",
      "Fold: 3, Epoch: 811, Loss: 0.5450, Train 0.8366, Val 0.8154\n",
      "Fold: 3, Epoch: 812, Loss: 0.4575, Train 0.8663, Val 0.8154\n",
      "Fold: 3, Epoch: 813, Loss: 0.6396, Train 0.9134, Val 0.8769\n",
      "Fold: 3, Epoch: 814, Loss: 0.4674, Train 0.7129, Val 0.6923\n",
      "Fold: 3, Epoch: 815, Loss: 0.4358, Train 0.6337, Val 0.6154\n",
      "Fold: 3, Epoch: 816, Loss: 0.4834, Train 0.6733, Val 0.6462\n",
      "Fold: 3, Epoch: 817, Loss: 0.5895, Train 0.8614, Val 0.7846\n",
      "Fold: 3, Epoch: 818, Loss: 0.4988, Train 0.8985, Val 0.8462\n",
      "Fold: 3, Epoch: 819, Loss: 0.4611, Train 0.8936, Val 0.8462\n",
      "Fold: 3, Epoch: 820, Loss: 0.4263, Train 0.8614, Val 0.8308\n",
      "Fold: 3, Epoch: 821, Loss: 0.4321, Train 0.8366, Val 0.8308\n",
      "Fold: 3, Epoch: 822, Loss: 0.4347, Train 0.8391, Val 0.8308\n",
      "Fold: 3, Epoch: 823, Loss: 0.4455, Train 0.8614, Val 0.8000\n",
      "Fold: 3, Epoch: 824, Loss: 0.4392, Train 0.9109, Val 0.8615\n",
      "Fold: 3, Epoch: 825, Loss: 0.4493, Train 0.9282, Val 0.8923\n",
      "Fold: 3, Epoch: 826, Loss: 0.4032, Train 0.9035, Val 0.8769\n",
      "Fold: 3, Epoch: 827, Loss: 0.3900, Train 0.8639, Val 0.8462\n",
      "Fold: 3, Epoch: 828, Loss: 0.4761, Train 0.7401, Val 0.7385\n",
      "Fold: 3, Epoch: 829, Loss: 0.5318, Train 0.8762, Val 0.8308\n",
      "Fold: 3, Epoch: 830, Loss: 0.4700, Train 0.9134, Val 0.8615\n",
      "Fold: 3, Epoch: 831, Loss: 0.4149, Train 0.9134, Val 0.8769\n",
      "Fold: 3, Epoch: 832, Loss: 0.4180, Train 0.8837, Val 0.8615\n",
      "Fold: 3, Epoch: 833, Loss: 0.5539, Train 0.8490, Val 0.8154\n",
      "Fold: 3, Epoch: 834, Loss: 0.5233, Train 0.8589, Val 0.8154\n",
      "Fold: 3, Epoch: 835, Loss: 0.4224, Train 0.9059, Val 0.8615\n",
      "Fold: 3, Epoch: 836, Loss: 0.3774, Train 0.9257, Val 0.8769\n",
      "Fold: 3, Epoch: 837, Loss: 0.4514, Train 0.9059, Val 0.8462\n",
      "Fold: 3, Epoch: 838, Loss: 0.4312, Train 0.8515, Val 0.7692\n",
      "Fold: 3, Epoch: 839, Loss: 0.4204, Train 0.7946, Val 0.7077\n",
      "Fold: 3, Epoch: 840, Loss: 0.4086, Train 0.7871, Val 0.6769\n",
      "Fold: 3, Epoch: 841, Loss: 0.4799, Train 0.8119, Val 0.7231\n",
      "Fold: 3, Epoch: 842, Loss: 0.4281, Train 0.7946, Val 0.7538\n",
      "Fold: 3, Epoch: 843, Loss: 0.4491, Train 0.8094, Val 0.7538\n",
      "Fold: 3, Epoch: 844, Loss: 0.3387, Train 0.7871, Val 0.7692\n",
      "Fold: 3, Epoch: 845, Loss: 0.4561, Train 0.7500, Val 0.7077\n",
      "Fold: 3, Epoch: 846, Loss: 0.4472, Train 0.8069, Val 0.7846\n",
      "Fold: 3, Epoch: 847, Loss: 0.5522, Train 0.9059, Val 0.8769\n",
      "Fold: 3, Epoch: 848, Loss: 0.4093, Train 0.9183, Val 0.8615\n",
      "Fold: 3, Epoch: 849, Loss: 0.4310, Train 0.9134, Val 0.8615\n",
      "Fold: 3, Epoch: 850, Loss: 0.4287, Train 0.9109, Val 0.8769\n",
      "Fold: 3, Epoch: 851, Loss: 0.4288, Train 0.9134, Val 0.8769\n",
      "Fold: 3, Epoch: 852, Loss: 0.4790, Train 0.9183, Val 0.8769\n",
      "Fold: 3, Epoch: 853, Loss: 0.4450, Train 0.9158, Val 0.8769\n",
      "Fold: 3, Epoch: 854, Loss: 0.5093, Train 0.9208, Val 0.8615\n",
      "Fold: 3, Epoch: 855, Loss: 0.3776, Train 0.9158, Val 0.8462\n",
      "Fold: 3, Epoch: 856, Loss: 0.4189, Train 0.9059, Val 0.8462\n",
      "Fold: 3, Epoch: 857, Loss: 0.4476, Train 0.8936, Val 0.8462\n",
      "Fold: 3, Epoch: 858, Loss: 0.4581, Train 0.8762, Val 0.8308\n",
      "Fold: 3, Epoch: 859, Loss: 0.4613, Train 0.8490, Val 0.8154\n",
      "Fold: 3, Epoch: 860, Loss: 0.3766, Train 0.8639, Val 0.8308\n",
      "Fold: 3, Epoch: 861, Loss: 0.3967, Train 0.8960, Val 0.8462\n",
      "Fold: 3, Epoch: 862, Loss: 0.3750, Train 0.9035, Val 0.8615\n",
      "Fold: 3, Epoch: 863, Loss: 0.3673, Train 0.9010, Val 0.8615\n",
      "Fold: 3, Epoch: 864, Loss: 0.4154, Train 0.8688, Val 0.8462\n",
      "Fold: 3, Epoch: 865, Loss: 0.4078, Train 0.8243, Val 0.8154\n",
      "Fold: 3, Epoch: 866, Loss: 0.3281, Train 0.7376, Val 0.7385\n",
      "Fold: 3, Epoch: 867, Loss: 0.4360, Train 0.7054, Val 0.7077\n",
      "Fold: 3, Epoch: 868, Loss: 0.3836, Train 0.6733, Val 0.6769\n",
      "Fold: 3, Epoch: 869, Loss: 0.3847, Train 0.6634, Val 0.6615\n",
      "Fold: 3, Epoch: 870, Loss: 0.4317, Train 0.6485, Val 0.6615\n",
      "Fold: 3, Epoch: 871, Loss: 0.4608, Train 0.6584, Val 0.6615\n",
      "Fold: 3, Epoch: 872, Loss: 0.4169, Train 0.7030, Val 0.6769\n",
      "Fold: 3, Epoch: 873, Loss: 0.4055, Train 0.7624, Val 0.7077\n",
      "Fold: 3, Epoch: 874, Loss: 0.3655, Train 0.8564, Val 0.8154\n",
      "Fold: 3, Epoch: 875, Loss: 0.4859, Train 0.9233, Val 0.8769\n",
      "Fold: 3, Epoch: 876, Loss: 0.3577, Train 0.9208, Val 0.8769\n",
      "Fold: 3, Epoch: 877, Loss: 0.3971, Train 0.8837, Val 0.8308\n",
      "Fold: 3, Epoch: 878, Loss: 0.4323, Train 0.8317, Val 0.8000\n",
      "Fold: 3, Epoch: 879, Loss: 0.3701, Train 0.8094, Val 0.7692\n",
      "Fold: 3, Epoch: 880, Loss: 0.4512, Train 0.8094, Val 0.7846\n",
      "Fold: 3, Epoch: 881, Loss: 0.4231, Train 0.8243, Val 0.8000\n",
      "Fold: 3, Epoch: 882, Loss: 0.4886, Train 0.8861, Val 0.8462\n",
      "Fold: 3, Epoch: 883, Loss: 0.4519, Train 0.9183, Val 0.8615\n",
      "Fold: 3, Epoch: 884, Loss: 0.3642, Train 0.8985, Val 0.8615\n",
      "Fold: 3, Epoch: 885, Loss: 0.3508, Train 0.8812, Val 0.8615\n",
      "Fold: 3, Epoch: 886, Loss: 0.3321, Train 0.8243, Val 0.7846\n",
      "Fold: 3, Epoch: 887, Loss: 0.4056, Train 0.7921, Val 0.7538\n",
      "Fold: 3, Epoch: 888, Loss: 0.4493, Train 0.8168, Val 0.7692\n",
      "Fold: 3, Epoch: 889, Loss: 0.3851, Train 0.8762, Val 0.8154\n",
      "Fold: 3, Epoch: 890, Loss: 0.4130, Train 0.9134, Val 0.8923\n",
      "Fold: 3, Epoch: 891, Loss: 0.4870, Train 0.9307, Val 0.9077\n",
      "Fold: 3, Epoch: 892, Loss: 0.3634, Train 0.9381, Val 0.8923\n",
      "Fold: 3, Epoch: 893, Loss: 0.3134, Train 0.9059, Val 0.8769\n",
      "Fold: 3, Epoch: 894, Loss: 0.3518, Train 0.8564, Val 0.8615\n",
      "Fold: 3, Epoch: 895, Loss: 0.4580, Train 0.8218, Val 0.8308\n",
      "Fold: 3, Epoch: 896, Loss: 0.4355, Train 0.8391, Val 0.8615\n",
      "Fold: 3, Epoch: 897, Loss: 0.3976, Train 0.8317, Val 0.8615\n",
      "Fold: 3, Epoch: 898, Loss: 0.4734, Train 0.8861, Val 0.9077\n",
      "Fold: 3, Epoch: 899, Loss: 0.3710, Train 0.9233, Val 0.9231\n",
      "Fold: 3, Epoch: 900, Loss: 0.4824, Train 0.9134, Val 0.8769\n",
      "Fold: 3, Epoch: 901, Loss: 0.3784, Train 0.9109, Val 0.8615\n",
      "Fold: 3, Epoch: 902, Loss: 0.4551, Train 0.9109, Val 0.8769\n",
      "Fold: 3, Epoch: 903, Loss: 0.3444, Train 0.9084, Val 0.8615\n",
      "Fold: 3, Epoch: 904, Loss: 0.3605, Train 0.8936, Val 0.8462\n",
      "Fold: 3, Epoch: 905, Loss: 0.3501, Train 0.8490, Val 0.8000\n",
      "Fold: 3, Epoch: 906, Loss: 0.3574, Train 0.8342, Val 0.8154\n",
      "Fold: 3, Epoch: 907, Loss: 0.3444, Train 0.8391, Val 0.8154\n",
      "Fold: 3, Epoch: 908, Loss: 0.3685, Train 0.8540, Val 0.8000\n",
      "Fold: 3, Epoch: 909, Loss: 0.4220, Train 0.8911, Val 0.8462\n",
      "Fold: 3, Epoch: 910, Loss: 0.3426, Train 0.9035, Val 0.8615\n",
      "Fold: 3, Epoch: 911, Loss: 0.4116, Train 0.9233, Val 0.8923\n",
      "Fold: 3, Epoch: 912, Loss: 0.4670, Train 0.9233, Val 0.8769\n",
      "Fold: 3, Epoch: 913, Loss: 0.4251, Train 0.9307, Val 0.8769\n",
      "Fold: 3, Epoch: 914, Loss: 0.3147, Train 0.9183, Val 0.8769\n",
      "Fold: 3, Epoch: 915, Loss: 0.3546, Train 0.9183, Val 0.8769\n",
      "Fold: 3, Epoch: 916, Loss: 0.4913, Train 0.9183, Val 0.8769\n",
      "Fold: 3, Epoch: 917, Loss: 0.4852, Train 0.9233, Val 0.8769\n",
      "Fold: 3, Epoch: 918, Loss: 0.4327, Train 0.8911, Val 0.8462\n",
      "Fold: 3, Epoch: 919, Loss: 0.4301, Train 0.7079, Val 0.6308\n",
      "Fold: 3, Epoch: 920, Loss: 0.3048, Train 0.5965, Val 0.5692\n",
      "Fold: 3, Epoch: 921, Loss: 0.3883, Train 0.5322, Val 0.4769\n",
      "Fold: 3, Epoch: 922, Loss: 0.4793, Train 0.5272, Val 0.4769\n",
      "Fold: 3, Epoch: 923, Loss: 0.3508, Train 0.5421, Val 0.4923\n",
      "Fold: 3, Epoch: 924, Loss: 0.4316, Train 0.6337, Val 0.6000\n",
      "Fold: 3, Epoch: 925, Loss: 0.4236, Train 0.8812, Val 0.9077\n",
      "Fold: 3, Epoch: 926, Loss: 0.4726, Train 0.9406, Val 0.9077\n",
      "Fold: 3, Epoch: 927, Loss: 0.3044, Train 0.9332, Val 0.9077\n",
      "Fold: 3, Epoch: 928, Loss: 0.3727, Train 0.9233, Val 0.8615\n",
      "Fold: 3, Epoch: 929, Loss: 0.3376, Train 0.8663, Val 0.8308\n",
      "Fold: 3, Epoch: 930, Loss: 0.3531, Train 0.8243, Val 0.8000\n",
      "Fold: 3, Epoch: 931, Loss: 0.3497, Train 0.8168, Val 0.8000\n",
      "Fold: 3, Epoch: 932, Loss: 0.3077, Train 0.8168, Val 0.8000\n",
      "Fold: 3, Epoch: 933, Loss: 0.3911, Train 0.8515, Val 0.8308\n",
      "Fold: 3, Epoch: 934, Loss: 0.4176, Train 0.9134, Val 0.8769\n",
      "Fold: 3, Epoch: 935, Loss: 0.3543, Train 0.9233, Val 0.8769\n",
      "Fold: 3, Epoch: 936, Loss: 0.4173, Train 0.9183, Val 0.8769\n",
      "Fold: 3, Epoch: 937, Loss: 0.3370, Train 0.9134, Val 0.8769\n",
      "Fold: 3, Epoch: 938, Loss: 0.4672, Train 0.9233, Val 0.8769\n",
      "Fold: 3, Epoch: 939, Loss: 0.4197, Train 0.9183, Val 0.8769\n",
      "Fold: 3, Epoch: 940, Loss: 0.3780, Train 0.8985, Val 0.8462\n",
      "Fold: 3, Epoch: 941, Loss: 0.4621, Train 0.8837, Val 0.8462\n",
      "Fold: 3, Epoch: 942, Loss: 0.3806, Train 0.9059, Val 0.8923\n",
      "Fold: 3, Epoch: 943, Loss: 0.4366, Train 0.8812, Val 0.8462\n",
      "Fold: 3, Epoch: 944, Loss: 0.4348, Train 0.8416, Val 0.7846\n",
      "Fold: 3, Epoch: 945, Loss: 0.3784, Train 0.8391, Val 0.7846\n",
      "Fold: 3, Epoch: 946, Loss: 0.3365, Train 0.7921, Val 0.7538\n",
      "Fold: 3, Epoch: 947, Loss: 0.3942, Train 0.7550, Val 0.7231\n",
      "Fold: 3, Epoch: 948, Loss: 0.4850, Train 0.7797, Val 0.7385\n",
      "Fold: 3, Epoch: 949, Loss: 0.4896, Train 0.8639, Val 0.8462\n",
      "Fold: 3, Epoch: 950, Loss: 0.3681, Train 0.9530, Val 0.9385\n",
      "Fold: 3, Epoch: 951, Loss: 0.3895, Train 0.9035, Val 0.8769\n",
      "Fold: 3, Epoch: 952, Loss: 0.3892, Train 0.8243, Val 0.8000\n",
      "Fold: 3, Epoch: 953, Loss: 0.3724, Train 0.8168, Val 0.8000\n",
      "Fold: 3, Epoch: 954, Loss: 0.3736, Train 0.8144, Val 0.8000\n",
      "Fold: 3, Epoch: 955, Loss: 0.4113, Train 0.8144, Val 0.8000\n",
      "Fold: 3, Epoch: 956, Loss: 0.3804, Train 0.8564, Val 0.8308\n",
      "Fold: 3, Epoch: 957, Loss: 0.3278, Train 0.8985, Val 0.8769\n",
      "Fold: 3, Epoch: 958, Loss: 0.4067, Train 0.9134, Val 0.8769\n",
      "Fold: 3, Epoch: 959, Loss: 0.2929, Train 0.9158, Val 0.8769\n",
      "Fold: 3, Epoch: 960, Loss: 0.3521, Train 0.9183, Val 0.8769\n",
      "Fold: 3, Epoch: 961, Loss: 0.3227, Train 0.9084, Val 0.8769\n",
      "Fold: 3, Epoch: 962, Loss: 0.3987, Train 0.9084, Val 0.8769\n",
      "Fold: 3, Epoch: 963, Loss: 0.3475, Train 0.8960, Val 0.8615\n",
      "Fold: 3, Epoch: 964, Loss: 0.3589, Train 0.8688, Val 0.8308\n",
      "Fold: 3, Epoch: 965, Loss: 0.3807, Train 0.8564, Val 0.8308\n",
      "Fold: 3, Epoch: 966, Loss: 0.3357, Train 0.8515, Val 0.8308\n",
      "Fold: 3, Epoch: 967, Loss: 0.4090, Train 0.8589, Val 0.8308\n",
      "Fold: 3, Epoch: 968, Loss: 0.3959, Train 0.8713, Val 0.8308\n",
      "Fold: 3, Epoch: 969, Loss: 0.4097, Train 0.9084, Val 0.8615\n",
      "Fold: 3, Epoch: 970, Loss: 0.3650, Train 0.9257, Val 0.8769\n",
      "Fold: 3, Epoch: 971, Loss: 0.3666, Train 0.9158, Val 0.8769\n",
      "Fold: 3, Epoch: 972, Loss: 0.3421, Train 0.8515, Val 0.8615\n",
      "Fold: 3, Epoch: 973, Loss: 0.2844, Train 0.7599, Val 0.7692\n",
      "Fold: 3, Epoch: 974, Loss: 0.3591, Train 0.8045, Val 0.8000\n",
      "Fold: 3, Epoch: 975, Loss: 0.2578, Train 0.8639, Val 0.8769\n",
      "Fold: 3, Epoch: 976, Loss: 0.2856, Train 0.9010, Val 0.8923\n",
      "Fold: 3, Epoch: 977, Loss: 0.2976, Train 0.9257, Val 0.8769\n",
      "Fold: 3, Epoch: 978, Loss: 0.4377, Train 0.9307, Val 0.8769\n",
      "Fold: 3, Epoch: 979, Loss: 0.4164, Train 0.9208, Val 0.8769\n",
      "Fold: 3, Epoch: 980, Loss: 0.3474, Train 0.9109, Val 0.8923\n",
      "Fold: 3, Epoch: 981, Loss: 0.4013, Train 0.9059, Val 0.8769\n",
      "Fold: 3, Epoch: 982, Loss: 0.3607, Train 0.8911, Val 0.8769\n",
      "Fold: 3, Epoch: 983, Loss: 0.3266, Train 0.8614, Val 0.8615\n",
      "Fold: 3, Epoch: 984, Loss: 0.4018, Train 0.8639, Val 0.8462\n",
      "Fold: 3, Epoch: 985, Loss: 0.3705, Train 0.8762, Val 0.8615\n",
      "Fold: 3, Epoch: 986, Loss: 0.3361, Train 0.9109, Val 0.8769\n",
      "Fold: 3, Epoch: 987, Loss: 0.3498, Train 0.9431, Val 0.9231\n",
      "Fold: 3, Epoch: 988, Loss: 0.4314, Train 0.9579, Val 0.9385\n",
      "Fold: 3, Epoch: 989, Loss: 0.3170, Train 0.9158, Val 0.9077\n",
      "Fold: 3, Epoch: 990, Loss: 0.3425, Train 0.8713, Val 0.8769\n",
      "Fold: 3, Epoch: 991, Loss: 0.3589, Train 0.8540, Val 0.8615\n",
      "Fold: 3, Epoch: 992, Loss: 0.3220, Train 0.8589, Val 0.8769\n",
      "Fold: 3, Epoch: 993, Loss: 0.2775, Train 0.8589, Val 0.8615\n",
      "Fold: 3, Epoch: 994, Loss: 0.4093, Train 0.8540, Val 0.8462\n",
      "Fold: 3, Epoch: 995, Loss: 0.4326, Train 0.8589, Val 0.8615\n",
      "Fold: 3, Epoch: 996, Loss: 0.3544, Train 0.8762, Val 0.8308\n",
      "Fold: 3, Epoch: 997, Loss: 0.2943, Train 0.9109, Val 0.8769\n",
      "Fold: 3, Epoch: 998, Loss: 0.3689, Train 0.9233, Val 0.8769\n",
      "Fold: 3, Epoch: 999, Loss: 0.3156, Train 0.9208, Val 0.8769\n",
      "Fold: 4, Epoch: 001, Loss: 2.7025, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 002, Loss: 5.0333, Train 0.3713, Val 0.3125\n",
      "Fold: 4, Epoch: 003, Loss: 3.0025, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 004, Loss: 4.0523, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 005, Loss: 3.8551, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 006, Loss: 3.9654, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 007, Loss: 2.7595, Train 0.3119, Val 0.3906\n",
      "Fold: 4, Epoch: 008, Loss: 2.3593, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 009, Loss: 2.5280, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 010, Loss: 2.8683, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 011, Loss: 2.7766, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 012, Loss: 3.1365, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 013, Loss: 2.9956, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 014, Loss: 3.0888, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 015, Loss: 2.5373, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 016, Loss: 2.3925, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 017, Loss: 2.4832, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 018, Loss: 2.5630, Train 0.3119, Val 0.3906\n",
      "Fold: 4, Epoch: 019, Loss: 2.5266, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 020, Loss: 2.2176, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 021, Loss: 2.3180, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 022, Loss: 2.4216, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 023, Loss: 2.4055, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 024, Loss: 2.4469, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 025, Loss: 2.3699, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 026, Loss: 2.4498, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 027, Loss: 2.4039, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 028, Loss: 2.6175, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 029, Loss: 2.2921, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 030, Loss: 2.2089, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 031, Loss: 2.2237, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 032, Loss: 2.3629, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 033, Loss: 2.2808, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 034, Loss: 2.2661, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 035, Loss: 2.2295, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 036, Loss: 2.2419, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 037, Loss: 2.3699, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 038, Loss: 2.2275, Train 0.3762, Val 0.2969\n",
      "Fold: 4, Epoch: 039, Loss: 2.2105, Train 0.1361, Val 0.1719\n",
      "Fold: 4, Epoch: 040, Loss: 2.2174, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 041, Loss: 2.1658, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 042, Loss: 2.2685, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 043, Loss: 2.1632, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 044, Loss: 2.1924, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 045, Loss: 2.2019, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 046, Loss: 2.2718, Train 0.3515, Val 0.3906\n",
      "Fold: 4, Epoch: 047, Loss: 2.1666, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 048, Loss: 2.2554, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 049, Loss: 2.1881, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 050, Loss: 2.1485, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 051, Loss: 2.1579, Train 0.1361, Val 0.1719\n",
      "Fold: 4, Epoch: 052, Loss: 2.2033, Train 0.1361, Val 0.1719\n",
      "Fold: 4, Epoch: 053, Loss: 2.2037, Train 0.1361, Val 0.1719\n",
      "Fold: 4, Epoch: 054, Loss: 2.1993, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 055, Loss: 2.1826, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 056, Loss: 2.1707, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 057, Loss: 2.1733, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 058, Loss: 2.2259, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 059, Loss: 2.2726, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 060, Loss: 2.1928, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 061, Loss: 2.1687, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 062, Loss: 2.1633, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 063, Loss: 2.2429, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 064, Loss: 2.2252, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 065, Loss: 2.2029, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 066, Loss: 2.1630, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 067, Loss: 2.1321, Train 0.1312, Val 0.1719\n",
      "Fold: 4, Epoch: 068, Loss: 2.1904, Train 0.1361, Val 0.1719\n",
      "Fold: 4, Epoch: 069, Loss: 2.2163, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 070, Loss: 2.1536, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 071, Loss: 2.1873, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 072, Loss: 2.2598, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 073, Loss: 2.1797, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 074, Loss: 2.1565, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 075, Loss: 2.1940, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 076, Loss: 2.1587, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 077, Loss: 2.1363, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 078, Loss: 2.1494, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 079, Loss: 2.1800, Train 0.3540, Val 0.3594\n",
      "Fold: 4, Epoch: 080, Loss: 2.1717, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 081, Loss: 2.1536, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 082, Loss: 2.1829, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 083, Loss: 2.1455, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 084, Loss: 2.1628, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 085, Loss: 2.1948, Train 0.3762, Val 0.2969\n",
      "Fold: 4, Epoch: 086, Loss: 2.1284, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 087, Loss: 2.1273, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 088, Loss: 2.1958, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 089, Loss: 2.1666, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 090, Loss: 2.1672, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 091, Loss: 2.1517, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 092, Loss: 2.1437, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 093, Loss: 2.1516, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 094, Loss: 2.1346, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 095, Loss: 2.1510, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 096, Loss: 2.1767, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 097, Loss: 2.1590, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 098, Loss: 2.1237, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 099, Loss: 2.1848, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 100, Loss: 2.1421, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 101, Loss: 2.1495, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 102, Loss: 2.2174, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 103, Loss: 2.1326, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 104, Loss: 2.1860, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 105, Loss: 2.1995, Train 0.3762, Val 0.2969\n",
      "Fold: 4, Epoch: 106, Loss: 2.1691, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 107, Loss: 2.1649, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 108, Loss: 2.1535, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 109, Loss: 2.1420, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 110, Loss: 2.1398, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 111, Loss: 2.1734, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 112, Loss: 2.2057, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 113, Loss: 2.1598, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 114, Loss: 2.1339, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 115, Loss: 2.1844, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 116, Loss: 2.2161, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 117, Loss: 2.1766, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 118, Loss: 2.2308, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 119, Loss: 2.1883, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 120, Loss: 2.1546, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 121, Loss: 2.1598, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 122, Loss: 2.1384, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 123, Loss: 2.1501, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 124, Loss: 2.1574, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 125, Loss: 2.1860, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 126, Loss: 2.1487, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 127, Loss: 2.2146, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 128, Loss: 2.1946, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 129, Loss: 2.1524, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 130, Loss: 2.1209, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 131, Loss: 2.1104, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 132, Loss: 2.1443, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 133, Loss: 2.1229, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 134, Loss: 2.1390, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 135, Loss: 2.1339, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 136, Loss: 2.1434, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 137, Loss: 2.1196, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 138, Loss: 2.1458, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 139, Loss: 2.1274, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 140, Loss: 2.1407, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 141, Loss: 2.1563, Train 0.3762, Val 0.2969\n",
      "Fold: 4, Epoch: 142, Loss: 2.1381, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 143, Loss: 2.1606, Train 0.3762, Val 0.2969\n",
      "Fold: 4, Epoch: 144, Loss: 2.1531, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 145, Loss: 2.1549, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 146, Loss: 2.1693, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 147, Loss: 2.1512, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 148, Loss: 2.1222, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 149, Loss: 2.1287, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 150, Loss: 2.1148, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 151, Loss: 2.1724, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 152, Loss: 2.1408, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 153, Loss: 2.1361, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 154, Loss: 2.1332, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 155, Loss: 2.1334, Train 0.3762, Val 0.2969\n",
      "Fold: 4, Epoch: 156, Loss: 2.1632, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 157, Loss: 2.1433, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 158, Loss: 2.1357, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 159, Loss: 2.1988, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 160, Loss: 2.1526, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 161, Loss: 2.1840, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 162, Loss: 2.2337, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 163, Loss: 2.1433, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 164, Loss: 2.1691, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 165, Loss: 2.1933, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 166, Loss: 2.1624, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 167, Loss: 2.1637, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 168, Loss: 2.1460, Train 0.3762, Val 0.2969\n",
      "Fold: 4, Epoch: 169, Loss: 2.1194, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 170, Loss: 2.1455, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 171, Loss: 2.1397, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 172, Loss: 2.1687, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 173, Loss: 2.1329, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 174, Loss: 2.1585, Train 0.3144, Val 0.3906\n",
      "Fold: 4, Epoch: 175, Loss: 2.2736, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 176, Loss: 2.1334, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 177, Loss: 2.1689, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 178, Loss: 2.2232, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 179, Loss: 2.1480, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 180, Loss: 2.1331, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 181, Loss: 2.1515, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 182, Loss: 2.2464, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 183, Loss: 2.1313, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 184, Loss: 2.1796, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 185, Loss: 2.2444, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 186, Loss: 2.1273, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 187, Loss: 2.1457, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 188, Loss: 2.1697, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 189, Loss: 2.1629, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 190, Loss: 2.1597, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 191, Loss: 2.1427, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 192, Loss: 2.2136, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 193, Loss: 2.1636, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 194, Loss: 2.1528, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 195, Loss: 2.1524, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 196, Loss: 2.1590, Train 0.3218, Val 0.3750\n",
      "Fold: 4, Epoch: 197, Loss: 2.1455, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 198, Loss: 2.1323, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 199, Loss: 2.1334, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 200, Loss: 2.1265, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 201, Loss: 2.1465, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 202, Loss: 2.1631, Train 0.3168, Val 0.3906\n",
      "Fold: 4, Epoch: 203, Loss: 2.1602, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 204, Loss: 2.1233, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 205, Loss: 2.1527, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 206, Loss: 2.1594, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 207, Loss: 2.1251, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 208, Loss: 2.1514, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 209, Loss: 2.1792, Train 0.3787, Val 0.2969\n",
      "Fold: 4, Epoch: 210, Loss: 2.1433, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 211, Loss: 2.1384, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 212, Loss: 2.1860, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 213, Loss: 2.1372, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 214, Loss: 2.1617, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 215, Loss: 2.1271, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 216, Loss: 2.1438, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 217, Loss: 2.1479, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 218, Loss: 2.1319, Train 0.3267, Val 0.3906\n",
      "Fold: 4, Epoch: 219, Loss: 2.1282, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 220, Loss: 2.1578, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 221, Loss: 2.1572, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 222, Loss: 2.1107, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 223, Loss: 2.1678, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 224, Loss: 2.1385, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 225, Loss: 2.1269, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 226, Loss: 2.1262, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 227, Loss: 2.1422, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 228, Loss: 2.1526, Train 0.3267, Val 0.3906\n",
      "Fold: 4, Epoch: 229, Loss: 2.1368, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 230, Loss: 2.1454, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 231, Loss: 2.1093, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 232, Loss: 2.1401, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 233, Loss: 2.1282, Train 0.4183, Val 0.4062\n",
      "Fold: 4, Epoch: 234, Loss: 2.1207, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 235, Loss: 2.1517, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 236, Loss: 2.1144, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 237, Loss: 2.1086, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 238, Loss: 2.1231, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 239, Loss: 2.1187, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 240, Loss: 2.1226, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 241, Loss: 2.1295, Train 0.3267, Val 0.3906\n",
      "Fold: 4, Epoch: 242, Loss: 2.1286, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 243, Loss: 2.1083, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 244, Loss: 2.1667, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 245, Loss: 2.1221, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 246, Loss: 2.1539, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 247, Loss: 2.1246, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 248, Loss: 2.1439, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 249, Loss: 2.1428, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 250, Loss: 2.1305, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 251, Loss: 2.1174, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 252, Loss: 2.1285, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 253, Loss: 2.1281, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 254, Loss: 2.1234, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 255, Loss: 2.1389, Train 0.3267, Val 0.3750\n",
      "Fold: 4, Epoch: 256, Loss: 2.1270, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 257, Loss: 2.1508, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 258, Loss: 2.0953, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 259, Loss: 2.1266, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 260, Loss: 2.1481, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 261, Loss: 2.1373, Train 0.3267, Val 0.3906\n",
      "Fold: 4, Epoch: 262, Loss: 2.1354, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 263, Loss: 2.1329, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 264, Loss: 2.1073, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 265, Loss: 2.1370, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 266, Loss: 2.1187, Train 0.3342, Val 0.3750\n",
      "Fold: 4, Epoch: 267, Loss: 2.1158, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 268, Loss: 2.1217, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 269, Loss: 2.1260, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 270, Loss: 2.1260, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 271, Loss: 2.1165, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 272, Loss: 2.1536, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 273, Loss: 2.1424, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 274, Loss: 2.1292, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 275, Loss: 2.1120, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 276, Loss: 2.1388, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 277, Loss: 2.1160, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 278, Loss: 2.1081, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 279, Loss: 2.1473, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 280, Loss: 2.1218, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 281, Loss: 2.1277, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 282, Loss: 2.1039, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 283, Loss: 2.1074, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 284, Loss: 2.1106, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 285, Loss: 2.1287, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 286, Loss: 2.1219, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 287, Loss: 2.1058, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 288, Loss: 2.1382, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 289, Loss: 2.0977, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 290, Loss: 2.1709, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 291, Loss: 2.1263, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 292, Loss: 2.1224, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 293, Loss: 2.1287, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 294, Loss: 2.1284, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 295, Loss: 2.1228, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 296, Loss: 2.1272, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 297, Loss: 2.1210, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 298, Loss: 2.1110, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 299, Loss: 2.1145, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 300, Loss: 2.1210, Train 0.3267, Val 0.3906\n",
      "Fold: 4, Epoch: 301, Loss: 2.1099, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 302, Loss: 2.1218, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 303, Loss: 2.1224, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 304, Loss: 2.1113, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 305, Loss: 2.0933, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 306, Loss: 2.1279, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 307, Loss: 2.1236, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 308, Loss: 2.1145, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 309, Loss: 2.1133, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 310, Loss: 2.1479, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 311, Loss: 2.1218, Train 0.3688, Val 0.3750\n",
      "Fold: 4, Epoch: 312, Loss: 2.1318, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 313, Loss: 2.1152, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 314, Loss: 2.1051, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 315, Loss: 2.1047, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 316, Loss: 2.0983, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 317, Loss: 2.1086, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 318, Loss: 2.0900, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 319, Loss: 2.1096, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 320, Loss: 2.0972, Train 0.3243, Val 0.3906\n",
      "Fold: 4, Epoch: 321, Loss: 2.1577, Train 0.3837, Val 0.2969\n",
      "Fold: 4, Epoch: 322, Loss: 2.0962, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 323, Loss: 2.1074, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 324, Loss: 2.1246, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 325, Loss: 2.0901, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 326, Loss: 2.0955, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 327, Loss: 2.1076, Train 0.3713, Val 0.3906\n",
      "Fold: 4, Epoch: 328, Loss: 2.1291, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 329, Loss: 2.1030, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 330, Loss: 2.1090, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 331, Loss: 2.1641, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 332, Loss: 2.1361, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 333, Loss: 2.1081, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 334, Loss: 2.1389, Train 0.3837, Val 0.2969\n",
      "Fold: 4, Epoch: 335, Loss: 2.0963, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 336, Loss: 2.0848, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 337, Loss: 2.1410, Train 0.3812, Val 0.3750\n",
      "Fold: 4, Epoch: 338, Loss: 2.0794, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 339, Loss: 2.1468, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 340, Loss: 2.0839, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 341, Loss: 2.0912, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 342, Loss: 2.1417, Train 0.3837, Val 0.2969\n",
      "Fold: 4, Epoch: 343, Loss: 2.0841, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 344, Loss: 2.1373, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 345, Loss: 2.0594, Train 0.3540, Val 0.3906\n",
      "Fold: 4, Epoch: 346, Loss: 2.0927, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 347, Loss: 2.0615, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 348, Loss: 2.0787, Train 0.3985, Val 0.3125\n",
      "Fold: 4, Epoch: 349, Loss: 2.0459, Train 0.3812, Val 0.2969\n",
      "Fold: 4, Epoch: 350, Loss: 2.1088, Train 0.3540, Val 0.3906\n",
      "Fold: 4, Epoch: 351, Loss: 2.0608, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 352, Loss: 2.0284, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 353, Loss: 2.0564, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 354, Loss: 2.0346, Train 0.3267, Val 0.3906\n",
      "Fold: 4, Epoch: 355, Loss: 2.0464, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 356, Loss: 2.0065, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 357, Loss: 2.0664, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 358, Loss: 1.9781, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 359, Loss: 1.9755, Train 0.3342, Val 0.3906\n",
      "Fold: 4, Epoch: 360, Loss: 1.9929, Train 0.3342, Val 0.3906\n",
      "Fold: 4, Epoch: 361, Loss: 1.9651, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 362, Loss: 1.9598, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 363, Loss: 1.9681, Train 0.3614, Val 0.4062\n",
      "Fold: 4, Epoch: 364, Loss: 1.9615, Train 0.3391, Val 0.3906\n",
      "Fold: 4, Epoch: 365, Loss: 1.9093, Train 0.3267, Val 0.3906\n",
      "Fold: 4, Epoch: 366, Loss: 1.8480, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 367, Loss: 1.9081, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 368, Loss: 1.8535, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 369, Loss: 1.8733, Train 0.3045, Val 0.4062\n",
      "Fold: 4, Epoch: 370, Loss: 1.9279, Train 0.3193, Val 0.3906\n",
      "Fold: 4, Epoch: 371, Loss: 1.8472, Train 0.3317, Val 0.3906\n",
      "Fold: 4, Epoch: 372, Loss: 1.8284, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 373, Loss: 1.7827, Train 0.3292, Val 0.3906\n",
      "Fold: 4, Epoch: 374, Loss: 1.7170, Train 0.3317, Val 0.3906\n",
      "Fold: 4, Epoch: 375, Loss: 1.6578, Train 0.6485, Val 0.6875\n",
      "Fold: 4, Epoch: 376, Loss: 1.6217, Train 0.3366, Val 0.3906\n",
      "Fold: 4, Epoch: 377, Loss: 1.6056, Train 0.3515, Val 0.3906\n",
      "Fold: 4, Epoch: 378, Loss: 1.5537, Train 0.3416, Val 0.3906\n",
      "Fold: 4, Epoch: 379, Loss: 1.4929, Train 0.3267, Val 0.3906\n",
      "Fold: 4, Epoch: 380, Loss: 1.5501, Train 0.5223, Val 0.5625\n",
      "Fold: 4, Epoch: 381, Loss: 1.4588, Train 0.6906, Val 0.7031\n",
      "Fold: 4, Epoch: 382, Loss: 1.5803, Train 0.6782, Val 0.6875\n",
      "Fold: 4, Epoch: 383, Loss: 1.3583, Train 0.3218, Val 0.3906\n",
      "Fold: 4, Epoch: 384, Loss: 1.7362, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 385, Loss: 1.3459, Train 0.5149, Val 0.4688\n",
      "Fold: 4, Epoch: 386, Loss: 1.6905, Train 0.5842, Val 0.5469\n",
      "Fold: 4, Epoch: 387, Loss: 1.3619, Train 0.6733, Val 0.6719\n",
      "Fold: 4, Epoch: 388, Loss: 1.5872, Train 0.6955, Val 0.7031\n",
      "Fold: 4, Epoch: 389, Loss: 1.4322, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 390, Loss: 1.4672, Train 0.5941, Val 0.5625\n",
      "Fold: 4, Epoch: 391, Loss: 1.5540, Train 0.7030, Val 0.6875\n",
      "Fold: 4, Epoch: 392, Loss: 1.3135, Train 0.7178, Val 0.7344\n",
      "Fold: 4, Epoch: 393, Loss: 1.3955, Train 0.7153, Val 0.7500\n",
      "Fold: 4, Epoch: 394, Loss: 1.4248, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 395, Loss: 1.3147, Train 0.5173, Val 0.4688\n",
      "Fold: 4, Epoch: 396, Loss: 1.4051, Train 0.5173, Val 0.4688\n",
      "Fold: 4, Epoch: 397, Loss: 1.3179, Train 0.5495, Val 0.4844\n",
      "Fold: 4, Epoch: 398, Loss: 1.2964, Train 0.7030, Val 0.7031\n",
      "Fold: 4, Epoch: 399, Loss: 1.3548, Train 0.5198, Val 0.5469\n",
      "Fold: 4, Epoch: 400, Loss: 1.3012, Train 0.4257, Val 0.4688\n",
      "Fold: 4, Epoch: 401, Loss: 1.3296, Train 0.4233, Val 0.4688\n",
      "Fold: 4, Epoch: 402, Loss: 1.2605, Train 0.2723, Val 0.2500\n",
      "Fold: 4, Epoch: 403, Loss: 1.3455, Train 0.5149, Val 0.4688\n",
      "Fold: 4, Epoch: 404, Loss: 1.3144, Train 0.5149, Val 0.4688\n",
      "Fold: 4, Epoch: 405, Loss: 1.3721, Train 0.5792, Val 0.5156\n",
      "Fold: 4, Epoch: 406, Loss: 1.4904, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 407, Loss: 1.2839, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 408, Loss: 1.3206, Train 0.7030, Val 0.7500\n",
      "Fold: 4, Epoch: 409, Loss: 1.3817, Train 0.4282, Val 0.5000\n",
      "Fold: 4, Epoch: 410, Loss: 1.3354, Train 0.3416, Val 0.3438\n",
      "Fold: 4, Epoch: 411, Loss: 1.2697, Train 0.5173, Val 0.4688\n",
      "Fold: 4, Epoch: 412, Loss: 1.2427, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 413, Loss: 1.3993, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 414, Loss: 1.2661, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 415, Loss: 1.2944, Train 0.7005, Val 0.7031\n",
      "Fold: 4, Epoch: 416, Loss: 1.2908, Train 0.5371, Val 0.5625\n",
      "Fold: 4, Epoch: 417, Loss: 1.2339, Train 0.4282, Val 0.4688\n",
      "Fold: 4, Epoch: 418, Loss: 1.2587, Train 0.2574, Val 0.2500\n",
      "Fold: 4, Epoch: 419, Loss: 1.2978, Train 0.5297, Val 0.4844\n",
      "Fold: 4, Epoch: 420, Loss: 1.2582, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 421, Loss: 1.1893, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 422, Loss: 1.2742, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 423, Loss: 1.3709, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 424, Loss: 1.3688, Train 0.7030, Val 0.7031\n",
      "Fold: 4, Epoch: 425, Loss: 1.2273, Train 0.3168, Val 0.3125\n",
      "Fold: 4, Epoch: 426, Loss: 1.4385, Train 0.5644, Val 0.5469\n",
      "Fold: 4, Epoch: 427, Loss: 1.2571, Train 0.7104, Val 0.7031\n",
      "Fold: 4, Epoch: 428, Loss: 1.2196, Train 0.7005, Val 0.7031\n",
      "Fold: 4, Epoch: 429, Loss: 1.2493, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 430, Loss: 1.2478, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 431, Loss: 1.3218, Train 0.7005, Val 0.6875\n",
      "Fold: 4, Epoch: 432, Loss: 1.2143, Train 0.7153, Val 0.7656\n",
      "Fold: 4, Epoch: 433, Loss: 1.2093, Train 0.7203, Val 0.7344\n",
      "Fold: 4, Epoch: 434, Loss: 1.2068, Train 0.7129, Val 0.7500\n",
      "Fold: 4, Epoch: 435, Loss: 1.3645, Train 0.7005, Val 0.6875\n",
      "Fold: 4, Epoch: 436, Loss: 1.2064, Train 0.6955, Val 0.7031\n",
      "Fold: 4, Epoch: 437, Loss: 1.2881, Train 0.6955, Val 0.7031\n",
      "Fold: 4, Epoch: 438, Loss: 1.2765, Train 0.6980, Val 0.6875\n",
      "Fold: 4, Epoch: 439, Loss: 1.1810, Train 0.7475, Val 0.7656\n",
      "Fold: 4, Epoch: 440, Loss: 1.1851, Train 0.4431, Val 0.4688\n",
      "Fold: 4, Epoch: 441, Loss: 1.1784, Train 0.4876, Val 0.5156\n",
      "Fold: 4, Epoch: 442, Loss: 1.1221, Train 0.4431, Val 0.4688\n",
      "Fold: 4, Epoch: 443, Loss: 1.2071, Train 0.5792, Val 0.5469\n",
      "Fold: 4, Epoch: 444, Loss: 1.2664, Train 0.7079, Val 0.7031\n",
      "Fold: 4, Epoch: 445, Loss: 1.3147, Train 0.5446, Val 0.4844\n",
      "Fold: 4, Epoch: 446, Loss: 1.2421, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 447, Loss: 1.1991, Train 0.7104, Val 0.6875\n",
      "Fold: 4, Epoch: 448, Loss: 1.1982, Train 0.7178, Val 0.7344\n",
      "Fold: 4, Epoch: 449, Loss: 1.1912, Train 0.7203, Val 0.6719\n",
      "Fold: 4, Epoch: 450, Loss: 1.1860, Train 0.7450, Val 0.7188\n",
      "Fold: 4, Epoch: 451, Loss: 1.1722, Train 0.7550, Val 0.7500\n",
      "Fold: 4, Epoch: 452, Loss: 1.1661, Train 0.5248, Val 0.5156\n",
      "Fold: 4, Epoch: 453, Loss: 1.1724, Train 0.5198, Val 0.4688\n",
      "Fold: 4, Epoch: 454, Loss: 1.2510, Train 0.7376, Val 0.7656\n",
      "Fold: 4, Epoch: 455, Loss: 1.1294, Train 0.6906, Val 0.6719\n",
      "Fold: 4, Epoch: 456, Loss: 1.1946, Train 0.6757, Val 0.6719\n",
      "Fold: 4, Epoch: 457, Loss: 1.2463, Train 0.6955, Val 0.6719\n",
      "Fold: 4, Epoch: 458, Loss: 1.1701, Train 0.7450, Val 0.7656\n",
      "Fold: 4, Epoch: 459, Loss: 1.1475, Train 0.5396, Val 0.5000\n",
      "Fold: 4, Epoch: 460, Loss: 1.3099, Train 0.5644, Val 0.5156\n",
      "Fold: 4, Epoch: 461, Loss: 1.4009, Train 0.4678, Val 0.3594\n",
      "Fold: 4, Epoch: 462, Loss: 1.1027, Train 0.7079, Val 0.7344\n",
      "Fold: 4, Epoch: 463, Loss: 1.1565, Train 0.6906, Val 0.6719\n",
      "Fold: 4, Epoch: 464, Loss: 1.2246, Train 0.7054, Val 0.6719\n",
      "Fold: 4, Epoch: 465, Loss: 1.1614, Train 0.7327, Val 0.7344\n",
      "Fold: 4, Epoch: 466, Loss: 1.0860, Train 0.7104, Val 0.7188\n",
      "Fold: 4, Epoch: 467, Loss: 1.1154, Train 0.5272, Val 0.4844\n",
      "Fold: 4, Epoch: 468, Loss: 1.1562, Train 0.5470, Val 0.5312\n",
      "Fold: 4, Epoch: 469, Loss: 1.2245, Train 0.7723, Val 0.7500\n",
      "Fold: 4, Epoch: 470, Loss: 1.2690, Train 0.7772, Val 0.7500\n",
      "Fold: 4, Epoch: 471, Loss: 1.1888, Train 0.7649, Val 0.7500\n",
      "Fold: 4, Epoch: 472, Loss: 1.1230, Train 0.7574, Val 0.7500\n",
      "Fold: 4, Epoch: 473, Loss: 1.1446, Train 0.7723, Val 0.7656\n",
      "Fold: 4, Epoch: 474, Loss: 1.1676, Train 0.7723, Val 0.7500\n",
      "Fold: 4, Epoch: 475, Loss: 1.2067, Train 0.7327, Val 0.7031\n",
      "Fold: 4, Epoch: 476, Loss: 1.0868, Train 0.6955, Val 0.6719\n",
      "Fold: 4, Epoch: 477, Loss: 1.1277, Train 0.6856, Val 0.6719\n",
      "Fold: 4, Epoch: 478, Loss: 1.0807, Train 0.7054, Val 0.6719\n",
      "Fold: 4, Epoch: 479, Loss: 1.0726, Train 0.7673, Val 0.7500\n",
      "Fold: 4, Epoch: 480, Loss: 1.0142, Train 0.7673, Val 0.7500\n",
      "Fold: 4, Epoch: 481, Loss: 1.0317, Train 0.5743, Val 0.5000\n",
      "Fold: 4, Epoch: 482, Loss: 1.0756, Train 0.4901, Val 0.3750\n",
      "Fold: 4, Epoch: 483, Loss: 1.0482, Train 0.5124, Val 0.4219\n",
      "Fold: 4, Epoch: 484, Loss: 1.0459, Train 0.7847, Val 0.7656\n",
      "Fold: 4, Epoch: 485, Loss: 1.0405, Train 0.7450, Val 0.7188\n",
      "Fold: 4, Epoch: 486, Loss: 1.0526, Train 0.7030, Val 0.6719\n",
      "Fold: 4, Epoch: 487, Loss: 1.0121, Train 0.6931, Val 0.6719\n",
      "Fold: 4, Epoch: 488, Loss: 1.0036, Train 0.7252, Val 0.6875\n",
      "Fold: 4, Epoch: 489, Loss: 1.0413, Train 0.7624, Val 0.7344\n",
      "Fold: 4, Epoch: 490, Loss: 1.0597, Train 0.4851, Val 0.3594\n",
      "Fold: 4, Epoch: 491, Loss: 1.0001, Train 0.4777, Val 0.3594\n",
      "Fold: 4, Epoch: 492, Loss: 1.0782, Train 0.4876, Val 0.3594\n",
      "Fold: 4, Epoch: 493, Loss: 1.0728, Train 0.7797, Val 0.7500\n",
      "Fold: 4, Epoch: 494, Loss: 1.0117, Train 0.7054, Val 0.6719\n",
      "Fold: 4, Epoch: 495, Loss: 1.0674, Train 0.6807, Val 0.6719\n",
      "Fold: 4, Epoch: 496, Loss: 1.2413, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 497, Loss: 1.0278, Train 0.7797, Val 0.7656\n",
      "Fold: 4, Epoch: 498, Loss: 0.9710, Train 0.4926, Val 0.3750\n",
      "Fold: 4, Epoch: 499, Loss: 1.2294, Train 0.4975, Val 0.3750\n",
      "Fold: 4, Epoch: 500, Loss: 1.0868, Train 0.5025, Val 0.3750\n",
      "Fold: 4, Epoch: 501, Loss: 0.9881, Train 0.7153, Val 0.6562\n",
      "Fold: 4, Epoch: 502, Loss: 0.9767, Train 0.7673, Val 0.7500\n",
      "Fold: 4, Epoch: 503, Loss: 0.9985, Train 0.7599, Val 0.7500\n",
      "Fold: 4, Epoch: 504, Loss: 1.0514, Train 0.7748, Val 0.7500\n",
      "Fold: 4, Epoch: 505, Loss: 1.1071, Train 0.7673, Val 0.7656\n",
      "Fold: 4, Epoch: 506, Loss: 1.1297, Train 0.7426, Val 0.7344\n",
      "Fold: 4, Epoch: 507, Loss: 1.0387, Train 0.7871, Val 0.7500\n",
      "Fold: 4, Epoch: 508, Loss: 0.9083, Train 0.7723, Val 0.7500\n",
      "Fold: 4, Epoch: 509, Loss: 1.0495, Train 0.7450, Val 0.7031\n",
      "Fold: 4, Epoch: 510, Loss: 1.0907, Train 0.7748, Val 0.7656\n",
      "Fold: 4, Epoch: 511, Loss: 1.0565, Train 0.7847, Val 0.7656\n",
      "Fold: 4, Epoch: 512, Loss: 0.9544, Train 0.7649, Val 0.7656\n",
      "Fold: 4, Epoch: 513, Loss: 1.0519, Train 0.7178, Val 0.6875\n",
      "Fold: 4, Epoch: 514, Loss: 0.9993, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 515, Loss: 1.0103, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 516, Loss: 0.9463, Train 0.7153, Val 0.6875\n",
      "Fold: 4, Epoch: 517, Loss: 0.9941, Train 0.7450, Val 0.7188\n",
      "Fold: 4, Epoch: 518, Loss: 0.9901, Train 0.7624, Val 0.7656\n",
      "Fold: 4, Epoch: 519, Loss: 0.9098, Train 0.7599, Val 0.7500\n",
      "Fold: 4, Epoch: 520, Loss: 0.9402, Train 0.7450, Val 0.7188\n",
      "Fold: 4, Epoch: 521, Loss: 0.9253, Train 0.7178, Val 0.6875\n",
      "Fold: 4, Epoch: 522, Loss: 0.8573, Train 0.7178, Val 0.6875\n",
      "Fold: 4, Epoch: 523, Loss: 1.0380, Train 0.7723, Val 0.7656\n",
      "Fold: 4, Epoch: 524, Loss: 0.9601, Train 0.7970, Val 0.7656\n",
      "Fold: 4, Epoch: 525, Loss: 0.9586, Train 0.7970, Val 0.7656\n",
      "Fold: 4, Epoch: 526, Loss: 1.0659, Train 0.7946, Val 0.7656\n",
      "Fold: 4, Epoch: 527, Loss: 0.9536, Train 0.7946, Val 0.7656\n",
      "Fold: 4, Epoch: 528, Loss: 0.8904, Train 0.7847, Val 0.7656\n",
      "Fold: 4, Epoch: 529, Loss: 0.9264, Train 0.7426, Val 0.7188\n",
      "Fold: 4, Epoch: 530, Loss: 0.9510, Train 0.7129, Val 0.6875\n",
      "Fold: 4, Epoch: 531, Loss: 0.9594, Train 0.7698, Val 0.7656\n",
      "Fold: 4, Epoch: 532, Loss: 0.9811, Train 0.7921, Val 0.7656\n",
      "Fold: 4, Epoch: 533, Loss: 1.0450, Train 0.7921, Val 0.7656\n",
      "Fold: 4, Epoch: 534, Loss: 1.0542, Train 0.7748, Val 0.7656\n",
      "Fold: 4, Epoch: 535, Loss: 1.0255, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 536, Loss: 0.9500, Train 0.6782, Val 0.6719\n",
      "Fold: 4, Epoch: 537, Loss: 0.9684, Train 0.6757, Val 0.6719\n",
      "Fold: 4, Epoch: 538, Loss: 0.9515, Train 0.6782, Val 0.6719\n",
      "Fold: 4, Epoch: 539, Loss: 1.0511, Train 0.7129, Val 0.6875\n",
      "Fold: 4, Epoch: 540, Loss: 0.9300, Train 0.7698, Val 0.7656\n",
      "Fold: 4, Epoch: 541, Loss: 0.9442, Train 0.7054, Val 0.6875\n",
      "Fold: 4, Epoch: 542, Loss: 1.1033, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 543, Loss: 0.9203, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 544, Loss: 0.9713, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 545, Loss: 0.9019, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 546, Loss: 0.9179, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 547, Loss: 0.9638, Train 0.7203, Val 0.7031\n",
      "Fold: 4, Epoch: 548, Loss: 0.9428, Train 0.7772, Val 0.7500\n",
      "Fold: 4, Epoch: 549, Loss: 0.9359, Train 0.7871, Val 0.7656\n",
      "Fold: 4, Epoch: 550, Loss: 1.0305, Train 0.7228, Val 0.7031\n",
      "Fold: 4, Epoch: 551, Loss: 0.8990, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 552, Loss: 0.9556, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 553, Loss: 0.9339, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 554, Loss: 0.9553, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 555, Loss: 0.9139, Train 0.7525, Val 0.7188\n",
      "Fold: 4, Epoch: 556, Loss: 0.9432, Train 0.7748, Val 0.7188\n",
      "Fold: 4, Epoch: 557, Loss: 0.9482, Train 0.7896, Val 0.7656\n",
      "Fold: 4, Epoch: 558, Loss: 0.9757, Train 0.7772, Val 0.7500\n",
      "Fold: 4, Epoch: 559, Loss: 0.8705, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 560, Loss: 0.9179, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 561, Loss: 0.9603, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 562, Loss: 0.9066, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 563, Loss: 0.8525, Train 0.7030, Val 0.6875\n",
      "Fold: 4, Epoch: 564, Loss: 0.8858, Train 0.7228, Val 0.7188\n",
      "Fold: 4, Epoch: 565, Loss: 0.9577, Train 0.6559, Val 0.6250\n",
      "Fold: 4, Epoch: 566, Loss: 0.9674, Train 0.7252, Val 0.7188\n",
      "Fold: 4, Epoch: 567, Loss: 0.9727, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 568, Loss: 0.8509, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 569, Loss: 0.8856, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 570, Loss: 0.8634, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 571, Loss: 0.8426, Train 0.7327, Val 0.7031\n",
      "Fold: 4, Epoch: 572, Loss: 0.9298, Train 0.7599, Val 0.7344\n",
      "Fold: 4, Epoch: 573, Loss: 0.9039, Train 0.7153, Val 0.7031\n",
      "Fold: 4, Epoch: 574, Loss: 0.8150, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 575, Loss: 0.9430, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 576, Loss: 0.7922, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 577, Loss: 0.8560, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 578, Loss: 0.9663, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 579, Loss: 0.9091, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 580, Loss: 0.9124, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 581, Loss: 0.8949, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 582, Loss: 0.9180, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 583, Loss: 0.8950, Train 0.7079, Val 0.7031\n",
      "Fold: 4, Epoch: 584, Loss: 0.8653, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 585, Loss: 0.9154, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 586, Loss: 0.8530, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 587, Loss: 0.9013, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 588, Loss: 0.9260, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 589, Loss: 0.9610, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 590, Loss: 0.9095, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 591, Loss: 0.8482, Train 0.7450, Val 0.7344\n",
      "Fold: 4, Epoch: 592, Loss: 1.0114, Train 0.7525, Val 0.7500\n",
      "Fold: 4, Epoch: 593, Loss: 0.8458, Train 0.7426, Val 0.7188\n",
      "Fold: 4, Epoch: 594, Loss: 0.9162, Train 0.7450, Val 0.7188\n",
      "Fold: 4, Epoch: 595, Loss: 0.9177, Train 0.7104, Val 0.7031\n",
      "Fold: 4, Epoch: 596, Loss: 0.8479, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 597, Loss: 0.9037, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 598, Loss: 0.8936, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 599, Loss: 0.9259, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 600, Loss: 0.9096, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 601, Loss: 1.0027, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 602, Loss: 0.9241, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 603, Loss: 0.8085, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 604, Loss: 0.8482, Train 0.6955, Val 0.7031\n",
      "Fold: 4, Epoch: 605, Loss: 0.8360, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 606, Loss: 0.9322, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 607, Loss: 0.9039, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 608, Loss: 0.8343, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 609, Loss: 1.0563, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 610, Loss: 0.8411, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 611, Loss: 0.8620, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 612, Loss: 0.8939, Train 0.6807, Val 0.6719\n",
      "Fold: 4, Epoch: 613, Loss: 0.9997, Train 0.6782, Val 0.6719\n",
      "Fold: 4, Epoch: 614, Loss: 1.0395, Train 0.6832, Val 0.6719\n",
      "Fold: 4, Epoch: 615, Loss: 0.9240, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 616, Loss: 0.8961, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 617, Loss: 0.8433, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 618, Loss: 0.9272, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 619, Loss: 0.8446, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 620, Loss: 0.9014, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 621, Loss: 0.8340, Train 0.7351, Val 0.7031\n",
      "Fold: 4, Epoch: 622, Loss: 0.9299, Train 0.7129, Val 0.6875\n",
      "Fold: 4, Epoch: 623, Loss: 0.8930, Train 0.6832, Val 0.6719\n",
      "Fold: 4, Epoch: 624, Loss: 0.9910, Train 0.6832, Val 0.6719\n",
      "Fold: 4, Epoch: 625, Loss: 0.8509, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 626, Loss: 0.8700, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 627, Loss: 0.8967, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 628, Loss: 0.9028, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 629, Loss: 0.9012, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 630, Loss: 0.7826, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 631, Loss: 0.8455, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 632, Loss: 0.8452, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 633, Loss: 0.8691, Train 0.6807, Val 0.6719\n",
      "Fold: 4, Epoch: 634, Loss: 0.9089, Train 0.6807, Val 0.6719\n",
      "Fold: 4, Epoch: 635, Loss: 0.8673, Train 0.6832, Val 0.6719\n",
      "Fold: 4, Epoch: 636, Loss: 0.8696, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 637, Loss: 0.9219, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 638, Loss: 0.8634, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 639, Loss: 0.8976, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 640, Loss: 0.8483, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 641, Loss: 0.8209, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 642, Loss: 0.7620, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 643, Loss: 0.9500, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 644, Loss: 0.8629, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 645, Loss: 0.7810, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 646, Loss: 0.8210, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 647, Loss: 0.8945, Train 0.7054, Val 0.7031\n",
      "Fold: 4, Epoch: 648, Loss: 0.9666, Train 0.7327, Val 0.7031\n",
      "Fold: 4, Epoch: 649, Loss: 0.9110, Train 0.7475, Val 0.7188\n",
      "Fold: 4, Epoch: 650, Loss: 0.8945, Train 0.5743, Val 0.5156\n",
      "Fold: 4, Epoch: 651, Loss: 0.8883, Train 0.4653, Val 0.4062\n",
      "Fold: 4, Epoch: 652, Loss: 0.8333, Train 0.4035, Val 0.3125\n",
      "Fold: 4, Epoch: 653, Loss: 0.8748, Train 0.5248, Val 0.4688\n",
      "Fold: 4, Epoch: 654, Loss: 0.8661, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 655, Loss: 0.8324, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 656, Loss: 0.8399, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 657, Loss: 0.8540, Train 0.7054, Val 0.7031\n",
      "Fold: 4, Epoch: 658, Loss: 0.8342, Train 0.7104, Val 0.7031\n",
      "Fold: 4, Epoch: 659, Loss: 0.8467, Train 0.4480, Val 0.3594\n",
      "Fold: 4, Epoch: 660, Loss: 0.8331, Train 0.6064, Val 0.5781\n",
      "Fold: 4, Epoch: 661, Loss: 0.8433, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 662, Loss: 0.7877, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 663, Loss: 0.8539, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 664, Loss: 0.8197, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 665, Loss: 0.8220, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 666, Loss: 0.8530, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 667, Loss: 0.7765, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 668, Loss: 0.8655, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 669, Loss: 0.8379, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 670, Loss: 0.8979, Train 0.6807, Val 0.6719\n",
      "Fold: 4, Epoch: 671, Loss: 0.9995, Train 0.6807, Val 0.6719\n",
      "Fold: 4, Epoch: 672, Loss: 0.9566, Train 0.6807, Val 0.6719\n",
      "Fold: 4, Epoch: 673, Loss: 0.8668, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 674, Loss: 0.8315, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 675, Loss: 0.8895, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 676, Loss: 0.8390, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 677, Loss: 0.9481, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 678, Loss: 0.8313, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 679, Loss: 0.8430, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 680, Loss: 0.9860, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 681, Loss: 0.7987, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 682, Loss: 0.8166, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 683, Loss: 0.8104, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 684, Loss: 0.8523, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 685, Loss: 0.9058, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 686, Loss: 0.9260, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 687, Loss: 0.9069, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 688, Loss: 0.8288, Train 0.6708, Val 0.6562\n",
      "Fold: 4, Epoch: 689, Loss: 0.9095, Train 0.5668, Val 0.4844\n",
      "Fold: 4, Epoch: 690, Loss: 0.8524, Train 0.6287, Val 0.5781\n",
      "Fold: 4, Epoch: 691, Loss: 0.9999, Train 0.7005, Val 0.7031\n",
      "Fold: 4, Epoch: 692, Loss: 0.8249, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 693, Loss: 0.7735, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 694, Loss: 0.8825, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 695, Loss: 0.8093, Train 0.7030, Val 0.7031\n",
      "Fold: 4, Epoch: 696, Loss: 0.8389, Train 0.7079, Val 0.7031\n",
      "Fold: 4, Epoch: 697, Loss: 0.8472, Train 0.6955, Val 0.7031\n",
      "Fold: 4, Epoch: 698, Loss: 0.8427, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 699, Loss: 0.8004, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 700, Loss: 0.7808, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 701, Loss: 0.8747, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 702, Loss: 0.8584, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 703, Loss: 0.8539, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 704, Loss: 0.7722, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 705, Loss: 0.8206, Train 0.6980, Val 0.7031\n",
      "Fold: 4, Epoch: 706, Loss: 0.8473, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 707, Loss: 0.8580, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 708, Loss: 0.7882, Train 0.6856, Val 0.6875\n",
      "Fold: 4, Epoch: 709, Loss: 0.8130, Train 0.6881, Val 0.6875\n",
      "Fold: 4, Epoch: 710, Loss: 0.8826, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 711, Loss: 0.7695, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 712, Loss: 0.8047, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 713, Loss: 0.8099, Train 0.7079, Val 0.6875\n",
      "Fold: 4, Epoch: 714, Loss: 0.7482, Train 0.4752, Val 0.3750\n",
      "Fold: 4, Epoch: 715, Loss: 0.8158, Train 0.7129, Val 0.7031\n",
      "Fold: 4, Epoch: 716, Loss: 0.8291, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 717, Loss: 0.8472, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 718, Loss: 0.7389, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 719, Loss: 0.8242, Train 0.6931, Val 0.6875\n",
      "Fold: 4, Epoch: 720, Loss: 0.7693, Train 0.6906, Val 0.6875\n",
      "Fold: 4, Epoch: 721, Loss: 0.7272, Train 0.7054, Val 0.6875\n",
      "Fold: 4, Epoch: 722, Loss: 0.8326, Train 0.7178, Val 0.7031\n",
      "Fold: 4, Epoch: 723, Loss: 0.7240, Train 0.7871, Val 0.7656\n",
      "Fold: 4, Epoch: 724, Loss: 0.7128, Train 0.7896, Val 0.7656\n",
      "Fold: 4, Epoch: 725, Loss: 0.7261, Train 0.7847, Val 0.7500\n",
      "Fold: 4, Epoch: 726, Loss: 0.7770, Train 0.6559, Val 0.5938\n",
      "Fold: 4, Epoch: 727, Loss: 0.7418, Train 0.5495, Val 0.4219\n",
      "Fold: 4, Epoch: 728, Loss: 0.8148, Train 0.4554, Val 0.3594\n",
      "Fold: 4, Epoch: 729, Loss: 0.7558, Train 0.4505, Val 0.3594\n",
      "Fold: 4, Epoch: 730, Loss: 0.7691, Train 0.4381, Val 0.3438\n",
      "Fold: 4, Epoch: 731, Loss: 0.7681, Train 0.4653, Val 0.3594\n",
      "Fold: 4, Epoch: 732, Loss: 0.7713, Train 0.5223, Val 0.4219\n",
      "Fold: 4, Epoch: 733, Loss: 0.7626, Train 0.7574, Val 0.6875\n",
      "Fold: 4, Epoch: 734, Loss: 0.7364, Train 0.7475, Val 0.7500\n",
      "Fold: 4, Epoch: 735, Loss: 0.7847, Train 0.7079, Val 0.7031\n",
      "Fold: 4, Epoch: 736, Loss: 0.7566, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 737, Loss: 0.7367, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 738, Loss: 0.7388, Train 0.7129, Val 0.7031\n",
      "Fold: 4, Epoch: 739, Loss: 0.7575, Train 0.7228, Val 0.6562\n",
      "Fold: 4, Epoch: 740, Loss: 0.6390, Train 0.6955, Val 0.6719\n",
      "Fold: 4, Epoch: 741, Loss: 0.7159, Train 0.6559, Val 0.6250\n",
      "Fold: 4, Epoch: 742, Loss: 0.6948, Train 0.7426, Val 0.7031\n",
      "Fold: 4, Epoch: 743, Loss: 0.7215, Train 0.6955, Val 0.7031\n",
      "Fold: 4, Epoch: 744, Loss: 0.6847, Train 0.6955, Val 0.6875\n",
      "Fold: 4, Epoch: 745, Loss: 0.6953, Train 0.6955, Val 0.7031\n",
      "Fold: 4, Epoch: 746, Loss: 0.6486, Train 0.7104, Val 0.7188\n",
      "Fold: 4, Epoch: 747, Loss: 0.6721, Train 0.7698, Val 0.7500\n",
      "Fold: 4, Epoch: 748, Loss: 0.6749, Train 0.7104, Val 0.6562\n",
      "Fold: 4, Epoch: 749, Loss: 0.6802, Train 0.6634, Val 0.6562\n",
      "Fold: 4, Epoch: 750, Loss: 0.6689, Train 0.6733, Val 0.5938\n",
      "Fold: 4, Epoch: 751, Loss: 0.7047, Train 0.6931, Val 0.6406\n",
      "Fold: 4, Epoch: 752, Loss: 0.5998, Train 0.8317, Val 0.7656\n",
      "Fold: 4, Epoch: 753, Loss: 0.6324, Train 0.8861, Val 0.8281\n",
      "Fold: 4, Epoch: 754, Loss: 0.6525, Train 0.8416, Val 0.8281\n",
      "Fold: 4, Epoch: 755, Loss: 0.5571, Train 0.7673, Val 0.7344\n",
      "Fold: 4, Epoch: 756, Loss: 0.6182, Train 0.7376, Val 0.6562\n",
      "Fold: 4, Epoch: 757, Loss: 0.6075, Train 0.7203, Val 0.5938\n",
      "Fold: 4, Epoch: 758, Loss: 0.6546, Train 0.6708, Val 0.6094\n",
      "Fold: 4, Epoch: 759, Loss: 0.5658, Train 0.6733, Val 0.6094\n",
      "Fold: 4, Epoch: 760, Loss: 0.5245, Train 0.6609, Val 0.5938\n",
      "Fold: 4, Epoch: 761, Loss: 0.6856, Train 0.5520, Val 0.4375\n",
      "Fold: 4, Epoch: 762, Loss: 0.6490, Train 0.6757, Val 0.5781\n",
      "Fold: 4, Epoch: 763, Loss: 0.5013, Train 0.8168, Val 0.7344\n",
      "Fold: 4, Epoch: 764, Loss: 0.6635, Train 0.8985, Val 0.8750\n",
      "Fold: 4, Epoch: 765, Loss: 0.5625, Train 0.8762, Val 0.8281\n",
      "Fold: 4, Epoch: 766, Loss: 0.4691, Train 0.8564, Val 0.8125\n",
      "Fold: 4, Epoch: 767, Loss: 0.5924, Train 0.8589, Val 0.8438\n",
      "Fold: 4, Epoch: 768, Loss: 0.5719, Train 0.8738, Val 0.8906\n",
      "Fold: 4, Epoch: 769, Loss: 0.6114, Train 0.8465, Val 0.8750\n",
      "Fold: 4, Epoch: 770, Loss: 0.5611, Train 0.8688, Val 0.8906\n",
      "Fold: 4, Epoch: 771, Loss: 0.5971, Train 0.8985, Val 0.9062\n",
      "Fold: 4, Epoch: 772, Loss: 0.4635, Train 0.8985, Val 0.9062\n",
      "Fold: 4, Epoch: 773, Loss: 0.6000, Train 0.9010, Val 0.9062\n",
      "Fold: 4, Epoch: 774, Loss: 0.5621, Train 0.8812, Val 0.8906\n",
      "Fold: 4, Epoch: 775, Loss: 0.4581, Train 0.8366, Val 0.8750\n",
      "Fold: 4, Epoch: 776, Loss: 0.5154, Train 0.8267, Val 0.8750\n",
      "Fold: 4, Epoch: 777, Loss: 0.5548, Train 0.8465, Val 0.8438\n",
      "Fold: 4, Epoch: 778, Loss: 0.5086, Train 0.8762, Val 0.9062\n",
      "Fold: 4, Epoch: 779, Loss: 0.5078, Train 0.8812, Val 0.9219\n",
      "Fold: 4, Epoch: 780, Loss: 0.6063, Train 0.9010, Val 0.9375\n",
      "Fold: 4, Epoch: 781, Loss: 0.4245, Train 0.9010, Val 0.9375\n",
      "Fold: 4, Epoch: 782, Loss: 0.4305, Train 0.9010, Val 0.9375\n",
      "Fold: 4, Epoch: 783, Loss: 0.4665, Train 0.9059, Val 0.9531\n",
      "Fold: 4, Epoch: 784, Loss: 0.5269, Train 0.9035, Val 0.9219\n",
      "Fold: 4, Epoch: 785, Loss: 0.5522, Train 0.8886, Val 0.8906\n",
      "Fold: 4, Epoch: 786, Loss: 0.4026, Train 0.8515, Val 0.8750\n",
      "Fold: 4, Epoch: 787, Loss: 0.5108, Train 0.8144, Val 0.8438\n",
      "Fold: 4, Epoch: 788, Loss: 0.6300, Train 0.8589, Val 0.8750\n",
      "Fold: 4, Epoch: 789, Loss: 0.5760, Train 0.8911, Val 0.9062\n",
      "Fold: 4, Epoch: 790, Loss: 0.5946, Train 0.8960, Val 0.9062\n",
      "Fold: 4, Epoch: 791, Loss: 0.5702, Train 0.8639, Val 0.8594\n",
      "Fold: 4, Epoch: 792, Loss: 0.5243, Train 0.7104, Val 0.5781\n",
      "Fold: 4, Epoch: 793, Loss: 0.4655, Train 0.6238, Val 0.5469\n",
      "Fold: 4, Epoch: 794, Loss: 0.4976, Train 0.6188, Val 0.5469\n",
      "Fold: 4, Epoch: 795, Loss: 0.6256, Train 0.6213, Val 0.5469\n",
      "Fold: 4, Epoch: 796, Loss: 0.6106, Train 0.7376, Val 0.6875\n",
      "Fold: 4, Epoch: 797, Loss: 0.5639, Train 0.7475, Val 0.7188\n",
      "Fold: 4, Epoch: 798, Loss: 0.4563, Train 0.7599, Val 0.7812\n",
      "Fold: 4, Epoch: 799, Loss: 0.5372, Train 0.7995, Val 0.8438\n",
      "Fold: 4, Epoch: 800, Loss: 0.5885, Train 0.8020, Val 0.8594\n",
      "Fold: 4, Epoch: 801, Loss: 0.4894, Train 0.8317, Val 0.8750\n",
      "Fold: 4, Epoch: 802, Loss: 0.5704, Train 0.8960, Val 0.9219\n",
      "Fold: 4, Epoch: 803, Loss: 0.5522, Train 0.9010, Val 0.9062\n",
      "Fold: 4, Epoch: 804, Loss: 0.5636, Train 0.9059, Val 0.9219\n",
      "Fold: 4, Epoch: 805, Loss: 0.4624, Train 0.9084, Val 0.9375\n",
      "Fold: 4, Epoch: 806, Loss: 0.6501, Train 0.8515, Val 0.8594\n",
      "Fold: 4, Epoch: 807, Loss: 0.4456, Train 0.7500, Val 0.7344\n",
      "Fold: 4, Epoch: 808, Loss: 0.4450, Train 0.7252, Val 0.7500\n",
      "Fold: 4, Epoch: 809, Loss: 0.4538, Train 0.7748, Val 0.8281\n",
      "Fold: 4, Epoch: 810, Loss: 0.5823, Train 0.8094, Val 0.8594\n",
      "Fold: 4, Epoch: 811, Loss: 0.5450, Train 0.8366, Val 0.8594\n",
      "Fold: 4, Epoch: 812, Loss: 0.4575, Train 0.8663, Val 0.8125\n",
      "Fold: 4, Epoch: 813, Loss: 0.6396, Train 0.9134, Val 0.9531\n",
      "Fold: 4, Epoch: 814, Loss: 0.4674, Train 0.7129, Val 0.6875\n",
      "Fold: 4, Epoch: 815, Loss: 0.4358, Train 0.6337, Val 0.5625\n",
      "Fold: 4, Epoch: 816, Loss: 0.4834, Train 0.6733, Val 0.6094\n",
      "Fold: 4, Epoch: 817, Loss: 0.5895, Train 0.8614, Val 0.8125\n",
      "Fold: 4, Epoch: 818, Loss: 0.4988, Train 0.8985, Val 0.8906\n",
      "Fold: 4, Epoch: 819, Loss: 0.4611, Train 0.8936, Val 0.8906\n",
      "Fold: 4, Epoch: 820, Loss: 0.4263, Train 0.8614, Val 0.8750\n",
      "Fold: 4, Epoch: 821, Loss: 0.4321, Train 0.8366, Val 0.8438\n",
      "Fold: 4, Epoch: 822, Loss: 0.4347, Train 0.8391, Val 0.8750\n",
      "Fold: 4, Epoch: 823, Loss: 0.4455, Train 0.8614, Val 0.8906\n",
      "Fold: 4, Epoch: 824, Loss: 0.4392, Train 0.9109, Val 0.9844\n",
      "Fold: 4, Epoch: 825, Loss: 0.4493, Train 0.9282, Val 0.9688\n",
      "Fold: 4, Epoch: 826, Loss: 0.4032, Train 0.9035, Val 0.9375\n",
      "Fold: 4, Epoch: 827, Loss: 0.3900, Train 0.8639, Val 0.8750\n",
      "Fold: 4, Epoch: 828, Loss: 0.4761, Train 0.7401, Val 0.7188\n",
      "Fold: 4, Epoch: 829, Loss: 0.5318, Train 0.8762, Val 0.9062\n",
      "Fold: 4, Epoch: 830, Loss: 0.4700, Train 0.9134, Val 0.9375\n",
      "Fold: 4, Epoch: 831, Loss: 0.4149, Train 0.9134, Val 0.9219\n",
      "Fold: 4, Epoch: 832, Loss: 0.4180, Train 0.8837, Val 0.8750\n",
      "Fold: 4, Epoch: 833, Loss: 0.5539, Train 0.8490, Val 0.8594\n",
      "Fold: 4, Epoch: 834, Loss: 0.5233, Train 0.8589, Val 0.8750\n",
      "Fold: 4, Epoch: 835, Loss: 0.4224, Train 0.9059, Val 0.9219\n",
      "Fold: 4, Epoch: 836, Loss: 0.3774, Train 0.9257, Val 0.9375\n",
      "Fold: 4, Epoch: 837, Loss: 0.4514, Train 0.9059, Val 0.9062\n",
      "Fold: 4, Epoch: 838, Loss: 0.4312, Train 0.8515, Val 0.8594\n",
      "Fold: 4, Epoch: 839, Loss: 0.4204, Train 0.7946, Val 0.7656\n",
      "Fold: 4, Epoch: 840, Loss: 0.4086, Train 0.7871, Val 0.7500\n",
      "Fold: 4, Epoch: 841, Loss: 0.4799, Train 0.8119, Val 0.8125\n",
      "Fold: 4, Epoch: 842, Loss: 0.4281, Train 0.7946, Val 0.7812\n",
      "Fold: 4, Epoch: 843, Loss: 0.4491, Train 0.8094, Val 0.8438\n",
      "Fold: 4, Epoch: 844, Loss: 0.3387, Train 0.7871, Val 0.8281\n",
      "Fold: 4, Epoch: 845, Loss: 0.4561, Train 0.7500, Val 0.7656\n",
      "Fold: 4, Epoch: 846, Loss: 0.4472, Train 0.8069, Val 0.7969\n",
      "Fold: 4, Epoch: 847, Loss: 0.5522, Train 0.9059, Val 0.9219\n",
      "Fold: 4, Epoch: 848, Loss: 0.4093, Train 0.9183, Val 0.9062\n",
      "Fold: 4, Epoch: 849, Loss: 0.4310, Train 0.9134, Val 0.9062\n",
      "Fold: 4, Epoch: 850, Loss: 0.4287, Train 0.9109, Val 0.9062\n",
      "Fold: 4, Epoch: 851, Loss: 0.4288, Train 0.9134, Val 0.9375\n",
      "Fold: 4, Epoch: 852, Loss: 0.4790, Train 0.9183, Val 0.9375\n",
      "Fold: 4, Epoch: 853, Loss: 0.4450, Train 0.9158, Val 0.9375\n",
      "Fold: 4, Epoch: 854, Loss: 0.5093, Train 0.9208, Val 0.9375\n",
      "Fold: 4, Epoch: 855, Loss: 0.3776, Train 0.9158, Val 0.9375\n",
      "Fold: 4, Epoch: 856, Loss: 0.4189, Train 0.9059, Val 0.9375\n",
      "Fold: 4, Epoch: 857, Loss: 0.4476, Train 0.8936, Val 0.9219\n",
      "Fold: 4, Epoch: 858, Loss: 0.4581, Train 0.8762, Val 0.9062\n",
      "Fold: 4, Epoch: 859, Loss: 0.4613, Train 0.8490, Val 0.8906\n",
      "Fold: 4, Epoch: 860, Loss: 0.3766, Train 0.8639, Val 0.9062\n",
      "Fold: 4, Epoch: 861, Loss: 0.3967, Train 0.8960, Val 0.9219\n",
      "Fold: 4, Epoch: 862, Loss: 0.3750, Train 0.9035, Val 0.9219\n",
      "Fold: 4, Epoch: 863, Loss: 0.3673, Train 0.9010, Val 0.9219\n",
      "Fold: 4, Epoch: 864, Loss: 0.4154, Train 0.8688, Val 0.8594\n",
      "Fold: 4, Epoch: 865, Loss: 0.4078, Train 0.8243, Val 0.8125\n",
      "Fold: 4, Epoch: 866, Loss: 0.3281, Train 0.7376, Val 0.6875\n",
      "Fold: 4, Epoch: 867, Loss: 0.4360, Train 0.7054, Val 0.6875\n",
      "Fold: 4, Epoch: 868, Loss: 0.3836, Train 0.6733, Val 0.6406\n",
      "Fold: 4, Epoch: 869, Loss: 0.3847, Train 0.6634, Val 0.6406\n",
      "Fold: 4, Epoch: 870, Loss: 0.4317, Train 0.6485, Val 0.5938\n",
      "Fold: 4, Epoch: 871, Loss: 0.4608, Train 0.6584, Val 0.6250\n",
      "Fold: 4, Epoch: 872, Loss: 0.4169, Train 0.7030, Val 0.6875\n",
      "Fold: 4, Epoch: 873, Loss: 0.4055, Train 0.7624, Val 0.7812\n",
      "Fold: 4, Epoch: 874, Loss: 0.3655, Train 0.8564, Val 0.8438\n",
      "Fold: 4, Epoch: 875, Loss: 0.4859, Train 0.9233, Val 0.9531\n",
      "Fold: 4, Epoch: 876, Loss: 0.3577, Train 0.9208, Val 0.9531\n",
      "Fold: 4, Epoch: 877, Loss: 0.3971, Train 0.8837, Val 0.9219\n",
      "Fold: 4, Epoch: 878, Loss: 0.4323, Train 0.8317, Val 0.8750\n",
      "Fold: 4, Epoch: 879, Loss: 0.3701, Train 0.8094, Val 0.8594\n",
      "Fold: 4, Epoch: 880, Loss: 0.4512, Train 0.8094, Val 0.8594\n",
      "Fold: 4, Epoch: 881, Loss: 0.4231, Train 0.8243, Val 0.8594\n",
      "Fold: 4, Epoch: 882, Loss: 0.4886, Train 0.8861, Val 0.8750\n",
      "Fold: 4, Epoch: 883, Loss: 0.4519, Train 0.9183, Val 0.9062\n",
      "Fold: 4, Epoch: 884, Loss: 0.3642, Train 0.8985, Val 0.8750\n",
      "Fold: 4, Epoch: 885, Loss: 0.3508, Train 0.8812, Val 0.8438\n",
      "Fold: 4, Epoch: 886, Loss: 0.3321, Train 0.8243, Val 0.7969\n",
      "Fold: 4, Epoch: 887, Loss: 0.4056, Train 0.7921, Val 0.7344\n",
      "Fold: 4, Epoch: 888, Loss: 0.4493, Train 0.8168, Val 0.7969\n",
      "Fold: 4, Epoch: 889, Loss: 0.3851, Train 0.8762, Val 0.8750\n",
      "Fold: 4, Epoch: 890, Loss: 0.4130, Train 0.9134, Val 0.9531\n",
      "Fold: 4, Epoch: 891, Loss: 0.4870, Train 0.9307, Val 0.9688\n",
      "Fold: 4, Epoch: 892, Loss: 0.3634, Train 0.9381, Val 0.9844\n",
      "Fold: 4, Epoch: 893, Loss: 0.3134, Train 0.9059, Val 0.9531\n",
      "Fold: 4, Epoch: 894, Loss: 0.3518, Train 0.8564, Val 0.8906\n",
      "Fold: 4, Epoch: 895, Loss: 0.4580, Train 0.8218, Val 0.8594\n",
      "Fold: 4, Epoch: 896, Loss: 0.4355, Train 0.8391, Val 0.8594\n",
      "Fold: 4, Epoch: 897, Loss: 0.3976, Train 0.8317, Val 0.8281\n",
      "Fold: 4, Epoch: 898, Loss: 0.4734, Train 0.8861, Val 0.9219\n",
      "Fold: 4, Epoch: 899, Loss: 0.3710, Train 0.9233, Val 0.9375\n",
      "Fold: 4, Epoch: 900, Loss: 0.4824, Train 0.9134, Val 0.9375\n",
      "Fold: 4, Epoch: 901, Loss: 0.3784, Train 0.9109, Val 0.9375\n",
      "Fold: 4, Epoch: 902, Loss: 0.4551, Train 0.9109, Val 0.9375\n",
      "Fold: 4, Epoch: 903, Loss: 0.3444, Train 0.9084, Val 0.9375\n",
      "Fold: 4, Epoch: 904, Loss: 0.3605, Train 0.8936, Val 0.9219\n",
      "Fold: 4, Epoch: 905, Loss: 0.3501, Train 0.8490, Val 0.8438\n",
      "Fold: 4, Epoch: 906, Loss: 0.3574, Train 0.8342, Val 0.8125\n",
      "Fold: 4, Epoch: 907, Loss: 0.3444, Train 0.8391, Val 0.8125\n",
      "Fold: 4, Epoch: 908, Loss: 0.3685, Train 0.8540, Val 0.8750\n",
      "Fold: 4, Epoch: 909, Loss: 0.4220, Train 0.8911, Val 0.9219\n",
      "Fold: 4, Epoch: 910, Loss: 0.3426, Train 0.9035, Val 0.9219\n",
      "Fold: 4, Epoch: 911, Loss: 0.4116, Train 0.9233, Val 0.9375\n",
      "Fold: 4, Epoch: 912, Loss: 0.4670, Train 0.9233, Val 0.9375\n",
      "Fold: 4, Epoch: 913, Loss: 0.4251, Train 0.9307, Val 0.9531\n",
      "Fold: 4, Epoch: 914, Loss: 0.3147, Train 0.9183, Val 0.9375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads, data):\n",
    "        super(GATv2, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GATv2Conv(data.num_features, hidden_channels, heads=heads, edge_dim=1))\n",
    "        self.convs.append(GATv2Conv(hidden_channels * heads, data.num_classes, edge_dim=1))\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = F.dropout(x, p=0.6, training=self.training)\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "def weighted_cross_entropy_loss(output, target, weights):\n",
    "    loss = F.cross_entropy(output, target, reduction='none')\n",
    "    weighted_loss = loss * weights[target]\n",
    "    return weighted_loss.mean()\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = weighted_cross_entropy_loss(out[data.train_mask], data.y[data.train_mask], data.weights[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(model, data, mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data.y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc, pred[mask]\n",
    "\n",
    "def dinstinction(data):\n",
    "    train_val_indexes = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "    for i in range(len(train_val_indexes)):\n",
    "        if data.test_mask[i]:\n",
    "            train_val_indexes[i] = False\n",
    "\n",
    "    X=data.x[train_val_indexes]\n",
    "    Y=data.y[train_val_indexes]\n",
    "    return(X,Y)\n",
    "\n",
    "def objective(trial, data = train_val_data, test_data = test_data):\n",
    "    # Hyperparameters to be optimized\n",
    "    hidden_channels = trial.suggest_int('hidden_channels', 16, 30)\n",
    "    heads = trial.suggest_int('heads', 1, 16)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "    all_test_acc = []\n",
    "    \n",
    "    data_X_for_Val, data_Y_for_val  = dinstinction(data)\n",
    "\n",
    "    for fold, (_, val_index) in enumerate(skf.split(data_X_for_Val, data_Y_for_val)):\n",
    "\n",
    "        train_index = [i for i in range(data.num_nodes)]\n",
    "        data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.train_mask[train_index] = True\n",
    "        \n",
    "        data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.val_mask[val_index] = True\n",
    "        \n",
    "        # Initialize model, optimizer, and loss function\n",
    "        model = GATv2(hidden_channels=hidden_channels, heads=heads, data=data)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1, 1000):\n",
    "            loss = train(model, data, optimizer)\n",
    "            val_acc, _ = test(model, data, data.val_mask)\n",
    "            train_acc, _ = test(model, data, data.train_mask)\n",
    "            print(f'Fold: {fold + 1}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Train {train_acc:.4f}, Val {val_acc:.4f}')\n",
    "\n",
    "        # Evaluate on test set   \n",
    "        test_mask = torch.ones(test_data.num_nodes, dtype=torch.bool)\n",
    "        test_acc, _ = test(model, test_data, test_mask)\n",
    "\n",
    "        # Store results\n",
    "        all_test_acc.append(test_acc)\n",
    "\n",
    "    # Calculate mean test accuracy for all folds\n",
    "    mean_test_acc = np.mean(all_test_acc)\n",
    "    storage[trial.number] = {\"values\": mean_test_acc, \"heads\": heads, \"hidden_channels\": hidden_channels}\n",
    "    return mean_test_acc\n",
    "\n",
    "storage = {}\n",
    "\n",
    "# Run the optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "print(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# PrÃ©paration des donnÃ©es pour le plot en 3D\n",
    "heads = [storage[i]['heads'] for i in storage]\n",
    "hidden_channels = [storage[i]['hidden_channels'] for i in storage]\n",
    "values = [storage[i]['values'] for i in storage]\n",
    "\n",
    "# CrÃ©ation du graphique en 3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(heads, hidden_channels, values, c=values, cmap='viridis', marker='o')\n",
    "\n",
    "ax.set_xlabel('Heads')\n",
    "ax.set_ylabel('Hidden Channels')\n",
    "ax.set_zlabel('Value')\n",
    "\n",
    "plt.colorbar(sc, label='Value')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
