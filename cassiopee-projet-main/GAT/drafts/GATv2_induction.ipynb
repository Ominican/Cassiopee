{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>class</th>\n",
       "      <th>class_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-2F-A9KO</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-2F-A9KP</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-2F-A9KQ</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-2F-A9KR</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-2F-A9KT</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>TCGA-ZF-AA56</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>TCGA-ZF-AA58</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>TCGA-ZF-AA5H</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>TCGA-ZF-AA5N</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>TCGA-ZF-AA5P</td>\n",
       "      <td>Stroma-rich</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Patient        class  class_int\n",
       "0    TCGA-2F-A9KO         LumP          0\n",
       "1    TCGA-2F-A9KP         LumP          0\n",
       "2    TCGA-2F-A9KQ         LumP          0\n",
       "3    TCGA-2F-A9KR        Ba/Sq          1\n",
       "4    TCGA-2F-A9KT        Ba/Sq          1\n",
       "..            ...          ...        ...\n",
       "399  TCGA-ZF-AA56        Ba/Sq          1\n",
       "400  TCGA-ZF-AA58        Ba/Sq          1\n",
       "401  TCGA-ZF-AA5H        Ba/Sq          1\n",
       "402  TCGA-ZF-AA5N         LumP          0\n",
       "403  TCGA-ZF-AA5P  Stroma-rich          3\n",
       "\n",
       "[404 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "Dataframe_Labels = pd.read_csv(\"../../../BLCA_DATA/Workspace/labels_str.csv\")\n",
    "Dataframe_link = pd.read_csv(\"../../../BLCA_DATA/Workspace/patient_norm.csv\")\n",
    "Dataframe_node= pd.read_csv(\"../../../BLCA_DATA/Workspace/node_embedding.csv\")\n",
    "\n",
    "Dataframe_Labels['class_int'], uniques = pd.factorize(Dataframe_Labels['class'])\n",
    "Dataframe_Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en place des arretes, noeuds et leur poids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### à améliorer, modification du treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4,\n",
      "        1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0,\n",
      "        1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0,\n",
      "        3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0,\n",
      "        0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3,\n",
      "        3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1,\n",
      "        1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4,\n",
      "        4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0,\n",
      "        0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0,\n",
      "        0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1,\n",
      "        3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3,\n",
      "        4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0,\n",
      "        1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3])\n",
      "tensor([[  0,   0,   0,  ..., 400, 401, 401],\n",
      "        [  1,   2,   4,  ..., 403, 402, 403]]) 2\n",
      "tensor([[0.4701, 0.0000, 0.5965,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4157, 0.0000, 0.8246,  ..., 1.0000, 0.0000, 0.0000],\n",
      "        [0.3748, 0.0000, 0.5088,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.5292, 1.0000, 0.4211,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4987, 1.0000, 0.5088,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4382, 0.0000, 0.6667,  ..., 0.0000, 0.0000, 0.0000]]) 404\n",
      "tensor([0.4688, 0.0954, 0.4535,  ..., 0.7794, 0.3396, 0.0024]) 49308\n"
     ]
    }
   ],
   "source": [
    "node_features = Dataframe_node.drop(columns=['Patient']).values\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "x = node_features\n",
    "patient_similarity = cosine_similarity(Dataframe_link.iloc[:, 1:])\n",
    "similarity_threshold = 0.5  # Exemple de seuil de similarité\n",
    "\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "\n",
    "for i in range(patient_similarity.shape[0]):\n",
    "    for j in range(i + 1, patient_similarity.shape[0]):\n",
    "        if patient_similarity[i, j] > similarity_threshold:\n",
    "            edge_index.append([i, j])\n",
    "            edge_attr.append((patient_similarity[i, j] - similarity_threshold)/(1 - similarity_threshold))\n",
    "        patient_similarity[i, i] = 0\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.int64).t().contiguous()\n",
    "edge_features = torch.tensor(Dataframe_link.drop(columns=['Patient']).values, dtype=torch.float)\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "temporary_node_tab = Dataframe_Labels[\"class_int\"].values\n",
    "node_labels = torch.tensor(temporary_node_tab, dtype=torch.long)\n",
    "print(node_labels)\n",
    "print(edge_index, len(edge_index))\n",
    "print(edge_features, len(edge_features))\n",
    "print(edge_attr, len(edge_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4465,  1.4465,  1.4465,  1.2007,  1.2007,  3.4662,  3.4662,  4.0825,\n",
       "         1.4465,  1.2007,  3.4662,  1.4465,  1.2007,  1.4465,  3.4662,  1.2007,\n",
       "         1.2007,  1.4465,  1.2007,  1.2007,  3.4662,  1.4465,  3.4662,  9.1855,\n",
       "         1.2007,  1.2007, 30.6185,  1.2007,  1.2007,  1.2007,  1.2007,  1.2007,\n",
       "         1.2007,  1.4465,  4.0825,  4.0825,  1.4465,  1.2007,  1.2007,  3.4662,\n",
       "         1.2007, 30.6185,  9.1855,  1.2007,  1.4465,  1.2007,  1.2007,  1.4465,\n",
       "         1.2007,  1.2007, 30.6185,  1.2007,  1.2007,  1.4465,  1.2007,  1.4465,\n",
       "         1.2007,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,\n",
       "         1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,\n",
       "         1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.2007,  1.4465,  3.4662,\n",
       "         1.2007,  3.4662,  1.4465,  1.4465,  1.2007,  3.4662,  1.2007,  3.4662,\n",
       "         1.4465,  1.4465,  1.2007,  3.4662,  9.1855,  1.2007,  4.0825,  1.4465,\n",
       "         4.0825,  9.1855,  1.2007,  1.2007,  1.2007,  9.1855,  3.4662,  1.2007,\n",
       "         1.2007,  4.0825,  1.4465,  9.1855,  1.2007,  3.4662,  1.2007,  1.2007,\n",
       "         4.0825,  1.4465,  1.2007,  1.4465,  1.4465,  3.4662,  9.1855,  1.2007,\n",
       "         1.4465,  1.2007,  1.2007,  1.4465,  1.2007,  1.2007,  1.2007,  4.0825,\n",
       "         3.4662, 30.6185,  1.4465,  1.4465,  1.2007,  3.4662,  1.4465,  1.4465,\n",
       "         1.2007,  1.2007,  1.4465,  1.2007,  1.4465,  3.4662,  1.4465,  1.4465,\n",
       "         1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  4.0825,  3.4662,  1.4465,\n",
       "         1.4465,  1.2007,  1.4465,  1.2007,  3.4662,  1.4465,  1.4465,  1.2007,\n",
       "         1.2007,  1.2007,  1.2007,  1.2007,  1.2007,  1.2007,  1.2007,  1.2007,\n",
       "         1.4465,  3.4662,  4.0825,  4.0825,  1.2007,  1.2007,  1.2007,  4.0825,\n",
       "         3.4662,  1.2007,  3.4662,  3.4662,  4.0825,  4.0825,  1.4465,  1.2007,\n",
       "         1.4465,  4.0825,  1.2007,  1.2007,  1.4465,  4.0825,  1.2007,  4.0825,\n",
       "         4.0825,  1.2007,  1.2007,  1.4465,  1.2007,  1.2007,  9.1855,  4.0825,\n",
       "         4.0825,  1.2007,  1.4465,  4.0825,  4.0825,  1.2007,  9.1855,  1.2007,\n",
       "         3.4662,  1.4465,  3.4662,  4.0825,  1.4465,  1.2007,  1.4465,  1.2007,\n",
       "         1.2007,  1.4465, 30.6185,  1.4465,  1.2007,  1.2007,  1.4465,  3.4662,\n",
       "         1.4465,  1.2007,  1.4465,  3.4662,  1.4465,  1.2007,  4.0825,  1.4465,\n",
       "         1.2007,  1.4465,  1.2007,  1.2007,  3.4662,  1.2007,  3.4662,  9.1855,\n",
       "         9.1855,  1.2007,  1.2007,  1.4465,  3.4662,  3.4662,  1.2007,  1.4465,\n",
       "         1.2007,  1.4465,  1.2007,  1.2007,  1.4465,  4.0825,  3.4662,  1.2007,\n",
       "         9.1855,  1.4465,  3.4662,  3.4662,  1.4465,  4.0825,  3.4662,  1.4465,\n",
       "         1.4465,  1.2007,  3.4662,  1.4465,  3.4662,  1.4465,  1.4465,  3.4662,\n",
       "         1.2007,  1.2007,  4.0825,  9.1855,  1.2007,  1.2007,  1.2007,  1.4465,\n",
       "         1.2007,  1.2007,  1.2007,  1.2007,  3.4662,  4.0825,  4.0825,  1.4465,\n",
       "         1.4465,  1.4465,  3.4662,  1.4465,  3.4662,  1.2007,  3.4662,  1.4465,\n",
       "         3.4662,  1.2007,  1.2007,  4.0825,  9.1855,  1.4465,  4.0825,  1.2007,\n",
       "         3.4662,  1.2007,  1.4465,  1.4465,  1.4465,  1.2007,  1.2007,  1.2007,\n",
       "         4.0825,  1.4465,  1.2007,  3.4662,  3.4662,  1.4465,  1.4465,  1.4465,\n",
       "         3.4662,  1.2007,  1.2007,  9.1855,  1.4465,  9.1855,  1.4465,  1.4465,\n",
       "         4.0825,  1.2007,  4.0825,  4.0825,  1.2007,  4.0825,  1.2007,  4.0825,\n",
       "         9.1855,  4.0825,  1.2007,  1.2007,  9.1855,  3.4662,  1.2007,  1.2007,\n",
       "         1.2007,  1.2007,  1.2007,  1.2007,  1.2007,  3.4662,  1.2007,  4.0825,\n",
       "         3.4662,  1.4465,  4.0825,  1.2007,  1.2007,  3.4662,  9.1855,  1.4465,\n",
       "         1.2007,  1.4465,  1.2007,  1.2007,  1.2007,  1.2007, 30.6185,  4.0825,\n",
       "         1.4465,  1.4465,  1.2007,  1.4465,  1.2007,  9.1855,  1.4465,  1.4465,\n",
       "         1.4465,  1.4465,  1.4465,  1.4465,  4.0825,  4.0825,  1.2007,  1.2007,\n",
       "         1.2007,  1.4465,  1.4465,  1.2007,  1.2007,  1.2007,  3.4662,  1.4465,\n",
       "         1.2007,  1.2007,  1.4465,  1.4465,  4.0825,  1.2007,  1.2007,  1.2007,\n",
       "         1.2007,  1.2007,  1.4465,  4.0825])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_dict = {0: 'LumP', \n",
    "                1: 'Ba/Sq', \n",
    "                2: 'LumU', \n",
    "                3: 'Stroma-rich', \n",
    "                4: 'LumNS', \n",
    "                5: 'NE-like'\n",
    "}\n",
    "\n",
    "def count_classes_weights(tensor):\n",
    "    array = tensor.numpy()\n",
    "    N = np.shape(array)[0]\n",
    "    classes_tab = { 0: 0, \n",
    "                    1: 0,\n",
    "                    2: 0, \n",
    "                    3: 0, \n",
    "                    4: 0,\n",
    "                    5: 0\n",
    "    }\n",
    "    for i in array:\n",
    "        classes_tab[i]+=1\n",
    "\n",
    "    mean_nb_classes = 0\n",
    "    for i in classes_tab:\n",
    "        mean_nb_classes += i\n",
    "    mean_nb_classes *= 1/len(classes_tab)\n",
    "    \n",
    "    # normalize the weights\n",
    "    weight_sum = 0\n",
    "    for i in range(len(classes_tab)):\n",
    "        if classes_tab[i] != 0:\n",
    "            weight_sum += mean_nb_classes / classes_tab[i]\n",
    "    alpha = 1 / weight_sum\n",
    "\n",
    "    weight_dict = {}\n",
    "    used_classes = []\n",
    "    for i in range(len(classes_tab)):\n",
    "        if classes_tab[i] != 0:\n",
    "            weight_dict[i] = alpha * (mean_nb_classes / classes_tab[i]) *50\n",
    "            used_classes.append(classes_dict[i])\n",
    "\n",
    "    return used_classes, weight_dict\n",
    "\n",
    "used_classes, weight_dict = count_classes_weights(node_labels)\n",
    "\n",
    "Dataframe_Labels['weight'] = [weight_dict[x] for x in Dataframe_Labels['class_int']]\n",
    "\n",
    "node_weights = torch.tensor(Dataframe_Labels['weight'], dtype=torch.float)\n",
    "node_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1er GATv2 - Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des masques utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mask(start, length=0, to_end=False):\n",
    "    mask = []\n",
    "    for i in range(404):\n",
    "        if (i < start or i >= start + length) and not to_end: \n",
    "            mask.append(False)\n",
    "        else : \n",
    "            mask.append(True)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data on which the model will be trained\n",
    "train_length = int(404 * 0.7)\n",
    "train_mask = torch.tensor(set_mask(start=0, length=train_length), dtype=torch.bool)\n",
    "\n",
    "length = int((404 - train_length) * 0.5)\n",
    "val_mask = torch.tensor(set_mask(start=train_length, length=length), dtype=torch.bool)\n",
    "\n",
    "test_mask = torch.tensor(set_mask(start=train_length + length, length=length), dtype=torch.bool)\n",
    "\n",
    "train_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de l'objet data utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=node_labels, weights = node_weights, num_classes=6, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATv2(\n",
      "  (conv1): GATv2Conv(825, 16, heads=8)\n",
      "  (conv2): GATv2Conv(128, 6, heads=1)\n",
      ")\n",
      "test 404\n",
      "test 404\n",
      "Epoch: 001, Loss: 2.0898, Val 0.2623, Test 0.2951\n"
     ]
    }
   ],
   "source": [
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super(GATv2, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GATv2Conv(data.num_features, hidden_channels, heads=heads, edge_dim=1)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, data.num_classes, edge_dim=1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = GATv2(hidden_channels=16, heads=8)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(mask):\n",
    "    print(\"test\", len(mask))\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    #print(f\"out shape: {out.shape}\")\n",
    "    pred = out.argmax(dim=1)\n",
    "    #print(pred[mask])\n",
    "    correct = pred[mask] == data.y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc , pred\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    loss = train()\n",
    "    val_acc = test(data.val_mask)[0]\n",
    "    test_acc, pred = test(data.test_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val {val_acc:.4f}, Test {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5,\n",
      "        3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2,\n",
      "        0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(data.y[data.test_mask])\n",
    "print(pred[test_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2eme GATv2 - Avec mise à jour des poids des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    range(data.x.shape[0]), \n",
    "    test_size=0.2, \n",
    "    stratify=data.y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "node_features = Dataframe_node.drop(columns=['Patient']).values\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "x_train = node_features[train_val_indices]\n",
    "x_test = node_features[test_indices]\n",
    "\n",
    "patient_similarity = cosine_similarity(Dataframe_link.iloc[:, 1:])\n",
    "similarity_threshold = 0.5  # Exemple de seuil de similarité\n",
    "\n",
    "# Calculate the edges and attention ridges for training\n",
    "edge_index_for_training = []\n",
    "edge_attr_for_training = []\n",
    "\n",
    "for i in train_val_indices:\n",
    "    for j in train_val_indices:\n",
    "        if i >= j :\n",
    "            break\n",
    "        if patient_similarity[i, j] > similarity_threshold:\n",
    "            edge_index_for_training.append([i, j])\n",
    "            edge_attr_for_training.append((patient_similarity[i, j] - similarity_threshold)/(1 - similarity_threshold))\n",
    "\n",
    "# Calculate the edges and attention ridges for testing\n",
    "edge_index_for_testing = []\n",
    "edge_attr_for_testing = []\n",
    "for i in test_indices:\n",
    "    for j in test_indices:\n",
    "        if i >= j :\n",
    "            break\n",
    "        if patient_similarity[i, j] > similarity_threshold:\n",
    "            edge_index_for_testing.append([i, j])\n",
    "            edge_attr_for_testing.append((patient_similarity[i, j] - similarity_threshold)/(1 - similarity_threshold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10580/743607179.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_features_for_training = torch.tensor(x_train, dtype=torch.float)\n",
      "/tmp/ipykernel_10580/743607179.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_features_for_testing = torch.tensor(x_test, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "edge_features_for_training = torch.tensor(x_train, dtype=torch.float)\n",
    "edge_index_for_training = torch.tensor(edge_index_for_training, dtype=torch.int64).t().contiguous()\n",
    "edge_attr_for_training = torch.tensor(edge_attr_for_training, dtype=torch.float)\n",
    "node_weights_for_training = node_weights[train_val_indices]\n",
    "\n",
    "edge_features_for_testing = torch.tensor(x_test, dtype=torch.float)\n",
    "edge_index_for_testing = torch.tensor(edge_index_for_testing, dtype=torch.int64).t().contiguous()\n",
    "edge_attr_for_testing = torch.tensor(edge_attr_for_testing, dtype=torch.float)\n",
    "node_weights_for_testing = node_weights[test_indices]\n",
    "\n",
    "node_labels = Dataframe_Labels[\"class_int\"].values\n",
    "train_labels = torch.tensor(node_labels[train_val_indices], dtype=torch.long)\n",
    "test_labels = torch.tensor(node_labels[test_indices], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "\n",
    "train_val_data = Data(\n",
    "    x=edge_features_for_training, \n",
    "    edge_index=edge_index_for_training, \n",
    "    edge_attr=edge_attr_for_training, \n",
    "    y=train_labels, \n",
    "    weights=node_weights_for_training, \n",
    "    num_classes=num_classes,\n",
    "    num_nodes = len(edge_features_for_training)\n",
    ")\n",
    "\n",
    "test_data = Data(\n",
    "    x=edge_features_for_testing, \n",
    "    edge_index=edge_index_for_testing, \n",
    "    edge_attr=edge_attr_for_testing, \n",
    "    y=test_labels, \n",
    "    weights=node_weights_for_testing, \n",
    "    num_classes=num_classes, \n",
    "    num_nodes = len(edge_features_for_testing)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like'] 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remik/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index 333 is out of bounds for dimension 0 with size 323",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 127\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model\n\u001b[0;32m--> 127\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_val_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"labels=['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like']\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03mfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mplt.show()\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[72], line 85\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(data, k_folds, hidden_channels, heads, num_epochs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train_acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val_acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m val_acc, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes_for_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m y_true \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mval_mask]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[72], line 39\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, data, mask, num_classes)\u001b[0m\n\u001b[1;32m     37\u001b[0m data\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 39\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m correct \u001b[38;5;241m=\u001b[39m pred[mask] \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my[mask]\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[72], line 15\u001b[0m, in \u001b[0;36mGATv2.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch_geometric/nn/conv/gatv2_conv.py:285\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    282\u001b[0m         num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_nodes, x_r\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    283\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m remove_self_loops(\n\u001b[1;32m    284\u001b[0m         edge_index, edge_attr)\n\u001b[0;32m--> 285\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43madd_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch_geometric/utils/loop.py:487\u001b[0m, in \u001b[0;36madd_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected sparse tensor layout (got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     loop_attr \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loop_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_attr, loop_attr], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m full_edge_index, edge_attr\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch_geometric/utils/loop.py:766\u001b[0m, in \u001b[0;36mcompute_loop_attr\u001b[0;34m(edge_index, edge_attr, num_nodes, is_sparse, fill_value)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fill_value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    765\u001b[0m     col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_sparse \u001b[38;5;28;01melse\u001b[39;00m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill_value\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch_geometric/utils/_scatter.py:79\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mcount\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_ones\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     count \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index 333 is out of bounds for dimension 0 with size 323"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sn\n",
    "import random\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super(GATv2, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GATv2Conv(data.num_features, hidden_channels, heads=heads, edge_dim=1)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, data.num_classes, edge_dim=1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "def weighted_cross_entropy_loss(output, target, weights):\n",
    "    loss = F.cross_entropy(output, target, reduction='none')\n",
    "    weighted_loss = loss * weights[target]\n",
    "    return weighted_loss.mean()\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = weighted_cross_entropy_loss(out[data.train_mask], data.y[data.train_mask], data.weights[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, model\n",
    "\n",
    "\n",
    "def test(model, data, mask, num_classes):\n",
    "    data.num_classes = num_classes\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data.y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc, pred[mask]\n",
    "\n",
    "def cross_validation(data, k_folds, hidden_channels, heads, num_epochs=1000):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=1234)\n",
    "    all_val_acc = []\n",
    "\n",
    "    val_acc_best = 0\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(data.x, data.y)):\n",
    "        # Define masks\n",
    "        data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.train_mask[train_index] = True\n",
    "        \n",
    "        data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.val_mask[val_index] = True\n",
    "        \n",
    "        train_classes, _ = count_classes_weights(data.y[data.train_mask])\n",
    "        val_classes, _ = count_classes_weights(data.y[data.val_mask])\n",
    "\n",
    "        num_classes_for_train = len(train_classes)\n",
    "        num_classes_for_val = len(val_classes)\n",
    "\n",
    "        print(val_classes, len(val_classes))\n",
    "\n",
    "        # Initialize model, optimizer, and loss function\n",
    "        model = GATv2(hidden_channels=hidden_channels, heads=heads)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "        lost_function_compil = []\n",
    "        all_val_acc_plot = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1, num_epochs):\n",
    "            loss, model = train(model, data, optimizer)\n",
    "            train_acc, _ = test (model, data, data.train_mask, num_classes_for_train)\n",
    "            val_acc, _ = test(model, data, data.val_mask, num_classes_for_val)\n",
    "            lost_function_compil.append(loss.detach().tolist())\n",
    "            all_val_acc_plot.append(val_acc)\n",
    "\n",
    "            print(f'Fold: {fold + 1}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Train_acc {train_acc:.4f}, Val_acc {val_acc:.4f}')\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_acc, y_pred = test(model, data, data.val_mask, num_classes_for_val)\n",
    "        y_true = data.y[data.val_mask]\n",
    "\n",
    "        # Store results\n",
    "        all_val_acc.append(val_acc)\n",
    "        conf_matrix = confusion_matrix(y_true.cpu().tolist(), y_pred.cpu().tolist())\n",
    "\n",
    "        if val_acc > val_acc_best:\n",
    "            best_model = model\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        df_cm = pd.DataFrame(conf_matrix_normalized, index=val_classes, columns=val_classes)  \n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix on Validation')\n",
    "        plt.show()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        ax1.plot(range(len(lost_function_compil)), lost_function_compil, label='Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Loss Function over Epochs')\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2.plot(range(len(all_val_acc_plot)), all_val_acc_plot, label='val_accuracy')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('val_accuracy')\n",
    "        ax2.set_title('val_accuracy over Epochs')\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Calculate and print overall metrics\n",
    "    mean_val_acc = np.mean(all_val_acc)\n",
    "    std_val_acc = np.std(all_val_acc)\n",
    "    print(f'Mean Validation Accuracy: {mean_val_acc:.4f}, Std Validation Accuracy: {std_val_acc:.4f}')\n",
    "\n",
    "    return best_model\n",
    "    \n",
    "best_model = cross_validation(train_val_data, k_folds=10, hidden_channels=20, heads=8, num_epochs=1)\n",
    "\n",
    "\"\"\"labels=['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.pie(train_classes, labels=labels, startangle=90, autopct='%1.1f%%')\n",
    "ax1.axis('equal')\n",
    "ax1.set_title('Cancer repartition for training', fontsize=14)\n",
    "\n",
    "ax2.pie(val_classes, labels=labels, startangle=90, autopct='%1.1f%%')\n",
    "ax2.axis('equal')\n",
    "ax2.set_title('Cancer repartition for validation', fontsize=14)\n",
    "\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "print(data.train_mask)\n",
    "print(data.val_mask)\n",
    "print(data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9508\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_acc, y_pred = test(best_model, data, data.test_mask)\n",
    "y_true = data.y[data.test_mask]\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true.cpu().tolist(), y_pred.cpu().tolist())\n",
    "\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "classes = ('LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like')\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(conf_matrix_normalized, index=classes, columns=classes)  \n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix on Test')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
