{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>class</th>\n",
       "      <th>class_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-2F-A9KO</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-2F-A9KP</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-2F-A9KQ</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-2F-A9KR</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-2F-A9KT</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>TCGA-ZF-AA56</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>TCGA-ZF-AA58</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>TCGA-ZF-AA5H</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>TCGA-ZF-AA5N</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>TCGA-ZF-AA5P</td>\n",
       "      <td>Stroma-rich</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Patient        class  class_int\n",
       "0    TCGA-2F-A9KO         LumP          0\n",
       "1    TCGA-2F-A9KP         LumP          0\n",
       "2    TCGA-2F-A9KQ         LumP          0\n",
       "3    TCGA-2F-A9KR        Ba/Sq          1\n",
       "4    TCGA-2F-A9KT        Ba/Sq          1\n",
       "..            ...          ...        ...\n",
       "399  TCGA-ZF-AA56        Ba/Sq          1\n",
       "400  TCGA-ZF-AA58        Ba/Sq          1\n",
       "401  TCGA-ZF-AA5H        Ba/Sq          1\n",
       "402  TCGA-ZF-AA5N         LumP          0\n",
       "403  TCGA-ZF-AA5P  Stroma-rich          3\n",
       "\n",
       "[404 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataframe_Labels = pd.read_csv(\"../../../BLCA_DATA/Workspace/labels_str.csv\")\n",
    "Dataframe_link = pd.read_csv(\"../../../BLCA_DATA/Workspace/patient_norm.csv\")\n",
    "Dataframe_node= pd.read_csv(\"../../../BLCA_DATA/Workspace/node_embedding.csv\")\n",
    "\n",
    "Dataframe_Labels['class_int'], uniques = pd.factorize(Dataframe_Labels['class'])\n",
    "Dataframe_Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN - Mise en place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage des labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4,\n",
       "        1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0,\n",
       "        1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0,\n",
       "        3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0,\n",
       "        0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3,\n",
       "        3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1,\n",
       "        1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4,\n",
       "        4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0,\n",
       "        0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0,\n",
       "        0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1,\n",
       "        3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3,\n",
       "        4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0,\n",
       "        1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = Dataframe_node.drop(columns=['Patient']).values\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "x = node_features\n",
    "patient_similarity = cosine_similarity(Dataframe_link.iloc[:, 1:])\n",
    "similarity_threshold = 0.5  # Exemple de seuil de similarité\n",
    "\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "\n",
    "for i in range(patient_similarity.shape[0]):\n",
    "    for j in range(i + 1, patient_similarity.shape[0]):\n",
    "        if patient_similarity[i, j] > similarity_threshold:\n",
    "            edge_index.append([i, j])\n",
    "            edge_attr.append((patient_similarity[i, j] - similarity_threshold)/(1 - similarity_threshold))\n",
    "        patient_similarity[i, i] = 0\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.int64).t().contiguous()\n",
    "edge_features = torch.tensor(Dataframe_link.drop(columns=['Patient']).values, dtype=torch.float)\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "temporary_node_tab = Dataframe_Labels[\"class_int\"].values\n",
    "node_labels = torch.tensor(temporary_node_tab, dtype=torch.long)\n",
    "node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_mask(start, length):\n",
    "    mask = []\n",
    "    for i in range(404):\n",
    "        if i < start or i >= start + length: \n",
    "            mask.append(False)\n",
    "        else : \n",
    "            mask.append(True)\n",
    "    return mask      \n",
    "\n",
    "# data on which the model will be trained\n",
    "train_mask = torch.tensor(set_mask(start=0, length=300), dtype=torch.bool)\n",
    "val_mask = torch.tensor(set_mask(start=300, length=50), dtype=torch.bool)\n",
    "test_mask = torch.tensor(set_mask(start=350, length=50), dtype=torch.bool)\n",
    "\n",
    "train_mask  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[404, 825], edge_index=[2, 49308], edge_attr=[49308], y=[404], num_classes=6, train_mask=[404], val_mask=[404], test_mask=[404])\n",
      "404\n",
      "[0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0, 3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1, 1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1, 3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3, 4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0, 3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1, 1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3]\n",
      "[0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0, 3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1, 1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1, 3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3, 4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3]\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False]\n",
      "[1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1]\n",
      "[0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0, 3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1, 1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1, 3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3, 4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3]\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1, 3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3, 4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2]\n",
      "tensor([[0.6311, 0.6632, 0.1897,  ..., 0.2745, 0.5263, 0.4457],\n",
      "        [0.7192, 0.6342, 0.2297,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4369, 0.7220, 0.0566,  ..., 0.4525, 0.1483, 0.9542],\n",
      "        ...,\n",
      "        [0.4963, 0.7932, 0.2578,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.2678, 0.7633, 0.1379,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.5489, 0.7777, 0.2237,  ..., 0.4013, 0.5078, 0.3990]])\n",
      "tensor([[  0,   0,   0,  ..., 400, 401, 401],\n",
      "        [  1,   2,   4,  ..., 403, 402, 403]])\n"
     ]
    }
   ],
   "source": [
    "data1 = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=node_labels, num_classes=6, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "print(data1)\n",
    "\n",
    "print(len(data1.y.tolist()))\n",
    "print(data1.y.tolist())\n",
    "print(data1.train_mask.tolist())\n",
    "print(data1.y[data1.train_mask].tolist())\n",
    "print(data1.y.tolist())\n",
    "print(data1.test_mask.tolist())\n",
    "print(data1.y[data1.test_mask].tolist())\n",
    "print(data1.y.tolist())\n",
    "print(data1.val_mask.tolist())\n",
    "print(data1.y[data1.val_mask].tolist())\n",
    "\n",
    "print(data1.x)\n",
    "print(data1.edge_index)\n",
    "\n",
    "data = data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Epoch: 001, Loss: 1.8446, Val 0.2200, Test 0.3210\n",
      "Fold: 1, Epoch: 002, Loss: 8.8413, Val 0.2200, Test 0.3210\n",
      "Fold: 1, Epoch: 003, Loss: 3.4744, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 004, Loss: 3.8248, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 005, Loss: 3.8794, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 006, Loss: 3.7523, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 007, Loss: 3.2025, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 008, Loss: 3.2805, Val 0.1200, Test 0.1358\n",
      "Fold: 1, Epoch: 009, Loss: 2.7699, Val 0.1200, Test 0.1358\n",
      "Fold: 1, Epoch: 010, Loss: 2.4546, Val 0.1200, Test 0.1358\n",
      "Fold: 1, Epoch: 011, Loss: 2.4440, Val 0.1200, Test 0.1358\n",
      "Fold: 1, Epoch: 012, Loss: 1.7970, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 013, Loss: 1.8054, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 014, Loss: 1.6183, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 015, Loss: 1.6240, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 016, Loss: 1.6903, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 017, Loss: 1.6699, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 018, Loss: 1.7724, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 019, Loss: 1.6786, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 020, Loss: 1.6099, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 021, Loss: 1.6290, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 022, Loss: 1.5966, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 023, Loss: 1.7084, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 024, Loss: 1.6374, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 025, Loss: 1.5671, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 026, Loss: 1.6076, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 027, Loss: 1.5345, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 028, Loss: 1.6773, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 029, Loss: 1.5571, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 030, Loss: 1.4907, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 031, Loss: 1.5379, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 032, Loss: 1.6181, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 033, Loss: 1.5485, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 034, Loss: 1.5294, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 035, Loss: 1.4849, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 036, Loss: 1.5094, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 037, Loss: 1.5470, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 038, Loss: 1.5605, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 039, Loss: 1.5821, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 040, Loss: 1.5660, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 041, Loss: 1.5136, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 042, Loss: 1.5386, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 043, Loss: 1.5365, Val 0.4000, Test 0.4198\n",
      "Fold: 1, Epoch: 044, Loss: 1.5287, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 045, Loss: 1.5098, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 046, Loss: 1.4887, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 047, Loss: 1.5320, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 048, Loss: 1.4826, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 049, Loss: 1.5114, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 050, Loss: 1.5316, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 051, Loss: 1.5195, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 052, Loss: 1.4819, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 053, Loss: 1.4637, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 054, Loss: 1.4558, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 055, Loss: 1.5023, Val 0.4000, Test 0.3951\n",
      "Fold: 1, Epoch: 056, Loss: 1.5061, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 057, Loss: 1.5022, Val 0.4000, Test 0.3951\n",
      "Fold: 1, Epoch: 058, Loss: 1.5009, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 059, Loss: 1.5064, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 060, Loss: 1.4675, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 061, Loss: 1.5353, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 062, Loss: 1.4821, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 063, Loss: 1.4806, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 064, Loss: 1.4966, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 065, Loss: 1.5111, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 066, Loss: 1.4616, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 067, Loss: 1.4796, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 068, Loss: 1.5053, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 069, Loss: 1.4801, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 070, Loss: 1.4941, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 071, Loss: 1.4805, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 072, Loss: 1.4623, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 073, Loss: 1.4880, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 074, Loss: 1.4797, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 075, Loss: 1.4757, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 076, Loss: 1.4503, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 077, Loss: 1.4660, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 078, Loss: 1.4272, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 079, Loss: 1.4771, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 080, Loss: 1.4608, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 081, Loss: 1.4947, Val 0.4000, Test 0.4198\n",
      "Fold: 1, Epoch: 082, Loss: 1.4850, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 083, Loss: 1.4550, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 084, Loss: 1.4757, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 085, Loss: 1.4316, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 086, Loss: 1.4996, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 087, Loss: 1.4613, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 088, Loss: 1.4632, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 089, Loss: 1.4700, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 090, Loss: 1.4350, Val 0.4200, Test 0.4321\n",
      "Fold: 1, Epoch: 091, Loss: 1.4891, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 092, Loss: 1.4760, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 093, Loss: 1.4495, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 094, Loss: 1.4425, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 095, Loss: 1.4383, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 096, Loss: 1.4573, Val 0.4000, Test 0.3704\n",
      "Fold: 1, Epoch: 097, Loss: 1.4681, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 098, Loss: 1.4397, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 099, Loss: 1.4468, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 100, Loss: 1.4219, Val 0.4600, Test 0.4198\n",
      "Fold: 1, Epoch: 101, Loss: 1.4478, Val 0.4600, Test 0.4198\n",
      "Fold: 1, Epoch: 102, Loss: 1.4703, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 103, Loss: 1.4149, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 104, Loss: 1.4521, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 105, Loss: 1.4380, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 106, Loss: 1.4401, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 107, Loss: 1.4183, Val 0.4000, Test 0.4074\n",
      "Fold: 1, Epoch: 108, Loss: 1.4529, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 109, Loss: 1.4201, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 110, Loss: 1.4169, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 111, Loss: 1.4615, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 112, Loss: 1.3994, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 113, Loss: 1.3764, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 114, Loss: 1.4032, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 115, Loss: 1.4034, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 116, Loss: 1.3975, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 117, Loss: 1.3510, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 118, Loss: 1.3864, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 119, Loss: 1.3709, Val 0.4400, Test 0.4321\n",
      "Fold: 1, Epoch: 120, Loss: 1.3597, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 121, Loss: 1.3388, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 122, Loss: 1.3413, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 123, Loss: 1.2652, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 124, Loss: 1.2957, Val 0.4200, Test 0.4321\n",
      "Fold: 1, Epoch: 125, Loss: 1.2719, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 126, Loss: 1.2985, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 127, Loss: 1.2375, Val 0.4000, Test 0.4198\n",
      "Fold: 1, Epoch: 128, Loss: 1.2862, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 129, Loss: 1.2611, Val 0.4000, Test 0.4321\n",
      "Fold: 1, Epoch: 130, Loss: 1.3562, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 131, Loss: 1.2288, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 132, Loss: 1.2540, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 133, Loss: 1.3140, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 134, Loss: 1.2603, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 135, Loss: 1.2784, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 136, Loss: 1.3446, Val 0.4800, Test 0.5309\n",
      "Fold: 1, Epoch: 137, Loss: 1.2735, Val 0.5800, Test 0.6667\n",
      "Fold: 1, Epoch: 138, Loss: 1.2443, Val 0.5800, Test 0.6790\n",
      "Fold: 1, Epoch: 139, Loss: 1.2285, Val 0.5000, Test 0.5802\n",
      "Fold: 1, Epoch: 140, Loss: 1.2205, Val 0.4600, Test 0.4568\n",
      "Fold: 1, Epoch: 141, Loss: 1.2412, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 142, Loss: 1.1569, Val 0.4600, Test 0.4321\n",
      "Fold: 1, Epoch: 143, Loss: 1.1796, Val 0.4600, Test 0.4568\n",
      "Fold: 1, Epoch: 144, Loss: 1.1669, Val 0.4800, Test 0.5309\n",
      "Fold: 1, Epoch: 145, Loss: 1.2533, Val 0.5800, Test 0.6667\n",
      "Fold: 1, Epoch: 146, Loss: 1.1412, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 147, Loss: 1.1432, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 148, Loss: 1.2215, Val 0.5000, Test 0.5802\n",
      "Fold: 1, Epoch: 149, Loss: 1.1430, Val 0.4800, Test 0.5309\n",
      "Fold: 1, Epoch: 150, Loss: 1.1494, Val 0.4800, Test 0.5679\n",
      "Fold: 1, Epoch: 151, Loss: 1.2034, Val 0.6200, Test 0.6420\n",
      "Fold: 1, Epoch: 152, Loss: 1.2307, Val 0.6200, Test 0.6667\n",
      "Fold: 1, Epoch: 153, Loss: 1.1724, Val 0.6200, Test 0.6667\n",
      "Fold: 1, Epoch: 154, Loss: 1.1629, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 155, Loss: 1.1453, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 156, Loss: 1.1511, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 157, Loss: 1.1460, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 158, Loss: 1.1132, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 159, Loss: 1.1113, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 160, Loss: 1.1024, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 161, Loss: 1.1698, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 162, Loss: 1.0411, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 163, Loss: 1.1337, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 164, Loss: 1.1180, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 165, Loss: 1.1227, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 166, Loss: 1.1033, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 167, Loss: 1.1407, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 168, Loss: 1.0846, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 169, Loss: 1.1419, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 170, Loss: 1.0644, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 171, Loss: 1.0995, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 172, Loss: 1.0882, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 173, Loss: 1.1102, Val 0.6200, Test 0.6543\n",
      "Fold: 1, Epoch: 174, Loss: 1.1045, Val 0.6200, Test 0.6543\n",
      "Fold: 1, Epoch: 175, Loss: 1.0832, Val 0.6200, Test 0.6667\n",
      "Fold: 1, Epoch: 176, Loss: 1.1270, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 177, Loss: 1.0126, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 178, Loss: 1.0372, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 179, Loss: 1.0525, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 180, Loss: 1.0644, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 181, Loss: 1.1351, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 182, Loss: 1.1121, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 183, Loss: 1.0021, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 184, Loss: 0.9764, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 185, Loss: 1.0117, Val 0.6200, Test 0.6667\n",
      "Fold: 1, Epoch: 186, Loss: 1.0617, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 187, Loss: 1.1144, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 188, Loss: 1.0400, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 189, Loss: 1.0095, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 190, Loss: 1.0444, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 191, Loss: 1.0137, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 192, Loss: 1.0596, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 193, Loss: 1.0448, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 194, Loss: 1.0766, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 195, Loss: 1.0416, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 196, Loss: 0.9613, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 197, Loss: 0.9885, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 198, Loss: 0.9893, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 199, Loss: 1.0131, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 200, Loss: 1.0518, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 201, Loss: 0.9808, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 202, Loss: 1.0321, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 203, Loss: 0.9600, Val 0.6200, Test 0.6667\n",
      "Fold: 1, Epoch: 204, Loss: 1.0152, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 205, Loss: 1.0360, Val 0.6200, Test 0.6790\n",
      "Fold: 1, Epoch: 206, Loss: 1.0074, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 207, Loss: 1.0451, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 208, Loss: 1.0435, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 209, Loss: 1.0034, Val 0.6200, Test 0.6914\n",
      "Fold: 1, Epoch: 210, Loss: 1.0138, Val 0.6200, Test 0.6914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m     std_test_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(all_test_acc)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(data, k_folds)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1001\u001b[39m):\n\u001b[1;32m     55\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(model, data, optimizer, criterion)\n\u001b[0;32m---> 56\u001b[0m     val_acc, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     test_acc, _ \u001b[38;5;241m=\u001b[39m test(model, data, data\u001b[38;5;241m.\u001b[39mtest_mask)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, data, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(model, data, mask):\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 31\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m     correct \u001b[38;5;241m=\u001b[39m pred[mask] \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my[mask]\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mGATv2.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch_geometric/nn/conv/gatv2_conv.py:298\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_r\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                          \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: PairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m    302\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39m(x_l, x_r), alpha\u001b[38;5;241m=\u001b[39malpha)\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater__02_xaj7.py:169\u001b[0m, in \u001b[0;36medge_updater\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    159\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    160\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    161\u001b[0m                 x_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    166\u001b[0m             )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# End Edge Update Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Begin Edge Update Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch_geometric/nn/conv/gatv2_conv.py:334\u001b[0m, in \u001b[0;36mGATv2Conv.edge_update\u001b[0;34m(self, x_j, x_i, edge_attr, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    332\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m edge_attr\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_edge \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m edge_attr\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n\u001b[1;32m    336\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m edge_attr\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sn\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super(GATv2, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GATv2Conv(data.num_features, hidden_channels, heads=heads, edge_dim=1)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, data.num_classes, edge_dim=1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(model, data, mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data.y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc, pred[mask]\n",
    "\n",
    "def cross_validation(data, k_folds=5):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=1234)\n",
    "    all_test_acc = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(data.x, data.y)):\n",
    "        # Define masks\n",
    "        data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.train_mask[train_index] = True\n",
    "        data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.test_mask[test_index] = True\n",
    "\n",
    "        # Initialize model, optimizer, and loss function\n",
    "        model = GATv2(hidden_channels=8, heads=8)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1, 1001):\n",
    "            loss = train(model, data, optimizer, criterion)\n",
    "            val_acc, _ = test(model, data, data.val_mask)\n",
    "            test_acc, _ = test(model, data, data.test_mask)\n",
    "            print(f'Fold: {fold + 1}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val {val_acc:.4f}, Test {test_acc:.4f}')\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_acc, y_pred = test(model, data, data.test_mask)\n",
    "        y_true = data.y[data.test_mask]\n",
    "\n",
    "        # Store results\n",
    "        all_test_acc.append(test_acc)\n",
    "        conf_matrix = confusion_matrix(y_true.cpu().tolist(), y_pred.cpu().tolist())\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        classes = ('LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like')\n",
    "        df_cm = pd.DataFrame(conf_matrix, index = [i for i in classes], columns = [i for i in classes])\n",
    "        #plt.figure(figsize=(12,7))\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig('output.png')\n",
    "        plt.show()\n",
    "            \n",
    "    # Calculate and print overall metrics\n",
    "    mean_test_acc = np.mean(all_test_acc)\n",
    "    std_test_acc = np.std(all_test_acc)\n",
    "    print(f'Mean Test Accuracy: {mean_test_acc:.4f}, Std Test Accuracy: {std_test_acc:.4f}')\n",
    "    \n",
    "cross_validation(data, k_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
