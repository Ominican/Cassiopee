{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>class</th>\n",
       "      <th>class_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-2F-A9KO</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-2F-A9KP</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-2F-A9KQ</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-2F-A9KR</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-2F-A9KT</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>TCGA-ZF-AA56</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>TCGA-ZF-AA58</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>TCGA-ZF-AA5H</td>\n",
       "      <td>Ba/Sq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>TCGA-ZF-AA5N</td>\n",
       "      <td>LumP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>TCGA-ZF-AA5P</td>\n",
       "      <td>Stroma-rich</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Patient        class  class_int\n",
       "0    TCGA-2F-A9KO         LumP          0\n",
       "1    TCGA-2F-A9KP         LumP          0\n",
       "2    TCGA-2F-A9KQ         LumP          0\n",
       "3    TCGA-2F-A9KR        Ba/Sq          1\n",
       "4    TCGA-2F-A9KT        Ba/Sq          1\n",
       "..            ...          ...        ...\n",
       "399  TCGA-ZF-AA56        Ba/Sq          1\n",
       "400  TCGA-ZF-AA58        Ba/Sq          1\n",
       "401  TCGA-ZF-AA5H        Ba/Sq          1\n",
       "402  TCGA-ZF-AA5N         LumP          0\n",
       "403  TCGA-ZF-AA5P  Stroma-rich          3\n",
       "\n",
       "[404 rows x 3 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "Dataframe_Labels = pd.read_csv(\"../../BLCA_DATA/Workspace/labels_str.csv\")\n",
    "Dataframe_link = pd.read_csv(\"../../BLCA_DATA/Workspace/patient_norm.csv\")\n",
    "Dataframe_node= pd.read_csv(\"../../BLCA_DATA/Workspace/node_embedding.csv\")\n",
    "\n",
    "Dataframe_Labels['class_int'], uniques = pd.factorize(Dataframe_Labels['class'])\n",
    "Dataframe_Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en place des arretes, noeuds et leur poids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### à améliorer, modification du treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4,\n",
       "        1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0,\n",
       "        1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0,\n",
       "        3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0,\n",
       "        0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3,\n",
       "        3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1,\n",
       "        1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4,\n",
       "        4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0,\n",
       "        0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0,\n",
       "        0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1,\n",
       "        3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3,\n",
       "        4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0,\n",
       "        1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = Dataframe_node.drop(columns=['Patient']).values\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "x = node_features\n",
    "patient_similarity = cosine_similarity(Dataframe_link.iloc[:, 1:])\n",
    "similarity_threshold = 0.5  # Exemple de seuil de similarité\n",
    "\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "\n",
    "for i in range(patient_similarity.shape[0]):\n",
    "    for j in range(i + 1, patient_similarity.shape[0]):\n",
    "        if patient_similarity[i, j] > similarity_threshold:\n",
    "            edge_index.append([i, j])\n",
    "            edge_attr.append((patient_similarity[i, j] - similarity_threshold)/(1 - similarity_threshold))\n",
    "        patient_similarity[i, i] = 0\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.int64).t().contiguous()\n",
    "edge_features = torch.tensor(Dataframe_link.drop(columns=['Patient']).values, dtype=torch.float)\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "temporary_node_tab = Dataframe_Labels[\"class_int\"].values\n",
    "node_labels = torch.tensor(temporary_node_tab, dtype=torch.long)\n",
    "node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4465,  1.4465,  1.4465,  1.2007,  1.2007,  3.4662,  3.4662,  4.0825,\n",
       "         1.4465,  1.2007,  3.4662,  1.4465,  1.2007,  1.4465,  3.4662,  1.2007,\n",
       "         1.2007,  1.4465,  1.2007,  1.2007,  3.4662,  1.4465,  3.4662,  9.1855,\n",
       "         1.2007,  1.2007, 30.6185,  1.2007,  1.2007,  1.2007,  1.2007,  1.2007,\n",
       "         1.2007,  1.4465,  4.0825,  4.0825,  1.4465,  1.2007,  1.2007,  3.4662,\n",
       "         1.2007, 30.6185,  9.1855,  1.2007,  1.4465,  1.2007,  1.2007,  1.4465,\n",
       "         1.2007,  1.2007, 30.6185,  1.2007,  1.2007,  1.4465,  1.2007,  1.4465,\n",
       "         1.2007,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,\n",
       "         1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.4465,\n",
       "         1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  1.2007,  1.4465,  3.4662,\n",
       "         1.2007,  3.4662,  1.4465,  1.4465,  1.2007,  3.4662,  1.2007,  3.4662,\n",
       "         1.4465,  1.4465,  1.2007,  3.4662,  9.1855,  1.2007,  4.0825,  1.4465,\n",
       "         4.0825,  9.1855,  1.2007,  1.2007,  1.2007,  9.1855,  3.4662,  1.2007,\n",
       "         1.2007,  4.0825,  1.4465,  9.1855,  1.2007,  3.4662,  1.2007,  1.2007,\n",
       "         4.0825,  1.4465,  1.2007,  1.4465,  1.4465,  3.4662,  9.1855,  1.2007,\n",
       "         1.4465,  1.2007,  1.2007,  1.4465,  1.2007,  1.2007,  1.2007,  4.0825,\n",
       "         3.4662, 30.6185,  1.4465,  1.4465,  1.2007,  3.4662,  1.4465,  1.4465,\n",
       "         1.2007,  1.2007,  1.4465,  1.2007,  1.4465,  3.4662,  1.4465,  1.4465,\n",
       "         1.4465,  1.4465,  1.4465,  1.4465,  1.4465,  4.0825,  3.4662,  1.4465,\n",
       "         1.4465,  1.2007,  1.4465,  1.2007,  3.4662,  1.4465,  1.4465,  1.2007,\n",
       "         1.2007,  1.2007,  1.2007,  1.2007,  1.2007,  1.2007,  1.2007,  1.2007,\n",
       "         1.4465,  3.4662,  4.0825,  4.0825,  1.2007,  1.2007,  1.2007,  4.0825,\n",
       "         3.4662,  1.2007,  3.4662,  3.4662,  4.0825,  4.0825,  1.4465,  1.2007,\n",
       "         1.4465,  4.0825,  1.2007,  1.2007,  1.4465,  4.0825,  1.2007,  4.0825,\n",
       "         4.0825,  1.2007,  1.2007,  1.4465,  1.2007,  1.2007,  9.1855,  4.0825,\n",
       "         4.0825,  1.2007,  1.4465,  4.0825,  4.0825,  1.2007,  9.1855,  1.2007,\n",
       "         3.4662,  1.4465,  3.4662,  4.0825,  1.4465,  1.2007,  1.4465,  1.2007,\n",
       "         1.2007,  1.4465, 30.6185,  1.4465,  1.2007,  1.2007,  1.4465,  3.4662,\n",
       "         1.4465,  1.2007,  1.4465,  3.4662,  1.4465,  1.2007,  4.0825,  1.4465,\n",
       "         1.2007,  1.4465,  1.2007,  1.2007,  3.4662,  1.2007,  3.4662,  9.1855,\n",
       "         9.1855,  1.2007,  1.2007,  1.4465,  3.4662,  3.4662,  1.2007,  1.4465,\n",
       "         1.2007,  1.4465,  1.2007,  1.2007,  1.4465,  4.0825,  3.4662,  1.2007,\n",
       "         9.1855,  1.4465,  3.4662,  3.4662,  1.4465,  4.0825,  3.4662,  1.4465,\n",
       "         1.4465,  1.2007,  3.4662,  1.4465,  3.4662,  1.4465,  1.4465,  3.4662,\n",
       "         1.2007,  1.2007,  4.0825,  9.1855,  1.2007,  1.2007,  1.2007,  1.4465,\n",
       "         1.2007,  1.2007,  1.2007,  1.2007,  3.4662,  4.0825,  4.0825,  1.4465,\n",
       "         1.4465,  1.4465,  3.4662,  1.4465,  3.4662,  1.2007,  3.4662,  1.4465,\n",
       "         3.4662,  1.2007,  1.2007,  4.0825,  9.1855,  1.4465,  4.0825,  1.2007,\n",
       "         3.4662,  1.2007,  1.4465,  1.4465,  1.4465,  1.2007,  1.2007,  1.2007,\n",
       "         4.0825,  1.4465,  1.2007,  3.4662,  3.4662,  1.4465,  1.4465,  1.4465,\n",
       "         3.4662,  1.2007,  1.2007,  9.1855,  1.4465,  9.1855,  1.4465,  1.4465,\n",
       "         4.0825,  1.2007,  4.0825,  4.0825,  1.2007,  4.0825,  1.2007,  4.0825,\n",
       "         9.1855,  4.0825,  1.2007,  1.2007,  9.1855,  3.4662,  1.2007,  1.2007,\n",
       "         1.2007,  1.2007,  1.2007,  1.2007,  1.2007,  3.4662,  1.2007,  4.0825,\n",
       "         3.4662,  1.4465,  4.0825,  1.2007,  1.2007,  3.4662,  9.1855,  1.4465,\n",
       "         1.2007,  1.4465,  1.2007,  1.2007,  1.2007,  1.2007, 30.6185,  4.0825,\n",
       "         1.4465,  1.4465,  1.2007,  1.4465,  1.2007,  9.1855,  1.4465,  1.4465,\n",
       "         1.4465,  1.4465,  1.4465,  1.4465,  4.0825,  4.0825,  1.2007,  1.2007,\n",
       "         1.2007,  1.4465,  1.4465,  1.2007,  1.2007,  1.2007,  3.4662,  1.4465,\n",
       "         1.2007,  1.2007,  1.4465,  1.4465,  4.0825,  1.2007,  1.2007,  1.2007,\n",
       "         1.2007,  1.2007,  1.4465,  4.0825])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_dict = {0: 'LumP', \n",
    "                1: 'Ba/Sq', \n",
    "                2: 'LumU', \n",
    "                3: 'Stroma-rich', \n",
    "                4: 'LumNS', \n",
    "                5: 'NE-like'\n",
    "}\n",
    "\n",
    "def count_classes_weights(tensor):\n",
    "    array = tensor.numpy()\n",
    "    N = np.shape(array)[0]\n",
    "    classes_tab = { 0: 0, \n",
    "                    1: 0,\n",
    "                    2: 0, \n",
    "                    3: 0, \n",
    "                    4: 0,\n",
    "                    5: 0\n",
    "    }\n",
    "    for i in array:\n",
    "        classes_tab[i]+=1\n",
    "\n",
    "    mean_nb_classes = 0\n",
    "    for i in classes_tab:\n",
    "        mean_nb_classes += i\n",
    "    mean_nb_classes *= 1/len(classes_tab)\n",
    "    \n",
    "    # normalize the weights\n",
    "    weight_sum = 0\n",
    "    for i in range(len(classes_tab)):\n",
    "        if classes_tab[i] != 0:\n",
    "            weight_sum += mean_nb_classes / classes_tab[i]\n",
    "    alpha = 1 / weight_sum\n",
    "\n",
    "    weight_dict = {}\n",
    "    used_classes = []\n",
    "    for i in range(len(classes_tab)):\n",
    "        if classes_tab[i] != 0:\n",
    "            weight_dict[i] = alpha * (mean_nb_classes / classes_tab[i]) *50\n",
    "            used_classes.append(classes_dict[i])\n",
    "\n",
    "    return used_classes, weight_dict\n",
    "\n",
    "used_classes, weight_dict = count_classes_weights(node_labels)\n",
    "\n",
    "Dataframe_Labels['weight'] = [weight_dict[x] for x in Dataframe_Labels['class_int']]\n",
    "\n",
    "node_weights = torch.tensor(Dataframe_Labels['weight'], dtype=torch.float)\n",
    "node_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des masques utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mask(start, length=0, to_end=False):\n",
    "    mask = []\n",
    "    for i in range(404):\n",
    "        if (i < start or i >= start + length) and not to_end: \n",
    "            mask.append(False)\n",
    "        else : \n",
    "            mask.append(True)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data on which the model will be trained\n",
    "train_length = int(404 * 0.7)\n",
    "train_mask = torch.tensor(set_mask(start=0, length=train_length), dtype=torch.bool)\n",
    "\n",
    "length = int((404 - train_length) * 0.5)\n",
    "val_mask = torch.tensor(set_mask(start=train_length, length=length), dtype=torch.bool)\n",
    "\n",
    "test_mask = torch.tensor(set_mask(start=train_length + length, length=length), dtype=torch.bool)\n",
    "\n",
    "train_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de l'objet data utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[404, 825], edge_index=[2, 49308], edge_attr=[49308], y=[404], weights=[404], train_mask=[404], val_mask=[404], test_mask=[404], num_classes=6)\n",
      "404\n",
      "[0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0, 3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1, 1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1, 3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3, 4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0, 3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1, 1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1]\n",
      "[0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0, 3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1, 1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1, 3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3, 4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3]\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "[1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3]\n",
      "[0, 0, 0, 1, 1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 0, 3, 3, 0, 1, 1, 2, 1, 5, 4, 1, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 4, 1, 3, 0, 3, 4, 1, 1, 1, 4, 2, 1, 1, 3, 0, 4, 1, 2, 1, 1, 3, 0, 1, 0, 0, 2, 4, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 5, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 3, 0, 1, 0, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 0, 1, 1, 4, 3, 3, 1, 0, 3, 3, 1, 4, 1, 2, 0, 2, 3, 0, 1, 0, 1, 1, 0, 5, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 3, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 4, 0, 2, 2, 0, 3, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 3, 4, 1, 1, 1, 0, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1, 3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3, 4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3]\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[1, 1, 2, 3, 3, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 3, 4, 0, 3, 1, 2, 1, 0, 0, 0, 1, 1, 1, 3, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 4, 0, 4, 0, 0, 3, 1, 3, 3, 1, 3, 1, 3, 4, 3, 1, 1, 4, 2, 1]\n",
      "tensor([[0.6311, 0.6632, 0.1897,  ..., 0.2745, 0.5263, 0.4457],\n",
      "        [0.7192, 0.6342, 0.2297,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4369, 0.7220, 0.0566,  ..., 0.4525, 0.1483, 0.9542],\n",
      "        ...,\n",
      "        [0.4963, 0.7932, 0.2578,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.2678, 0.7633, 0.1379,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.5489, 0.7777, 0.2237,  ..., 0.4013, 0.5078, 0.3990]])\n",
      "tensor([[  0,   0,   0,  ..., 400, 401, 401],\n",
      "        [  1,   2,   4,  ..., 403, 402, 403]])\n"
     ]
    }
   ],
   "source": [
    "data1 = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=node_labels, weights = node_weights, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask, num_classes = 6)\n",
    "print(data1)\n",
    "\n",
    "print(len(data1.y.tolist()))\n",
    "print(data1.y.tolist())\n",
    "print(data1.train_mask.tolist())\n",
    "print(data1.y[data1.train_mask].tolist())\n",
    "print(data1.y.tolist())\n",
    "print(data1.test_mask.tolist())\n",
    "print(data1.y[data1.test_mask].tolist())\n",
    "print(data1.y.tolist())\n",
    "print(data1.val_mask.tolist())\n",
    "print(data1.y[data1.val_mask].tolist())\n",
    "\n",
    "print(data1.x)\n",
    "print(data1.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1er GATv2 - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATv2(\n",
      "  (conv1): GATv2Conv(825, 16, heads=8)\n",
      "  (conv2): GATv2Conv(128, 6, heads=1)\n",
      ")\n",
      "test 404\n",
      "test 404\n",
      "Epoch: 001, Loss: 2.0898, Val 0.2623, Test 0.2951\n"
     ]
    }
   ],
   "source": [
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super(GATv2, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GATv2Conv(data.num_features, hidden_channels, heads=heads, edge_dim=1)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, 6, edge_dim=1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = GATv2(hidden_channels=16, heads=8)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(mask):\n",
    "    print(\"test\", len(mask))\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    #print(f\"out shape: {out.shape}\")\n",
    "    pred = out.argmax(dim=1)\n",
    "    #print(pred[mask])\n",
    "    correct = pred[mask] == data.y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc , pred\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    loss = train()\n",
    "    val_acc = test(data.val_mask)[0]\n",
    "    test_acc, pred = test(data.test_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val {val_acc:.4f}, Test {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 0, 3, 1, 1, 2, 4, 0, 1, 0, 1, 1, 1, 1, 5,\n",
      "        3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 1, 0, 0, 1, 1, 1, 2,\n",
      "        0, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(data.y[data.test_mask])\n",
    "print(pred[test_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2eme GATv2 - Avec mise à jour des poids des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113, 96, 50, 192, 330, 182, 366, 252, 225, 278, 128, 202, 167, 142, 392, 247, 365, 232, 139, 342, 230, 244, 358, 197, 394, 215, 54, 279, 119, 369, 82, 203, 250, 98, 90, 211, 338, 402, 327, 186, 33, 0, 209, 189, 273, 103, 40, 66, 259, 205, 176, 23, 72, 263, 381, 69, 11, 376, 315, 227, 360, 41, 116, 73, 65, 42, 100, 166, 28, 325, 286, 86, 51, 184, 67, 351, 93, 380, 154, 111, 110, 332, 29, 260, 157, 359, 257, 212, 60, 379, 334, 196, 298, 213, 294, 155, 364, 49, 305, 173, 357, 285, 7, 174, 345, 214, 146, 59, 333, 386, 208, 48, 13, 217, 130, 304, 387, 105, 316, 195, 297]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test_index = random.sample(range(0, data.num_nodes), int(0.3 * data.num_nodes))\n",
    "print(test_index)\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.test_mask[test_index] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like'] 6\n",
      "['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich'] 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remik/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_39388/3443011196.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (5, 5), indices imply (6, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 145\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model\n\u001b[0;32m--> 145\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m\"\"\"labels=['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like']\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03max2.axis('equal')\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03max2.set_title('Cancer repartition for validation', fontsize=14)\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[140], line 115\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(data, k_folds, hidden_channels, heads, num_epochs)\u001b[0m\n\u001b[1;32m    112\u001b[0m labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLumP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBa/Sq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLumU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStroma-rich\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLumNS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNE-like\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    114\u001b[0m conf_matrix_normalized \u001b[38;5;241m=\u001b[39m conf_matrix\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m conf_matrix\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m--> 115\u001b[0m df_cm \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf_matrix_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    116\u001b[0m sn\u001b[38;5;241m.\u001b[39mheatmap(df_cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/pandas/core/frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    825\u001b[0m         )\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (5, 5), indices imply (6, 6)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sn\n",
    "import random\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super(GATv2, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GATv2Conv(data.num_features, hidden_channels, heads=heads, edge_dim=1)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, data.num_classes, edge_dim=1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "def weighted_cross_entropy_loss(output, target, weights):\n",
    "    loss = F.cross_entropy(output, target, reduction='none')\n",
    "    weighted_loss = loss * weights[target]\n",
    "    return weighted_loss.mean()\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = weighted_cross_entropy_loss(out[data.train_mask], data.y[data.train_mask], data.weights[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, model\n",
    "\n",
    "\n",
    "def test(model, data, mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data.y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc, pred[mask]\n",
    "\n",
    "def complete(dict):\n",
    "    classes = ['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like']\n",
    "    for i in classes:\n",
    "        if not dict[i]:\n",
    "            dict[i] = 0\n",
    "\n",
    "def dinstinction(data):\n",
    "    train_val_indexes = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "    for i in range(len(train_val_indexes)):\n",
    "        if data.test_mask[i]:\n",
    "            train_val_indexes[i] = False\n",
    "\n",
    "    X=data.x[train_val_indexes]\n",
    "    Y=data.y[train_val_indexes]\n",
    "    return(X,Y)\n",
    "\n",
    "def cross_validation(data, k_folds, hidden_channels, heads, num_epochs=1000):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=1234)\n",
    "    all_val_acc = []\n",
    "\n",
    "    val_acc_best = 0\n",
    "\n",
    "    data_X_for_Val, data_Y_for_val  = dinstinction(data)\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(data_X_for_Val, data_Y_for_val)):\n",
    "        # Define masks\n",
    "        data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.train_mask[train_index] = True\n",
    "        \n",
    "        data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.val_mask[val_index] = True\n",
    "        \n",
    "        train_classes, _ = count_classes_weights(data.y[data.train_mask])\n",
    "        val_classes, _ = count_classes_weights(data.y[data.val_mask])\n",
    "\n",
    "        print(train_classes, len(train_classes))\n",
    "        print(val_classes, len(val_classes))\n",
    "\n",
    "        # Initialize model, optimizer, and loss function\n",
    "        model = GATv2(hidden_channels=hidden_channels, heads=heads)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "        lost_function_compil = []\n",
    "        all_val_acc_plot = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1, num_epochs):\n",
    "            loss, model = train(model, data, optimizer)\n",
    "            train_acc, _ = test (model, data, data.train_mask)\n",
    "            val_acc, _ = test(model, data, data.val_mask)\n",
    "            lost_function_compil.append(loss.detach().tolist())\n",
    "            all_val_acc_plot.append(val_acc)\n",
    "\n",
    "            print(f'Fold: {fold + 1}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Train_acc {train_acc:.4f}, Val_acc {val_acc:.4f}')\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_acc, y_pred = test(model, data, data.val_mask)\n",
    "        y_true = data.y[data.val_mask]\n",
    "\n",
    "        # Store results\n",
    "        all_val_acc.append(val_acc)\n",
    "        conf_matrix = confusion_matrix(y_true.cpu().tolist(), y_pred.cpu().tolist())\n",
    "\n",
    "        if val_acc > val_acc_best:\n",
    "            best_model = model\n",
    "\n",
    "        # Plot confusion matrix\n",
    "\n",
    "        labels=['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like']\n",
    "\n",
    "        conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        df_cm = pd.DataFrame(conf_matrix_normalized, index=labels, columns=labels)  \n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix on Validation')\n",
    "        plt.show()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        ax1.plot(range(len(lost_function_compil)), lost_function_compil, label='Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Loss Function over Epochs')\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2.plot(range(len(all_val_acc_plot)), all_val_acc_plot, label='val_accuracy')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('val_accuracy')\n",
    "        ax2.set_title('val_accuracy over Epochs')\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Calculate and print overall metrics\n",
    "    mean_val_acc = np.mean(all_val_acc)\n",
    "    std_val_acc = np.std(all_val_acc)\n",
    "    print(f'Mean Validation Accuracy: {mean_val_acc:.4f}, Std Validation Accuracy: {std_val_acc:.4f}')\n",
    "\n",
    "    return best_model\n",
    "    \n",
    "best_model = cross_validation(data, k_folds=10, hidden_channels=20, heads=8, num_epochs=1)\n",
    "\n",
    "\"\"\"labels=['LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.pie(train_classes, labels=labels, startangle=90, autopct='%1.1f%%')\n",
    "ax1.axis('equal')\n",
    "ax1.set_title('Cancer repartition for training', fontsize=14)\n",
    "\n",
    "ax2.pie(val_classes, labels=labels, startangle=90, autopct='%1.1f%%')\n",
    "ax2.axis('equal')\n",
    "ax2.set_title('Cancer repartition for validation', fontsize=14)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False, False,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True, False,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True, False,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True,  True,  True, False,\n",
      "        False,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "         True, False, False, False,  True, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False,  True,\n",
      "         True, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False])\n",
      "tensor([ True, False, False, False,  True, False, False, False,  True,  True,\n",
      "         True, False, False, False, False,  True,  True, False,  True, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "         True, False,  True,  True, False,  True, False, False, False,  True,\n",
      "        False, False, False, False, False, False,  True, False, False,  True,\n",
      "        False,  True, False, False,  True, False,  True, False,  True,  True,\n",
      "         True, False,  True, False, False, False, False, False,  True, False,\n",
      "        False, False, False,  True,  True, False,  True,  True, False, False,\n",
      "         True,  True, False, False,  True,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True,  True, False,  True, False,  True,  True, False,\n",
      "        False, False,  True, False, False,  True,  True, False,  True, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "         True, False,  True, False, False,  True,  True,  True,  True, False,\n",
      "        False,  True, False, False,  True,  True, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False,  True,  True, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False,  True, False, False, False,  True,\n",
      "         True, False, False,  True, False, False,  True,  True, False,  True,\n",
      "         True, False, False, False,  True, False, False,  True, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False,  True,\n",
      "        False,  True, False,  True,  True,  True, False, False, False, False,\n",
      "        False, False, False, False,  True, False,  True, False, False, False,\n",
      "        False, False, False,  True, False,  True, False,  True,  True, False,\n",
      "        False, False, False,  True,  True, False,  True, False, False, False,\n",
      "        False, False,  True, False, False, False,  True, False,  True,  True,\n",
      "        False, False,  True,  True,  True, False, False, False, False, False,\n",
      "         True, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False,  True, False, False,  True, False,  True, False,  True,\n",
      "         True,  True, False,  True,  True, False, False, False, False, False,\n",
      "        False,  True,  True, False,  True, False, False, False, False, False,\n",
      "        False, False,  True,  True, False, False, False,  True, False, False,\n",
      "         True, False, False, False,  True,  True, False, False,  True,  True,\n",
      "        False, False, False,  True, False, False, False,  True,  True, False,\n",
      "        False, False, False, False, False,  True, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False,  True,  True, False,\n",
      "        False,  True, False, False])\n"
     ]
    }
   ],
   "source": [
    "print(data.train_mask)\n",
    "print(data.val_mask)\n",
    "print(data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.2893\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (5, 5), indices imply (6, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m classes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLumP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBa/Sq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLumU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStroma-rich\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLumNS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNE-like\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m conf_matrix_normalized \u001b[38;5;241m=\u001b[39m conf_matrix\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m conf_matrix\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m---> 10\u001b[0m df_cm \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf_matrix_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     11\u001b[0m sn\u001b[38;5;241m.\u001b[39mheatmap(df_cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/pandas/core/frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    825\u001b[0m         )\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Cassiopée/cassiopee-projet/Cass/lib/python3.12/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (5, 5), indices imply (6, 6)"
     ]
    }
   ],
   "source": [
    "test_acc, y_pred = test(best_model, data, data.test_mask)\n",
    "y_true = data.y[data.test_mask]\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true.cpu().tolist(), y_pred.cpu().tolist())\n",
    "\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "classes = ('LumP', 'Ba/Sq', 'LumU', 'Stroma-rich', 'LumNS', 'NE-like')\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(conf_matrix_normalized, index=classes, columns=classes)  \n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix on Test')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
